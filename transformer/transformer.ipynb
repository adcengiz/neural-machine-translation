{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import transformer.Constants as Constants\n",
    "from dataset import TranslationDataset, paired_collate_fn\n",
    "from transformer.Models import Transformer\n",
    "from transformer.Optim import ScheduledOptim\n",
    "\n",
    "from train import prepare_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda = False\n",
    "device = torch.device('cuda' if cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    ''' A sequence to sequence model with attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_src_vocab, n_tgt_vocab, len_max_seq,\n",
    "            d_word_vec=512, d_model=512, d_inner=2048,\n",
    "            n_layers=6, n_head=8, d_k=64, d_v=64, dropout=0.1,\n",
    "            tgt_emb_prj_weight_sharing=True,\n",
    "            emb_src_tgt_weight_sharing=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            n_src_vocab=n_src_vocab, len_max_seq=len_max_seq,\n",
    "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
    "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
    "            dropout=dropout)\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            n_tgt_vocab=n_tgt_vocab, len_max_seq=len_max_seq,\n",
    "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
    "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
    "            dropout=dropout)\n",
    "\n",
    "        self.tgt_word_prj = nn.Linear(d_model, n_tgt_vocab, bias=False)\n",
    "        nn.init.xavier_normal_(self.tgt_word_prj.weight)\n",
    "\n",
    "        assert d_model == d_word_vec, \\\n",
    "        'To facilitate the residual connections, \\\n",
    "         the dimensions of all module outputs shall be the same.'\n",
    "\n",
    "        if tgt_emb_prj_weight_sharing:\n",
    "            # Share the weight matrix between target word embedding & the final logit dense layer\n",
    "            self.tgt_word_prj.weight = self.decoder.tgt_word_emb.weight\n",
    "            self.x_logit_scale = (d_model ** -0.5)\n",
    "        else:\n",
    "            self.x_logit_scale = 1.\n",
    "\n",
    "        if emb_src_tgt_weight_sharing:\n",
    "            # Share the weight matrix between source & target word embeddings\n",
    "            assert n_src_vocab == n_tgt_vocab, \\\n",
    "            \"To share word embedding table, the vocabulary size of src/tgt shall be the same.\"\n",
    "            self.encoder.src_word_emb.weight = self.decoder.tgt_word_emb.weight\n",
    "\n",
    "    def forward(self, src_seq, src_pos, tgt_seq, tgt_pos):\n",
    "\n",
    "        tgt_seq, tgt_pos = tgt_seq[:, :-1], tgt_pos[:, :-1]\n",
    "\n",
    "        enc_output, *_ = self.encoder(src_seq, src_pos)\n",
    "        dec_output, *_ = self.decoder(tgt_seq, tgt_pos, src_seq, enc_output)\n",
    "        seq_logit = self.tgt_word_prj(dec_output) * self.x_logit_scale\n",
    "\n",
    "        return seq_logit.view(-1, seq_logit.size(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e083b314ffef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_token_seq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m \u001b[0;31m#data['settings'].max_token_seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msrc_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m  \u001b[0;31m# training_data.dataset.src_vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtgt_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m  \u001b[0;31m# training_data.dataset.tgt_vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "max_token_seq_len = 15 #data['settings'].max_token_seq_len\n",
    "training_data, validation_data = prepare_dataloaders(data, opt)\n",
    "src_vocab_size = 50000  # training_data.dataset.src_vocab_size\n",
    "tgt_vocab_size = 50000  # training_data.dataset.tgt_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1c1fab78d7a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m transformer = Transformer(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_token_seq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtgt_emb_prj_weight_sharing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_share_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(\n",
    "    n_src_vocab = src_vocab_size,\n",
    "    n_tgt_vocab = tgt_vocab_size,\n",
    "    len_max_seq = max_token_seq_len, \n",
    "    tgt_emb_prj_weight_sharing = True, # True\n",
    "    emb_src_tgt_weight_sharing = True, # True\n",
    "    d_k = 64,           # 64\n",
    "    d_v = 64,           # 64\n",
    "    d_model = 512,      # 512\n",
    "    d_word_vec = 512,   # 512\n",
    "    d_inner = 2048,     # 2048\n",
    "    n_layers = 6,       # 6\n",
    "    n_head = 8,         # 8\n",
    "    dropout = 0.1       # 0.1\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ScheduledOptim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b7328525f9e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m optimizer = ScheduledOptim(\n\u001b[0m\u001b[1;32m      2\u001b[0m     optim.Adam(\n\u001b[1;32m      3\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         betas=(0.9, 0.98), eps=1e-09),\n\u001b[1;32m      5\u001b[0m     opt.d_model, opt.n_warmup_steps)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ScheduledOptim' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = ScheduledOptim(\n",
    "    optim.Adam(\n",
    "        filter(lambda x: x.requires_grad, transformer.parameters()),\n",
    "        betas=(0.9, 0.98), eps=1e-09),\n",
    "    opt.d_model, opt.n_warmup_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ebefe733fb75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train(transformer, training_data, validation_data, optimizer, device ,opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
