{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Spring 2018 NLP Class Project: Neural Machine Translation</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atakanokan/anaconda/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MultiLabelBinarizer from version 0.19.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/atakanokan/anaconda/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.19.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/atakanokan/anaconda/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator SVC from version 0.19.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/atakanokan/anaconda/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator OneVsRestClassifier from version 0.19.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pdb\n",
    "import os\n",
    "from underthesea import word_tokenize\n",
    "import jieba\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# running on cpu\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! pip install spacy && python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Project Overview\n",
    "\n",
    "The goal of this project is to build a neural machine translation system and experience how recent advances have made their way. Each team will build the following sequence of neural translation systems for two language pairs, __Vietnamese (Vi)→English (En)__ and __Chinese (Zh)→En__ (prepared corpora is be provided):\n",
    "\n",
    "1. Recurrent neural network based encoder-decoder without attention\n",
    "2. Recurrent neural network based encoder-decoder with attention\n",
    "2. Replace the recurrent encoder with either convolutional or self-attention based encoder.\n",
    "4. [Optional] Build either or both fully self-attention translation system or/and multilingual translation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Upload & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_index = 0\n",
    "SOS_index = 1 # start of sentence\n",
    "UNK_index = 2 # 2 = unk\n",
    "EOS_index = 3 # end of sentence\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    \"\"\"\n",
    "    Vocabulary:\n",
    "        self.word2index = {'Khoa': 2,'hoc': 3,'ang': 4,...}\n",
    "    Frequency Table:\n",
    "        self.word2count = {'Khoa': 104,'hoc': 8952,'ang': 13887,...}\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"<PAD>\":PAD_index,\n",
    "                           \"<SOS>\":SOS_index,\n",
    "                           \"<UNK>\":UNK_index, \n",
    "                           \"<EOS>\":EOS_index}\n",
    "        self.word2count = {\"<PAD>\":0,\n",
    "                           \"<SOS>\":0,\n",
    "                           \"<UNK>\":0, \n",
    "                           \"<EOS>\":0}\n",
    "        self.index2word = {PAD_index:\"<PAD>\",\n",
    "                           SOS_index: \"<SOS>\",\n",
    "                           UNK_index:\"<UNK>\", \n",
    "                           EOS_index: \"<EOS>\"}\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        # class addWord for every token in the sentence\n",
    "        # seperated by \" \"\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        # adds the new word to the vocabulary\n",
    "        if word not in self.word2index:\n",
    "            # sets frequency to 1 & increments vocab size \n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1                    \n",
    "        else:\n",
    "            # just increments the count if the word is already in the vocab\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    \"\"\"About \"NFC\" and \"NFD\": \n",
    "    \n",
    "    For each character, there are two normal forms: normal form C \n",
    "    and normal form D. Normal form D (NFD) is also known as canonical \n",
    "    decomposition, and translates each character into its decomposed form. \n",
    "    Normal form C (NFC) first applies a canonical decomposition, then composes \n",
    "    pre-combined characters again.\n",
    "    \n",
    "    About unicodedata.category: \n",
    "    \n",
    "    Returns the general category assigned to the Unicode character \n",
    "    unichr as string.\"\"\"\n",
    "    \n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trim\n",
    "def normalizeString(s):\n",
    "    # removes blankspaces at the beginning and the end of the string\n",
    "    s = unicodeToAscii(s.strip())\n",
    "    # \n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # \n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False,\n",
    "             dataset=\"train\"):\n",
    "    \n",
    "    \"\"\"Takes as input;\n",
    "    - lang1, lang2: either (vi, en) or (zh, en)\n",
    "    - dataset: one of (\"train\",\"dev\",\"test\")\"\"\"\n",
    "    \n",
    "#     print(\"Reading lines...\")\n",
    "    print(\"\\nPreparing the \"+ str(dataset) +\" Dataset of \" + str(lang1) + \" to \" + str(lang2))\n",
    "    \n",
    "    eos = [\".\",\"?\",\"!\",\"\\n\"]\n",
    "    \n",
    "    # Read the pretokenized lang1 file and split into lines\n",
    "    lang1_lines = open(\"../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-%s-%s-processed/%s.tok.%s\" % (lang1, lang2, dataset, lang1), encoding=\"utf-8\").\\\n",
    "        read().strip().split(\"\\n\")\n",
    "        \n",
    "    # Read the lang2 file and split into lines\n",
    "    lang2_lines = open(\"../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-%s-%s-processed/%s.tok.%s\" % (lang1, lang2, dataset, lang2), encoding=\"utf-8\").\\\n",
    "        read().strip().split(\"\\n\")\n",
    "        \n",
    "    # Examples of Pretokenized Sentences\n",
    "    random_sent_no = random.randint(0,len(lang1_lines)-1)\n",
    "    print(\"\\nExample of Language #1 sentence: \" + str(lang1_lines[random_sent_no]))\n",
    "    print(\"Example of Language #2 sentence: \" + str(lang2_lines[random_sent_no]))\n",
    "    \n",
    "    # create sentence pairs (lists of length 2 that consist of string pairs)\n",
    "    # e.g. [\"And we &apos;re going to tell you some stories from the sea here in video .\",\n",
    "    #       \"我们 将 用 一些 影片 来讲 讲述 一些 深海 海里 的 故事  \"]\n",
    "    # check if there are the same number of sentences in each set\n",
    "    assert len(lang1_lines) == len(lang2_lines), \"Two languages must have the same number of sentences. \"+ str(len(lang1_lines)) + \" sentences were passed for \" + str(lang1) + \".\" + str(len(lang2_lines)) + \" sentences were passed for \" + str(lang2)+\".\"\n",
    "    print(\"\\nNumber of sentences in Language #1 = \" + str(len(lang1_lines)))\n",
    "    print(\"Number of sentences in Language #2 = \" + str(len(lang2_lines)))\n",
    "    \n",
    "    # normalize if not Chinese, Chinese normalization is already handeled\n",
    "    # add <EOS> tag at the end of the sentence for chinese\n",
    "    if lang1 == \"zh\":\n",
    "        lang1_lines = [s + \"<EOS>\" for s in lang1_lines]\n",
    "    else:\n",
    "        # replace .?!\\n with <EOS> tag for Vietnamese and English\n",
    "        lang1_lines = [normalizeString(s).replace(\".\",\"<EOS>\").\\\n",
    "                       replace(\"?\",\"<EOS>\").replace(\"!\",\"<EOS>\").replace(\"\\n\",\"<EOS>\") for s in lang1_lines]\n",
    "    lang2_lines = [normalizeString(s).replace(\".\",\"<EOS>\").\\\n",
    "                       replace(\"?\",\"<EOS>\").replace(\"!\",\"<EOS>\").replace(\"\\n\",\"<EOS>\") for s in lang2_lines]\n",
    "    \n",
    "    # construct pairs\n",
    "    pair_ran = range(len(lang1_lines))\n",
    "    pairs = [[lang1_lines[i]] + [lang2_lines[i]] for i in pair_ran]\n",
    "    \n",
    "#     # Split every line into pairs and normalize\n",
    "#     pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False, dataset=\"train\"):\n",
    "    \n",
    "    input_lang, output_lang, pairs = readLangs(lang1 = lang1, \n",
    "                                               lang2 = lang2, \n",
    "                                               reverse = reverse, \n",
    "                                               dataset = dataset)\n",
    "    \n",
    "#     print(\"Read %s sentence pairs\" % len(pairs))\n",
    "#     print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "\n",
    "#     print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "        \n",
    "    print(\"\\nVocabulary sizes:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    # Checking number of <EOS> tags in the sentences\n",
    "    print(\"\\nnumber of sentence pairs = \" + str(len(pairs)))\n",
    "    print(\"number of <EOS> tags in the source sentences = \" + str(input_lang.word2count[\"<EOS>\"]))\n",
    "    print(\"number of <EOS> tags in the target sentences = \" + str(output_lang.word2count[\"<EOS>\"]))\n",
    "\n",
    "    print(\"\\nRandom preprocessed sentence pair:\")\n",
    "    print(random.choice(pairs))\n",
    "    \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the train Dataset of vi to en\n",
      "\n",
      "Example of Language #1 sentence: Tôi đã ngồi bên cạnh một trong luật_sư rất can_đảm của chúng_tôi , và tôi nói , \" Làm thế_nào để chúng_ta có_thể biến điều_luật này thành hiện_thực ? Làm thế_nào chúng_ta có_thể đảm_bảo rằng nó được thi_hành ? \"\n",
      "Example of Language #2 sentence: And I was sitting side by side with one of our very courageous lawyers , and said , &quot; How can we get this out ? How can we make sure that this is implemented ?\n",
      "\n",
      "Number of sentences in Language #1 = 133317\n",
      "Number of sentences in Language #2 = 133317\n",
      "\n",
      "Vocabulary sizes:\n",
      "vi 16143\n",
      "en 47567\n",
      "\n",
      "number of sentence pairs = 133317\n",
      "number of <EOS> tags in the source sentences = 138953\n",
      "number of <EOS> tags in the target sentences = 148908\n",
      "\n",
      "Random preprocessed sentence pair:\n",
      "['Va toi phan ung Khong <EOS> Tai sao su bi an hay quy mo lon lao cua vu tru lai can phai i cung voi mot trang thai cam xuc huyen bi <EOS>', 'And I go quot No <EOS> quot Why does that sense of mystery that sense of the dizzying scale of the universe need to be accompanied by a mystical feeling <EOS>']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData(lang1 = 'vi', \n",
    "                                             lang2 = 'en', \n",
    "                                             reverse = False, \n",
    "                                             dataset = \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<SOS>': 1,\n",
       " '<UNK>': 2,\n",
       " '<EOS>': 3,\n",
       " 'Khoa': 4,\n",
       " 'hoc': 5,\n",
       " 'ang': 6,\n",
       " 'sau': 7,\n",
       " 'mot': 8,\n",
       " 'tieu': 9,\n",
       " 'e': 10,\n",
       " 've': 11,\n",
       " 'khi': 12,\n",
       " 'hau': 13,\n",
       " 'Trong': 14,\n",
       " 'phut': 15,\n",
       " 'chuyen': 16,\n",
       " 'gia': 17,\n",
       " 'hoa': 18,\n",
       " 'quyen': 19,\n",
       " 'Rachel': 20,\n",
       " 'Pike': 21,\n",
       " 'gioi': 22,\n",
       " 'thieu': 23,\n",
       " 'so': 24,\n",
       " 'luoc': 25,\n",
       " 'nhung': 26,\n",
       " 'no': 27,\n",
       " 'luc': 28,\n",
       " 'khoa': 29,\n",
       " 'miet': 30,\n",
       " 'mai': 31,\n",
       " 'tao': 32,\n",
       " 'bao': 33,\n",
       " 'bien': 34,\n",
       " 'oi': 35,\n",
       " 'cung': 36,\n",
       " 'voi': 37,\n",
       " 'oan': 38,\n",
       " 'nghien': 39,\n",
       " 'cuu': 40,\n",
       " 'cua': 41,\n",
       " 'minh': 42,\n",
       " 'hang': 43,\n",
       " 'ngan': 44,\n",
       " 'nguoi': 45,\n",
       " 'a': 46,\n",
       " 'cong': 47,\n",
       " 'hien': 48,\n",
       " 'cho': 49,\n",
       " 'du': 50,\n",
       " 'an': 51,\n",
       " 'nay': 52,\n",
       " 'bay': 53,\n",
       " 'mao': 54,\n",
       " 'hiem': 55,\n",
       " 'qua': 56,\n",
       " 'rung': 57,\n",
       " 'tim': 58,\n",
       " 'kiem': 59,\n",
       " 'thong': 60,\n",
       " 'tin': 61,\n",
       " 'phan': 62,\n",
       " 'tu': 63,\n",
       " 'then': 64,\n",
       " 'chot': 65,\n",
       " 'Toi': 66,\n",
       " 'muon': 67,\n",
       " 'cac': 68,\n",
       " 'ban': 69,\n",
       " 'biet': 70,\n",
       " 'su': 71,\n",
       " 'to': 72,\n",
       " 'lon': 73,\n",
       " 'gop': 74,\n",
       " 'lam': 75,\n",
       " 'nen': 76,\n",
       " 'dong': 77,\n",
       " 'tit': 78,\n",
       " 'thuong': 79,\n",
       " 'thay': 80,\n",
       " 'tren': 81,\n",
       " 'Co': 82,\n",
       " 'trong': 83,\n",
       " 'nhu': 84,\n",
       " 'the': 85,\n",
       " 'va': 86,\n",
       " 'noi': 87,\n",
       " 'chat': 88,\n",
       " 'luong': 89,\n",
       " 'khong': 90,\n",
       " 'hay': 91,\n",
       " 'khoi': 92,\n",
       " 'bui': 93,\n",
       " 'Ca': 94,\n",
       " 'hai': 95,\n",
       " 'eu': 96,\n",
       " 'la': 97,\n",
       " 'nhanh': 98,\n",
       " 'linh': 99,\n",
       " 'vuc': 100,\n",
       " 'nganh': 101,\n",
       " 'Cac': 102,\n",
       " 'gan': 103,\n",
       " 'ay': 104,\n",
       " 'Ban': 105,\n",
       " 'ieu': 106,\n",
       " 'hanh': 107,\n",
       " 'Bien': 108,\n",
       " 'Lien': 109,\n",
       " 'chinh': 110,\n",
       " 'phu': 111,\n",
       " 'goi': 112,\n",
       " 'tat': 113,\n",
       " 'IPCC': 114,\n",
       " 'ua': 115,\n",
       " 'ra': 116,\n",
       " 'bai': 117,\n",
       " 'ho': 118,\n",
       " 'he': 119,\n",
       " 'Nghien': 120,\n",
       " 'uoc': 121,\n",
       " 'viet': 122,\n",
       " 'boi': 123,\n",
       " 'nha': 124,\n",
       " 'quoc': 125,\n",
       " 'khac': 126,\n",
       " 'nhau': 127,\n",
       " 'Ho': 128,\n",
       " 'trang': 129,\n",
       " 'chu': 130,\n",
       " 'Va': 131,\n",
       " 'ca': 132,\n",
       " 'xem': 133,\n",
       " 'xet': 134,\n",
       " 'phe': 135,\n",
       " 'binh': 136,\n",
       " '': 137,\n",
       " 'o': 138,\n",
       " 'ong': 139,\n",
       " 'en': 140,\n",
       " 'thuc': 141,\n",
       " 'te': 142,\n",
       " 'cuoc': 143,\n",
       " 'hoi': 144,\n",
       " 'nam': 145,\n",
       " 'chung': 146,\n",
       " 'toi': 147,\n",
       " 'nghi': 148,\n",
       " 'nhien': 149,\n",
       " 'nhat': 150,\n",
       " 'Moi': 151,\n",
       " 'hon': 152,\n",
       " 'San': 153,\n",
       " 'Francisco': 154,\n",
       " 'tham': 155,\n",
       " 'thuoc': 156,\n",
       " 'nhom': 157,\n",
       " 'moi': 158,\n",
       " 'rat': 159,\n",
       " 'nhieu': 160,\n",
       " 'tai': 161,\n",
       " 'dang': 162,\n",
       " 'Voi': 163,\n",
       " 'Cambridge': 164,\n",
       " 'dao': 165,\n",
       " 'El': 166,\n",
       " 'Nino': 167,\n",
       " 'von': 168,\n",
       " 'co': 169,\n",
       " 'tac': 170,\n",
       " 'thoi': 171,\n",
       " 'tiet': 172,\n",
       " 'tinh': 173,\n",
       " 'thai': 174,\n",
       " 'canh': 175,\n",
       " 'lieu': 176,\n",
       " 'sinh': 177,\n",
       " 'lai': 178,\n",
       " 'chia': 179,\n",
       " 'nho': 180,\n",
       " 'bang': 181,\n",
       " 'tien': 182,\n",
       " 'si': 183,\n",
       " 'phai': 184,\n",
       " 'vo': 185,\n",
       " 'cu': 186,\n",
       " 'chi': 187,\n",
       " 'vai': 188,\n",
       " 'quy': 189,\n",
       " 'trinh': 190,\n",
       " 'Mot': 191,\n",
       " 'ten': 192,\n",
       " 'isoprene': 193,\n",
       " 'No': 194,\n",
       " 'huu': 195,\n",
       " 'chua': 196,\n",
       " 'tung': 197,\n",
       " 'nghe': 198,\n",
       " 'chiec': 199,\n",
       " 'kep': 200,\n",
       " 'giay': 201,\n",
       " 'vao': 202,\n",
       " 'khoang': 203,\n",
       " 'zeta': 204,\n",
       " 'illion': 205,\n",
       " 'mu': 206,\n",
       " 'Du': 207,\n",
       " 'ngang': 208,\n",
       " 'ngua': 209,\n",
       " 'tong': 210,\n",
       " 'dan': 211,\n",
       " 'toan': 212,\n",
       " 'cau': 213,\n",
       " 'lo': 214,\n",
       " 'metan': 215,\n",
       " 'Chinh': 216,\n",
       " 'vi': 217,\n",
       " 'y': 218,\n",
       " 'nghia': 219,\n",
       " 'quan': 220,\n",
       " 'nao': 221,\n",
       " 'theo': 222,\n",
       " 'uoi': 223,\n",
       " 'Chung': 224,\n",
       " 'manh': 225,\n",
       " 'Phong': 226,\n",
       " 'EUPHORE': 227,\n",
       " 'Tay': 228,\n",
       " 'Nha': 229,\n",
       " 'chay': 230,\n",
       " 'hoan': 231,\n",
       " 'dien': 232,\n",
       " 'cham': 233,\n",
       " 'lan': 234,\n",
       " 'ung': 235,\n",
       " 'xe': 236,\n",
       " 'vay': 237,\n",
       " 'van': 238,\n",
       " 'mo': 239,\n",
       " 'hinh': 240,\n",
       " 'sieu': 241,\n",
       " 'may': 242,\n",
       " 'viec': 243,\n",
       " 'Mo': 244,\n",
       " 'gom': 245,\n",
       " 'tram': 246,\n",
       " 'thung': 247,\n",
       " 'xep': 248,\n",
       " 'chong': 249,\n",
       " 'gian': 250,\n",
       " 'cuc': 251,\n",
       " 'Ma': 252,\n",
       " 'can': 253,\n",
       " 'tuan': 254,\n",
       " 'xong': 255,\n",
       " 'phep': 256,\n",
       " 'tich': 257,\n",
       " 'ta': 258,\n",
       " 'hieu': 259,\n",
       " 'gi': 260,\n",
       " 'xay': 261,\n",
       " 'con': 262,\n",
       " 'khap': 263,\n",
       " 'Gan': 264,\n",
       " 'khao': 265,\n",
       " 'sat': 266,\n",
       " 'ia': 267,\n",
       " 'Malaysia': 268,\n",
       " 'Con': 269,\n",
       " 'nua': 270,\n",
       " 'thap': 271,\n",
       " 'ngay': 272,\n",
       " 'giua': 273,\n",
       " 'treo': 274,\n",
       " 'thiet': 275,\n",
       " 'bi': 276,\n",
       " 'tri': 277,\n",
       " 'xa': 278,\n",
       " 'cai': 279,\n",
       " 'thu': 280,\n",
       " 'suot': 281,\n",
       " 'nhin': 282,\n",
       " 'cao': 283,\n",
       " 'duoi': 284,\n",
       " 'at': 285,\n",
       " 'giai': 286,\n",
       " 'mang': 287,\n",
       " 'Chiec': 288,\n",
       " 'phi': 289,\n",
       " 'mau': 290,\n",
       " 'BA': 291,\n",
       " 'do': 292,\n",
       " 'FAAM': 293,\n",
       " 'Rat': 294,\n",
       " 'tuong': 295,\n",
       " 'hom': 296,\n",
       " 'cach': 297,\n",
       " 'tang': 298,\n",
       " 'vom': 299,\n",
       " 'met': 300,\n",
       " 'ac': 301,\n",
       " 'nguy': 302,\n",
       " 'nghieng': 303,\n",
       " 'Phai': 304,\n",
       " 'thue': 305,\n",
       " 'hach': 306,\n",
       " 'khien': 307,\n",
       " 'xin': 308,\n",
       " 'lenh': 309,\n",
       " 'Khi': 310,\n",
       " 'quanh': 311,\n",
       " 'bo': 312,\n",
       " 'song': 313,\n",
       " 'lung': 314,\n",
       " 'len': 315,\n",
       " 'G': 316,\n",
       " 'that': 317,\n",
       " 'ghe': 318,\n",
       " 'Vi': 319,\n",
       " 'dung': 320,\n",
       " 'ben': 321,\n",
       " 'giong': 322,\n",
       " 'bat': 323,\n",
       " 'ky': 324,\n",
       " 'lich': 325,\n",
       " 'phong': 326,\n",
       " 'thi': 327,\n",
       " 'nghiem': 328,\n",
       " 'di': 329,\n",
       " 'giup': 330,\n",
       " 'thich': 331,\n",
       " 'ai': 332,\n",
       " 'loai': 333,\n",
       " 'se': 334,\n",
       " 'ngoai': 335,\n",
       " 'kien': 336,\n",
       " 'inh': 337,\n",
       " 'thanh': 338,\n",
       " 'muc': 339,\n",
       " 'mac': 340,\n",
       " 'chuong': 341,\n",
       " 'Noi': 342,\n",
       " 'anh': 343,\n",
       " 'luon': 344,\n",
       " 'kem': 345,\n",
       " 'tom': 346,\n",
       " 'oc': 347,\n",
       " 'sach': 348,\n",
       " 'Cam': 349,\n",
       " 'on': 350,\n",
       " 'Christopher': 351,\n",
       " 'deCharms': 352,\n",
       " 'quet': 353,\n",
       " 'than': 354,\n",
       " 'kinh': 355,\n",
       " 'sang': 356,\n",
       " 'che': 357,\n",
       " 'fMRI': 358,\n",
       " 'ghi': 359,\n",
       " 'hoat': 360,\n",
       " 'suy': 361,\n",
       " 'cam': 362,\n",
       " 'xuc': 363,\n",
       " 'au': 364,\n",
       " 'nhan': 365,\n",
       " 'Xin': 366,\n",
       " 'chao': 367,\n",
       " 'gio': 368,\n",
       " 'tay': 369,\n",
       " 'phia': 370,\n",
       " 'hoang': 371,\n",
       " 'Bat': 372,\n",
       " 'chuoc': 373,\n",
       " 'lap': 374,\n",
       " 'bap': 375,\n",
       " 'som': 376,\n",
       " 'vung': 377,\n",
       " 'ma': 378,\n",
       " 'Vang': 379,\n",
       " 'buoc': 380,\n",
       " 'quyet': 381,\n",
       " 'kho': 382,\n",
       " 'thuyen': 383,\n",
       " 'mach': 384,\n",
       " 'bach': 385,\n",
       " 'tan': 386,\n",
       " 'Nhung': 387,\n",
       " 'nghiep': 388,\n",
       " 'Peter': 389,\n",
       " 'xam': 390,\n",
       " 'nhap': 391,\n",
       " 'MRI': 392,\n",
       " 'Khong': 393,\n",
       " 'bom': 394,\n",
       " 'tia': 395,\n",
       " 'buc': 396,\n",
       " 'hop': 397,\n",
       " 'tam': 398,\n",
       " 'vang': 399,\n",
       " 'be': 400,\n",
       " 'mat': 401,\n",
       " 'Truoc': 402,\n",
       " 'ien': 403,\n",
       " 'robot': 404,\n",
       " 'chup': 405,\n",
       " 'Cai': 406,\n",
       " 'chiem': 407,\n",
       " 'hoac': 408,\n",
       " 'thang': 409,\n",
       " 'mili': 410,\n",
       " 'Anh': 411,\n",
       " 'iem': 412,\n",
       " 'kich': 413,\n",
       " 'Neu': 414,\n",
       " 'ba': 415,\n",
       " 'huong': 416,\n",
       " 'giuong': 417,\n",
       " 'vien': 418,\n",
       " 'lua': 419,\n",
       " 'chon': 420,\n",
       " 'Ta': 421,\n",
       " 'rang': 422,\n",
       " 'kenh': 423,\n",
       " 'nien': 424,\n",
       " 'giat': 425,\n",
       " 'neu': 426,\n",
       " 'dut': 427,\n",
       " 'vong': 428,\n",
       " 'san': 429,\n",
       " 'xuat': 430,\n",
       " 'xung': 431,\n",
       " 'chieu': 432,\n",
       " 'Bay': 433,\n",
       " 'uong': 434,\n",
       " 'i': 435,\n",
       " 'benh': 436,\n",
       " 'shock': 437,\n",
       " 'doi': 438,\n",
       " 'gay': 439,\n",
       " 'tap': 440,\n",
       " 'xoa': 441,\n",
       " 'diu': 442,\n",
       " 'goc': 443,\n",
       " 'trai': 444,\n",
       " 'ket': 445,\n",
       " 'phuong': 446,\n",
       " 'phap': 447,\n",
       " 'giam': 448,\n",
       " 'tran': 449,\n",
       " 'ki': 450,\n",
       " 'uc': 451,\n",
       " 'danh': 452,\n",
       " 'tue': 453,\n",
       " 'Beeban': 454,\n",
       " 'Kidron': 455,\n",
       " 'dieu': 456,\n",
       " 'phim': 457,\n",
       " 'suc': 458,\n",
       " 'thuat': 459,\n",
       " 'ao': 460,\n",
       " 'Phep': 461,\n",
       " 'Milan': 462,\n",
       " 'khu': 463,\n",
       " 'ke': 464,\n",
       " 'FILMCLUB': 465,\n",
       " 'lu': 466,\n",
       " 'tre': 467,\n",
       " 'Bang': 468,\n",
       " 'tuoi': 469,\n",
       " 'Tu': 470,\n",
       " 'me': 471,\n",
       " 'gai': 472,\n",
       " 'thuyet': 473,\n",
       " 'giao': 474,\n",
       " 'khan': 475,\n",
       " 'thinh': 476,\n",
       " 'Internet': 477,\n",
       " 'ngon': 478,\n",
       " 'nang': 479,\n",
       " 'coi': 480,\n",
       " 'trao': 481,\n",
       " 'trung': 482,\n",
       " 'loi': 483,\n",
       " 'liet': 484,\n",
       " 'sac': 485,\n",
       " 'qui': 486,\n",
       " 'truyen': 487,\n",
       " 'tranh': 488,\n",
       " 'vuot': 489,\n",
       " 'ranh': 490,\n",
       " 'ngu': 491,\n",
       " 'triet': 492,\n",
       " 'ly': 493,\n",
       " 'Thuc': 494,\n",
       " 'rong': 495,\n",
       " 'Hollywood': 496,\n",
       " 'phuc': 497,\n",
       " 'vu': 498,\n",
       " 'kieng': 499,\n",
       " 'giac': 500,\n",
       " 'quen': 501,\n",
       " 'truoc': 502,\n",
       " 'La': 503,\n",
       " 'ngai': 504,\n",
       " 'reo': 505,\n",
       " 'Chua': 506,\n",
       " 'Tuong': 507,\n",
       " 'it': 508,\n",
       " 'That': 509,\n",
       " 'mia': 510,\n",
       " 'yeu': 511,\n",
       " 'chuc': 512,\n",
       " 'truong': 513,\n",
       " 'thao': 514,\n",
       " 'luan': 515,\n",
       " 'tra': 516,\n",
       " 'soat': 517,\n",
       " 'le': 518,\n",
       " 'ngung': 519,\n",
       " 'tiep': 520,\n",
       " 'thon': 521,\n",
       " 'DVD': 522,\n",
       " 'lac': 523,\n",
       " 'doc': 524,\n",
       " 'nuoc': 525,\n",
       " 'em': 526,\n",
       " 'ngat': 527,\n",
       " 'quang': 528,\n",
       " 'mon': 529,\n",
       " 'giau': 530,\n",
       " 'cap': 531,\n",
       " 'tuc': 532,\n",
       " 'duc': 533,\n",
       " 'tuy': 534,\n",
       " 'kha': 535,\n",
       " 'kham': 536,\n",
       " 'pha': 537,\n",
       " 'Ngay': 538,\n",
       " 'mong': 539,\n",
       " 'Bo': 540,\n",
       " 'Vittorio': 541,\n",
       " 'De': 542,\n",
       " 'Sica': 543,\n",
       " 'chuot': 544,\n",
       " 'ngheo': 545,\n",
       " 'khat': 546,\n",
       " 'dip': 547,\n",
       " 'Cong': 548,\n",
       " 'rap': 549,\n",
       " 'in': 550,\n",
       " 'cha': 551,\n",
       " 'Su': 552,\n",
       " 'mung': 553,\n",
       " 'teen': 554,\n",
       " 'niem': 555,\n",
       " 'hy': 556,\n",
       " 'cuoi': 557,\n",
       " 'bau': 558,\n",
       " 'troi': 559,\n",
       " 'cay': 560,\n",
       " 'choi': 561,\n",
       " 'Sau': 562,\n",
       " 'muoi': 563,\n",
       " 'guong': 564,\n",
       " 'ngac': 565,\n",
       " 'ngo': 566,\n",
       " 'toc': 567,\n",
       " 'lien': 568,\n",
       " 'Trieu': 569,\n",
       " 'pho': 570,\n",
       " 'favela': 571,\n",
       " 'Rio': 572,\n",
       " 'mua': 573,\n",
       " 'Cau': 574,\n",
       " 'Ong': 575,\n",
       " 'Smith': 576,\n",
       " 'Washington': 577,\n",
       " 'het': 578,\n",
       " 'Frank': 579,\n",
       " 'Capra': 580,\n",
       " 'tro': 581,\n",
       " 'long': 582,\n",
       " 'nguon': 583,\n",
       " 'lau': 584,\n",
       " 'buoi': 585,\n",
       " 'luat': 586,\n",
       " 'Toa': 587,\n",
       " 'vui': 588,\n",
       " 'sao': 589,\n",
       " 'nguyen': 590,\n",
       " 'Jimmy': 591,\n",
       " 'Stewart': 592,\n",
       " 'Khach': 593,\n",
       " 'Rwanda': 594,\n",
       " 'bon': 595,\n",
       " 'diet': 596,\n",
       " 'giot': 597,\n",
       " 'thuy': 598,\n",
       " 'gat': 599,\n",
       " 'Schindler': 600,\n",
       " 'roi': 601,\n",
       " 'Ke': 602,\n",
       " 'moc': 603,\n",
       " 'tui': 604,\n",
       " 'tuoc': 605,\n",
       " 'pham': 606,\n",
       " 'Gui': 607,\n",
       " 'men': 608,\n",
       " 'ot': 609,\n",
       " 'Briton': 610,\n",
       " 'da': 611,\n",
       " 'chui': 612,\n",
       " 'rua': 613,\n",
       " 'ngoi': 614,\n",
       " 'Sidney': 615,\n",
       " 'Potier': 616,\n",
       " 'lay': 617,\n",
       " 'xuoi': 618,\n",
       " 'am': 619,\n",
       " 'cang': 620,\n",
       " 'ganh': 621,\n",
       " 'vinh': 622,\n",
       " 'trieu': 623,\n",
       " 'Mac': 624,\n",
       " 'cat': 625,\n",
       " 'Persepolis': 626,\n",
       " 'Iran': 627,\n",
       " 'Ham': 628,\n",
       " 'map': 629,\n",
       " 'giet': 630,\n",
       " 'chet': 631,\n",
       " 'nem': 632,\n",
       " 'man': 633,\n",
       " 'tau': 634,\n",
       " 'Ai': 635,\n",
       " 'sai': 636,\n",
       " 'iep': 637,\n",
       " 'dau': 638,\n",
       " 'Lam': 639,\n",
       " 'mieng': 640,\n",
       " 'chang': 641,\n",
       " 'tuyet': 642,\n",
       " 'chan': 643,\n",
       " 'nui': 644,\n",
       " 'cuop': 645,\n",
       " 'Kha': 646,\n",
       " 'Israel': 647,\n",
       " 'loan': 648,\n",
       " 'Me': 649,\n",
       " 'chau': 650,\n",
       " 'Au': 651,\n",
       " 'nan': 652,\n",
       " 'kim': 653,\n",
       " 'cuong': 654,\n",
       " 'khau': 655,\n",
       " 'tron': 656,\n",
       " 'Luan': 657,\n",
       " 'im': 658,\n",
       " 'lang': 659,\n",
       " 'Anne': 660,\n",
       " 'thoat': 661,\n",
       " 'Shoah': 662,\n",
       " 'Chien': 663,\n",
       " 'Will': 664,\n",
       " 'Leni': 665,\n",
       " 'Riefenstahl': 666,\n",
       " 'Nazi': 667,\n",
       " 'chiu': 668,\n",
       " 'ich': 669,\n",
       " 'sot': 670,\n",
       " 'thoang': 671,\n",
       " 'xuyen': 672,\n",
       " 'Nguoi': 673,\n",
       " 'thuan': 674,\n",
       " 'xua': 675,\n",
       " 'tho': 676,\n",
       " 'thien': 677,\n",
       " 'cuon': 678,\n",
       " 'Nhu': 679,\n",
       " 'Phu': 680,\n",
       " 'xu': 681,\n",
       " 'Oz': 682,\n",
       " 'Hay': 683,\n",
       " 'Kane': 684,\n",
       " 'Jane': 685,\n",
       " 'Austen': 686,\n",
       " 'Tennyson': 687,\n",
       " 'khung': 688,\n",
       " 'thau': 689,\n",
       " 'phoi': 690,\n",
       " 'gach': 691,\n",
       " 'Tom': 692,\n",
       " 'Hanks': 693,\n",
       " 'tru': 694,\n",
       " 'Jim': 695,\n",
       " 'Lovell': 696,\n",
       " 'khuon': 697,\n",
       " 'Ben': 698,\n",
       " 'Kingley': 699,\n",
       " 'Gandhi': 700,\n",
       " 'Eve': 701,\n",
       " 'Harrington': 702,\n",
       " 'Howard': 703,\n",
       " 'Beale': 704,\n",
       " 'Mildred': 705,\n",
       " 'Pierce': 706,\n",
       " 'bot': 707,\n",
       " 'Shakespeare': 708,\n",
       " 'Elizabeth': 709,\n",
       " 'nhac': 710,\n",
       " 'hung': 711,\n",
       " 'phat': 712,\n",
       " 'trien': 713,\n",
       " 'mien': 714,\n",
       " 'Thanh': 715,\n",
       " 'thach': 716,\n",
       " 'nhi': 717,\n",
       " 'nac': 718,\n",
       " 'Boi': 719,\n",
       " 'Cua': 720,\n",
       " 'toa': 721,\n",
       " 'kia': 722,\n",
       " 'Ellen': 723,\n",
       " 'Jorgensen': 724,\n",
       " 'Hack': 725,\n",
       " 'vat': 726,\n",
       " 'Genspace': 727,\n",
       " 'nhuan': 728,\n",
       " 'Brooklyn': 729,\n",
       " 'Khac': 730,\n",
       " 'xau': 731,\n",
       " 'Frankenstein': 732,\n",
       " 'dai': 733,\n",
       " 'nhon': 734,\n",
       " 'DIYbio': 735,\n",
       " 'Thoi': 736,\n",
       " 'Viec': 737,\n",
       " 'DNA': 738,\n",
       " 'de': 739,\n",
       " 're': 740,\n",
       " 'gen': 741,\n",
       " 'euro': 742,\n",
       " 'tiem': 743,\n",
       " 'khia': 744,\n",
       " 'Vay': 745,\n",
       " 'yen': 746,\n",
       " 'Vao': 747,\n",
       " 'khuyen': 748,\n",
       " 'khich': 749,\n",
       " 'Y': 750,\n",
       " 'tot': 751,\n",
       " 'ro': 752,\n",
       " 'xac': 753,\n",
       " 'Do': 754,\n",
       " 'Nen': 755,\n",
       " 'New': 756,\n",
       " 'York': 757,\n",
       " 'lop': 758,\n",
       " 'voc': 759,\n",
       " 'chuan': 760,\n",
       " 'Bao': 761,\n",
       " 'google': 762,\n",
       " 'buon': 763,\n",
       " 'duy': 764,\n",
       " 'gang': 765,\n",
       " 'hacker': 766,\n",
       " 'soi': 767,\n",
       " 'sap': 768,\n",
       " 'Moscow': 769,\n",
       " 'Han': 770,\n",
       " 'Quoc': 771,\n",
       " 'rieng': 772,\n",
       " 'dua': 773,\n",
       " 'chut': 774,\n",
       " 'ap': 775,\n",
       " 'khuan': 776,\n",
       " 'hack': 777,\n",
       " 'mem': 778,\n",
       " 'uot': 779,\n",
       " 'nhay': 780,\n",
       " 'Tinh': 781,\n",
       " 'ninh': 782,\n",
       " 'luoi': 783,\n",
       " 'nano': 784,\n",
       " 'ha': 785,\n",
       " 'Hiep': 786,\n",
       " 'DYI': 787,\n",
       " 'My': 788,\n",
       " 'Chau': 789,\n",
       " 'Gio': 790,\n",
       " 'li': 791,\n",
       " 'rac': 792,\n",
       " 'A': 793,\n",
       " 'sa': 794,\n",
       " 'Sahara': 795,\n",
       " 'An': 796,\n",
       " 'Gen': 797,\n",
       " 'Nhan': 798,\n",
       " 'Havard': 799,\n",
       " 'chap': 800,\n",
       " 'xuong': 801,\n",
       " 'ADN': 802,\n",
       " 'Nao': 803,\n",
       " 'biohacker': 804,\n",
       " 'banh': 805,\n",
       " 'vot': 806,\n",
       " 'Nhat': 807,\n",
       " 'vach': 808,\n",
       " 'sushi': 809,\n",
       " 'coc': 810,\n",
       " 'tha': 811,\n",
       " 'duyet': 812,\n",
       " 'nhiem': 813,\n",
       " 'pin': 814,\n",
       " 'ngoan': 815,\n",
       " 'Vai': 816,\n",
       " 'hao': 817,\n",
       " 'Van': 818,\n",
       " 'thieng': 819,\n",
       " 'lieng': 820,\n",
       " 'huyt': 821,\n",
       " 'Tai': 822,\n",
       " 'TEDxRotterdam': 823,\n",
       " 'Geert': 824,\n",
       " 'Chatrou': 825,\n",
       " 'bieu': 826,\n",
       " 'Eleonora': 827,\n",
       " 'Honhoff': 828,\n",
       " 'Fete': 829,\n",
       " 'Belle': 830,\n",
       " 'huyet': 831,\n",
       " 'tieng': 832,\n",
       " 'tria': 833,\n",
       " 'xoan': 834,\n",
       " 'Ha': 835,\n",
       " 'Lan': 836,\n",
       " 'and': 837,\n",
       " 'quay': 838,\n",
       " 'ray': 839,\n",
       " 'tiec': 840,\n",
       " 'giang': 841,\n",
       " 'Bai': 842,\n",
       " 'loc': 843,\n",
       " 'mui': 844,\n",
       " 'Rudolph': 845,\n",
       " 'bua': 846,\n",
       " 'Chi': 847,\n",
       " 'ruou': 848,\n",
       " 'thua': 849,\n",
       " 'Em': 850,\n",
       " 'chac': 851,\n",
       " 'Louisburg': 852,\n",
       " 'bac': 853,\n",
       " 'Carolina': 854,\n",
       " 'chien': 855,\n",
       " 'judokas': 856,\n",
       " 'gianh': 857,\n",
       " 'Tokyo': 858,\n",
       " 'Rotterdam': 859,\n",
       " 'xinh': 860,\n",
       " 'ep': 861,\n",
       " 'Okay': 862,\n",
       " 'Troi': 863,\n",
       " 'luyen': 864,\n",
       " 'nhe': 865,\n",
       " 'hua': 866,\n",
       " 'hen': 867,\n",
       " 'ga': 868,\n",
       " 'Oh': 869,\n",
       " 'hah': 870,\n",
       " 'Phan': 871,\n",
       " 'Max': 872,\n",
       " 'Westerman': 873,\n",
       " 'Chartrou': 874,\n",
       " 'Roberto': 875,\n",
       " 'D': 876,\n",
       " 'Angelo': 877,\n",
       " 'Francesca': 878,\n",
       " 'Fedeli': 879,\n",
       " 'Mario': 880,\n",
       " 'khoe': 881,\n",
       " 'khoan': 882,\n",
       " 'Be': 883,\n",
       " 'ven': 884,\n",
       " 'thia': 885,\n",
       " 'xoay': 886,\n",
       " 'ruoi': 887,\n",
       " 'duong': 888,\n",
       " 'u': 889,\n",
       " 'Apgar': 890,\n",
       " 'ton': 891,\n",
       " 'Hau': 892,\n",
       " 'gui': 893,\n",
       " 'Giong': 894,\n",
       " 'hoach': 895,\n",
       " 'day': 896,\n",
       " 'khuyet': 897,\n",
       " 'ne': 898,\n",
       " 'neuron': 899,\n",
       " 'Ve': 900,\n",
       " 'sup': 901,\n",
       " 'Video': 902,\n",
       " 'cot': 903,\n",
       " 'Tam': 904,\n",
       " 'Mark': 905,\n",
       " 'Shaw': 906,\n",
       " 'Cuc': 907,\n",
       " 'Kho': 908,\n",
       " 'O': 909,\n",
       " 'Hom': 910,\n",
       " 'sua': 911,\n",
       " 'xi': 912,\n",
       " 'xit': 913,\n",
       " 'hut': 914,\n",
       " 'Rong': 915,\n",
       " 'Giot': 916,\n",
       " 'vua': 917,\n",
       " 'sut': 918,\n",
       " 'nanomet': 919,\n",
       " 'ti': 920,\n",
       " 'hat': 921,\n",
       " 'son': 922,\n",
       " 'bun': 923,\n",
       " 'vuong': 924,\n",
       " 'ria': 925,\n",
       " 'nhuom': 926,\n",
       " 'xanh': 927,\n",
       " 'khe': 928,\n",
       " 'truot': 929,\n",
       " 'ri': 930,\n",
       " 'set': 931,\n",
       " 'Thu': 932,\n",
       " 'Se': 933,\n",
       " 'TED': 934,\n",
       " 'Hai': 935,\n",
       " 'Dan': 936,\n",
       " 'Ariely': 937,\n",
       " 'trom': 938,\n",
       " 'kheo': 939,\n",
       " 'leo': 940,\n",
       " 'Niem': 941,\n",
       " 'bong': 942,\n",
       " 'gap': 943,\n",
       " 'Cach': 944,\n",
       " 'boc': 945,\n",
       " 'giu': 946,\n",
       " 'ghet': 947,\n",
       " 'khoanh': 948,\n",
       " 'toac': 949,\n",
       " 'thiep': 950,\n",
       " 'Hebrew': 951,\n",
       " 'truu': 952,\n",
       " 'Hoac': 953,\n",
       " 'kieu': 954,\n",
       " 'them': 955,\n",
       " 'rut': 956,\n",
       " 'Hoa': 957,\n",
       " 'Le': 958,\n",
       " 'keo': 959,\n",
       " 'Tat': 960,\n",
       " 'Ly': 961,\n",
       " 'Enron': 962,\n",
       " 'bung': 963,\n",
       " 'Lieu': 964,\n",
       " 'rau': 965,\n",
       " 'dich': 966,\n",
       " 'Nhet': 967,\n",
       " 'vun': 968,\n",
       " 'Xet': 969,\n",
       " 'cent': 970,\n",
       " 'The': 971,\n",
       " 'uu': 972,\n",
       " 'Mat': 973,\n",
       " 'Ran': 974,\n",
       " 'Ket': 975,\n",
       " 'nguong': 976,\n",
       " 'ran': 977,\n",
       " 'Khoanh': 978,\n",
       " 'Kinh': 979,\n",
       " 'Danh': 980,\n",
       " 'MIT': 981,\n",
       " 'Thi': 982,\n",
       " 'Coke': 983,\n",
       " 'lanh': 984,\n",
       " 'Nguoc': 985,\n",
       " 'X': 986,\n",
       " 'phieu': 987,\n",
       " 'ft': 988,\n",
       " 'truc': 989,\n",
       " 'but': 990,\n",
       " 'iieu': 991,\n",
       " 'Sinh': 992,\n",
       " 'Nhiem': 993,\n",
       " 'Chuyen': 994,\n",
       " 'Carnegie': 995,\n",
       " 'Mellon': 996,\n",
       " 'Pittsburgh': 997,\n",
       " 'ngot': 998,\n",
       " 'meo': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<SOS>': 0,\n",
       " '<UNK>': 0,\n",
       " '<EOS>': 138953,\n",
       " 'Khoa': 104,\n",
       " 'hoc': 8952,\n",
       " 'ang': 13887,\n",
       " 'sau': 5911,\n",
       " 'mot': 49113,\n",
       " 'tieu': 1943,\n",
       " 'e': 22485,\n",
       " 've': 18759,\n",
       " 'khi': 14456,\n",
       " 'hau': 1552,\n",
       " 'Trong': 1857,\n",
       " 'phut': 884,\n",
       " 'chuyen': 8085,\n",
       " 'gia': 8074,\n",
       " 'hoa': 5193,\n",
       " 'quyen': 2033,\n",
       " 'Rachel': 18,\n",
       " 'Pike': 3,\n",
       " 'gioi': 6504,\n",
       " 'thieu': 1295,\n",
       " 'so': 10381,\n",
       " 'luoc': 231,\n",
       " 'nhung': 44550,\n",
       " 'no': 17845,\n",
       " 'luc': 4834,\n",
       " 'khoa': 2212,\n",
       " 'miet': 18,\n",
       " 'mai': 1096,\n",
       " 'tao': 6492,\n",
       " 'bao': 8197,\n",
       " 'bien': 3299,\n",
       " 'oi': 12582,\n",
       " 'cung': 12828,\n",
       " 'voi': 22470,\n",
       " 'oan': 2548,\n",
       " 'nghien': 2217,\n",
       " 'cuu': 3023,\n",
       " 'cua': 38031,\n",
       " 'minh': 8123,\n",
       " 'hang': 4198,\n",
       " 'ngan': 1884,\n",
       " 'nguoi': 26945,\n",
       " 'a': 25028,\n",
       " 'cong': 10462,\n",
       " 'hien': 6025,\n",
       " 'cho': 21271,\n",
       " 'du': 6441,\n",
       " 'an': 6940,\n",
       " 'nay': 26190,\n",
       " 'bay': 3443,\n",
       " 'mao': 213,\n",
       " 'hiem': 1010,\n",
       " 'qua': 10007,\n",
       " 'rung': 558,\n",
       " 'tim': 4684,\n",
       " 'kiem': 2547,\n",
       " 'thong': 6853,\n",
       " 'tin': 4917,\n",
       " 'phan': 8197,\n",
       " 'tu': 19549,\n",
       " 'then': 62,\n",
       " 'chot': 277,\n",
       " 'Toi': 12741,\n",
       " 'muon': 6838,\n",
       " 'cac': 22542,\n",
       " 'ban': 34074,\n",
       " 'biet': 10658,\n",
       " 'su': 23898,\n",
       " 'to': 3528,\n",
       " 'lon': 5039,\n",
       " 'gop': 398,\n",
       " 'lam': 19181,\n",
       " 'nen': 7571,\n",
       " 'dong': 521,\n",
       " 'tit': 38,\n",
       " 'thuong': 4563,\n",
       " 'thay': 14567,\n",
       " 'tren': 8560,\n",
       " 'Co': 3612,\n",
       " 'trong': 29535,\n",
       " 'nhu': 16024,\n",
       " 'the': 38785,\n",
       " 'va': 48278,\n",
       " 'noi': 19934,\n",
       " 'chat': 2517,\n",
       " 'luong': 3640,\n",
       " 'khong': 31210,\n",
       " 'hay': 7100,\n",
       " 'khoi': 2660,\n",
       " 'bui': 126,\n",
       " 'Ca': 239,\n",
       " 'hai': 4585,\n",
       " 'eu': 2989,\n",
       " 'la': 59970,\n",
       " 'nhanh': 1334,\n",
       " 'linh': 1168,\n",
       " 'vuc': 1310,\n",
       " 'nganh': 740,\n",
       " 'Cac': 1735,\n",
       " 'gan': 2457,\n",
       " 'ay': 21298,\n",
       " 'Ban': 3966,\n",
       " 'ieu': 17121,\n",
       " 'hanh': 4124,\n",
       " 'Bien': 53,\n",
       " 'Lien': 198,\n",
       " 'chinh': 6603,\n",
       " 'phu': 3680,\n",
       " 'goi': 3227,\n",
       " 'tat': 5524,\n",
       " 'IPCC': 8,\n",
       " 'ua': 4673,\n",
       " 'ra': 18813,\n",
       " 'bai': 2749,\n",
       " 'ho': 14615,\n",
       " 'he': 4728,\n",
       " 'Nghien': 82,\n",
       " 'uoc': 23127,\n",
       " 'viet': 1549,\n",
       " 'boi': 5552,\n",
       " 'nha': 6960,\n",
       " 'quoc': 1511,\n",
       " 'khac': 9407,\n",
       " 'nhau': 4007,\n",
       " 'Ho': 2494,\n",
       " 'trang': 2376,\n",
       " 'chu': 4243,\n",
       " 'Va': 18770,\n",
       " 'ca': 11753,\n",
       " 'xem': 3246,\n",
       " 'xet': 819,\n",
       " 'phe': 371,\n",
       " 'binh': 2160,\n",
       " '': 20932,\n",
       " 'o': 55495,\n",
       " 'ong': 16881,\n",
       " 'en': 13060,\n",
       " 'thuc': 13512,\n",
       " 'te': 5461,\n",
       " 'cuoc': 5337,\n",
       " 'hoi': 8726,\n",
       " 'nam': 9405,\n",
       " 'chung': 40418,\n",
       " 'toi': 58865,\n",
       " 'nghi': 8815,\n",
       " 'nhien': 3854,\n",
       " 'nhat': 6765,\n",
       " 'Moi': 1003,\n",
       " 'hon': 10825,\n",
       " 'San': 168,\n",
       " 'Francisco': 68,\n",
       " 'tham': 3473,\n",
       " 'thuoc': 2328,\n",
       " 'nhom': 1369,\n",
       " 'moi': 14052,\n",
       " 'rat': 10138,\n",
       " 'nhieu': 10510,\n",
       " 'tai': 8600,\n",
       " 'dang': 1993,\n",
       " 'Voi': 492,\n",
       " 'Cambridge': 46,\n",
       " 'dao': 248,\n",
       " 'El': 28,\n",
       " 'Nino': 9,\n",
       " 'von': 353,\n",
       " 'co': 59782,\n",
       " 'tac': 3143,\n",
       " 'thoi': 5844,\n",
       " 'tiet': 923,\n",
       " 'tinh': 7836,\n",
       " 'thai': 1411,\n",
       " 'canh': 2893,\n",
       " 'lieu': 3402,\n",
       " 'sinh': 4352,\n",
       " 'lai': 12050,\n",
       " 'chia': 1440,\n",
       " 'nho': 4606,\n",
       " 'bang': 6149,\n",
       " 'tien': 7594,\n",
       " 'si': 1424,\n",
       " 'phai': 11543,\n",
       " 'vo': 2786,\n",
       " 'cu': 4637,\n",
       " 'chi': 11290,\n",
       " 'vai': 3559,\n",
       " 'quy': 1992,\n",
       " 'trinh': 3819,\n",
       " 'Mot': 2947,\n",
       " 'ten': 1623,\n",
       " 'isoprene': 4,\n",
       " 'No': 3931,\n",
       " 'huu': 842,\n",
       " 'chua': 3250,\n",
       " 'tung': 2648,\n",
       " 'nghe': 5300,\n",
       " 'chiec': 2454,\n",
       " 'kep': 79,\n",
       " 'giay': 1212,\n",
       " 'vao': 10897,\n",
       " 'khoang': 2354,\n",
       " 'zeta': 1,\n",
       " 'illion': 1,\n",
       " 'mu': 341,\n",
       " 'Du': 333,\n",
       " 'ngang': 246,\n",
       " 'ngua': 251,\n",
       " 'tong': 808,\n",
       " 'dan': 5040,\n",
       " 'toan': 5133,\n",
       " 'cau': 8547,\n",
       " 'lo': 2080,\n",
       " 'metan': 19,\n",
       " 'Chinh': 326,\n",
       " 'vi': 14440,\n",
       " 'y': 5421,\n",
       " 'nghia': 2706,\n",
       " 'quan': 8275,\n",
       " 'nao': 10327,\n",
       " 'theo': 4835,\n",
       " 'uoi': 483,\n",
       " 'Chung': 8239,\n",
       " 'manh': 2211,\n",
       " 'Phong': 101,\n",
       " 'EUPHORE': 1,\n",
       " 'Tay': 563,\n",
       " 'Nha': 468,\n",
       " 'chay': 1475,\n",
       " 'hoan': 2708,\n",
       " 'dien': 3359,\n",
       " 'cham': 1754,\n",
       " 'lan': 4147,\n",
       " 'ung': 6609,\n",
       " 'xe': 2171,\n",
       " 'vay': 6229,\n",
       " 'van': 9597,\n",
       " 'mo': 4380,\n",
       " 'hinh': 5636,\n",
       " 'sieu': 454,\n",
       " 'may': 4761,\n",
       " 'viec': 10797,\n",
       " 'Mo': 72,\n",
       " 'gom': 642,\n",
       " 'tram': 1401,\n",
       " 'thung': 236,\n",
       " 'xep': 496,\n",
       " 'chong': 1354,\n",
       " 'gian': 5215,\n",
       " 'cuc': 1467,\n",
       " 'Ma': 282,\n",
       " 'can': 8034,\n",
       " 'tuan': 896,\n",
       " 'xong': 192,\n",
       " 'phep': 1323,\n",
       " 'tich': 1113,\n",
       " 'ta': 30938,\n",
       " 'hieu': 4703,\n",
       " 'gi': 10823,\n",
       " 'xay': 4125,\n",
       " 'con': 14136,\n",
       " 'khap': 907,\n",
       " 'Gan': 137,\n",
       " 'khao': 330,\n",
       " 'sat': 1303,\n",
       " 'ia': 1197,\n",
       " 'Malaysia': 13,\n",
       " 'Con': 951,\n",
       " 'nua': 3888,\n",
       " 'thap': 1390,\n",
       " 'ngay': 6347,\n",
       " 'giua': 1957,\n",
       " 'treo': 140,\n",
       " 'thiet': 3387,\n",
       " 'bi': 7405,\n",
       " 'tri': 4471,\n",
       " 'xa': 3331,\n",
       " 'cai': 9329,\n",
       " 'thu': 14780,\n",
       " 'suot': 866,\n",
       " 'nhin': 4773,\n",
       " 'cao': 2839,\n",
       " 'duoi': 1394,\n",
       " 'at': 5641,\n",
       " 'giai': 3577,\n",
       " 'mang': 3728,\n",
       " 'Chiec': 98,\n",
       " 'phi': 1438,\n",
       " 'mau': 3121,\n",
       " 'BA': 1,\n",
       " 'do': 3205,\n",
       " 'FAAM': 1,\n",
       " 'Rat': 357,\n",
       " 'tuong': 8745,\n",
       " 'hom': 1208,\n",
       " 'cach': 10108,\n",
       " 'tang': 2782,\n",
       " 'vom': 52,\n",
       " 'met': 313,\n",
       " 'ac': 1878,\n",
       " 'nguy': 636,\n",
       " 'nghieng': 43,\n",
       " 'Phai': 203,\n",
       " 'thue': 302,\n",
       " 'hach': 77,\n",
       " 'khien': 1839,\n",
       " 'xin': 700,\n",
       " 'lenh': 139,\n",
       " 'Khi': 1981,\n",
       " 'quanh': 1327,\n",
       " 'bo': 8030,\n",
       " 'song': 6321,\n",
       " 'lung': 384,\n",
       " 'len': 4659,\n",
       " 'G': 76,\n",
       " 'that': 5325,\n",
       " 'ghe': 430,\n",
       " 'Vi': 3294,\n",
       " 'dung': 9874,\n",
       " 'ben': 2791,\n",
       " 'giong': 3194,\n",
       " 'bat': 8192,\n",
       " 'ky': 3965,\n",
       " 'lich': 936,\n",
       " 'phong': 3471,\n",
       " 'thi': 7927,\n",
       " 'nghiem': 2972,\n",
       " 'di': 1845,\n",
       " 'giup': 2152,\n",
       " 'thich': 3204,\n",
       " 'ai': 6126,\n",
       " 'loai': 4494,\n",
       " 'se': 16615,\n",
       " 'ngoai': 2483,\n",
       " 'kien': 2515,\n",
       " 'inh': 5435,\n",
       " 'thanh': 8586,\n",
       " 'muc': 2462,\n",
       " 'mac': 1432,\n",
       " 'chuong': 1061,\n",
       " 'Noi': 426,\n",
       " 'anh': 10406,\n",
       " 'luon': 1675,\n",
       " 'kem': 471,\n",
       " 'tom': 223,\n",
       " 'oc': 2160,\n",
       " 'sach': 2088,\n",
       " 'Cam': 1238,\n",
       " 'on': 5295,\n",
       " 'Christopher': 22,\n",
       " 'deCharms': 2,\n",
       " 'quet': 150,\n",
       " 'than': 3691,\n",
       " 'kinh': 3932,\n",
       " 'sang': 4019,\n",
       " 'che': 1755,\n",
       " 'fMRI': 7,\n",
       " 'ghi': 713,\n",
       " 'hoat': 1826,\n",
       " 'suy': 1457,\n",
       " 'cam': 5687,\n",
       " 'xuc': 816,\n",
       " 'au': 12578,\n",
       " 'nhan': 10510,\n",
       " 'Xin': 715,\n",
       " 'chao': 265,\n",
       " 'gio': 5978,\n",
       " 'tay': 2159,\n",
       " 'phia': 1190,\n",
       " 'hoang': 767,\n",
       " 'Bat': 232,\n",
       " 'chuoc': 120,\n",
       " 'lap': 2255,\n",
       " 'bap': 97,\n",
       " 'som': 305,\n",
       " 'vung': 1679,\n",
       " 'ma': 16935,\n",
       " 'Vang': 852,\n",
       " 'buoc': 1415,\n",
       " 'quyet': 2327,\n",
       " 'kho': 2317,\n",
       " 'thuyen': 208,\n",
       " 'mach': 392,\n",
       " 'bach': 227,\n",
       " 'tan': 1926,\n",
       " 'Nhung': 8583,\n",
       " 'nghiep': 1676,\n",
       " 'Peter': 63,\n",
       " 'xam': 315,\n",
       " 'nhap': 837,\n",
       " 'MRI': 55,\n",
       " 'Khong': 1880,\n",
       " 'bom': 304,\n",
       " 'tia': 198,\n",
       " 'buc': 1702,\n",
       " 'hop': 4122,\n",
       " 'tam': 4721,\n",
       " 'vang': 665,\n",
       " 'be': 2537,\n",
       " 'mat': 6917,\n",
       " 'Truoc': 293,\n",
       " 'ien': 2996,\n",
       " 'robot': 360,\n",
       " 'chup': 774,\n",
       " 'Cai': 695,\n",
       " 'chiem': 361,\n",
       " 'hoac': 2412,\n",
       " 'thang': 2751,\n",
       " 'mili': 8,\n",
       " 'Anh': 2078,\n",
       " 'iem': 2789,\n",
       " 'kich': 1083,\n",
       " 'Neu': 2237,\n",
       " 'ba': 3035,\n",
       " 'huong': 3177,\n",
       " 'giuong': 151,\n",
       " 'vien': 3799,\n",
       " 'lua': 1313,\n",
       " 'chon': 1630,\n",
       " 'Ta': 383,\n",
       " 'rang': 12026,\n",
       " 'kenh': 143,\n",
       " 'nien': 500,\n",
       " 'giat': 128,\n",
       " 'neu': 5494,\n",
       " 'dut': 132,\n",
       " 'vong': 2675,\n",
       " 'san': 3652,\n",
       " 'xuat': 2063,\n",
       " 'xung': 1125,\n",
       " 'chieu': 1130,\n",
       " 'Bay': 1379,\n",
       " 'uong': 3373,\n",
       " 'i': 7619,\n",
       " 'benh': 2836,\n",
       " 'shock': 18,\n",
       " 'doi': 762,\n",
       " 'gay': 1468,\n",
       " 'tap': 2995,\n",
       " 'xoa': 202,\n",
       " 'diu': 54,\n",
       " 'goc': 918,\n",
       " 'trai': 3130,\n",
       " 'ket': 3822,\n",
       " 'phuong': 1997,\n",
       " 'phap': 1783,\n",
       " 'giam': 1435,\n",
       " 'tran': 950,\n",
       " 'ki': 2262,\n",
       " 'uc': 1157,\n",
       " 'danh': 1596,\n",
       " 'tue': 156,\n",
       " 'Beeban': 2,\n",
       " 'Kidron': 2,\n",
       " 'dieu': 410,\n",
       " 'phim': 1279,\n",
       " 'suc': 1821,\n",
       " 'thuat': 2435,\n",
       " 'ao': 3197,\n",
       " 'Phep': 11,\n",
       " 'Milan': 6,\n",
       " 'khu': 1690,\n",
       " 'ke': 5100,\n",
       " 'FILMCLUB': 3,\n",
       " 'lu': 241,\n",
       " 'tre': 3163,\n",
       " 'Bang': 308,\n",
       " 'tuoi': 2154,\n",
       " 'Tu': 577,\n",
       " 'me': 2404,\n",
       " 'gai': 951,\n",
       " 'thuyet': 1278,\n",
       " 'giao': 3199,\n",
       " 'khan': 1162,\n",
       " 'thinh': 373,\n",
       " 'Internet': 422,\n",
       " 'ngon': 1371,\n",
       " 'nang': 5142,\n",
       " 'coi': 726,\n",
       " 'trao': 604,\n",
       " 'trung': 3066,\n",
       " 'loi': 4429,\n",
       " 'liet': 406,\n",
       " 'sac': 900,\n",
       " 'qui': 232,\n",
       " 'truyen': 2175,\n",
       " 'tranh': 2382,\n",
       " 'vuot': 655,\n",
       " 'ranh': 173,\n",
       " 'ngu': 2083,\n",
       " 'triet': 265,\n",
       " 'ly': 3783,\n",
       " 'Thuc': 699,\n",
       " 'rong': 1141,\n",
       " 'Hollywood': 37,\n",
       " 'phuc': 2491,\n",
       " 'vu': 2743,\n",
       " 'kieng': 65,\n",
       " 'giac': 1504,\n",
       " 'quen': 845,\n",
       " 'truoc': 4201,\n",
       " 'La': 337,\n",
       " 'ngai': 449,\n",
       " 'reo': 21,\n",
       " 'Chua': 460,\n",
       " 'Tuong': 199,\n",
       " 'it': 1685,\n",
       " 'That': 713,\n",
       " 'mia': 32,\n",
       " 'yeu': 2860,\n",
       " 'chuc': 1707,\n",
       " 'truong': 4651,\n",
       " 'thao': 744,\n",
       " 'luan': 1182,\n",
       " 'tra': 2739,\n",
       " 'soat': 546,\n",
       " 'le': 2852,\n",
       " 'ngung': 380,\n",
       " 'tiep': 3367,\n",
       " 'thon': 145,\n",
       " 'DVD': 35,\n",
       " 'lac': 751,\n",
       " 'doc': 247,\n",
       " 'nuoc': 4186,\n",
       " 'em': 3248,\n",
       " 'ngat': 83,\n",
       " 'quang': 931,\n",
       " 'mon': 913,\n",
       " 'giau': 549,\n",
       " 'cap': 2219,\n",
       " 'tuc': 2095,\n",
       " 'duc': 1098,\n",
       " 'tuy': 524,\n",
       " 'kha': 3226,\n",
       " 'kham': 835,\n",
       " 'pha': 1570,\n",
       " 'Ngay': 574,\n",
       " 'mong': 924,\n",
       " 'Bo': 435,\n",
       " 'Vittorio': 1,\n",
       " 'De': 28,\n",
       " 'Sica': 2,\n",
       " 'chuot': 420,\n",
       " 'ngheo': 590,\n",
       " 'khat': 188,\n",
       " 'dip': 59,\n",
       " 'Cong': 499,\n",
       " 'rap': 261,\n",
       " 'in': 396,\n",
       " 'cha': 686,\n",
       " 'Su': 754,\n",
       " 'mung': 233,\n",
       " 'teen': 18,\n",
       " 'niem': 1304,\n",
       " 'hy': 551,\n",
       " 'cuoi': 2420,\n",
       " 'bau': 529,\n",
       " 'troi': 1265,\n",
       " 'cay': 1610,\n",
       " 'choi': 2046,\n",
       " 'Sau': 823,\n",
       " 'muoi': 361,\n",
       " 'guong': 180,\n",
       " 'ngac': 824,\n",
       " 'ngo': 711,\n",
       " 'toc': 1041,\n",
       " 'lien': 2237,\n",
       " 'Trieu': 31,\n",
       " 'pho': 2276,\n",
       " 'favela': 3,\n",
       " 'Rio': 52,\n",
       " 'mua': 1524,\n",
       " 'Cau': 663,\n",
       " 'Ong': 1150,\n",
       " 'Smith': 62,\n",
       " 'Washington': 101,\n",
       " 'het': 2261,\n",
       " 'Frank': 51,\n",
       " 'Capra': 1,\n",
       " 'tro': 6675,\n",
       " 'long': 1447,\n",
       " 'nguon': 1244,\n",
       " 'lau': 914,\n",
       " 'buoi': 874,\n",
       " 'luat': 951,\n",
       " 'Toa': 56,\n",
       " 'vui': 930,\n",
       " 'sao': 4549,\n",
       " 'nguyen': 1806,\n",
       " 'Jimmy': 14,\n",
       " 'Stewart': 31,\n",
       " 'Khach': 17,\n",
       " 'Rwanda': 60,\n",
       " 'bon': 899,\n",
       " 'diet': 222,\n",
       " 'giot': 66,\n",
       " 'thuy': 283,\n",
       " 'gat': 115,\n",
       " 'Schindler': 3,\n",
       " 'roi': 4547,\n",
       " 'Ke': 143,\n",
       " 'moc': 358,\n",
       " 'tui': 304,\n",
       " 'tuoc': 72,\n",
       " 'pham': 2495,\n",
       " 'Gui': 23,\n",
       " 'men': 153,\n",
       " 'ot': 931,\n",
       " 'Briton': 1,\n",
       " 'da': 802,\n",
       " 'chui': 54,\n",
       " 'rua': 139,\n",
       " 'ngoi': 1766,\n",
       " 'Sidney': 4,\n",
       " 'Potier': 1,\n",
       " 'lay': 2106,\n",
       " 'xuoi': 26,\n",
       " 'am': 3586,\n",
       " 'cang': 1463,\n",
       " 'ganh': 113,\n",
       " 'vinh': 282,\n",
       " 'trieu': 1504,\n",
       " 'Mac': 151,\n",
       " 'cat': 879,\n",
       " 'Persepolis': 2,\n",
       " 'Iran': 188,\n",
       " 'Ham': 13,\n",
       " 'map': 214,\n",
       " 'giet': 519,\n",
       " 'chet': 1473,\n",
       " 'nem': 209,\n",
       " 'man': 1126,\n",
       " 'tau': 545,\n",
       " 'Ai': 369,\n",
       " 'sai': 987,\n",
       " 'iep': 250,\n",
       " 'dau': 1227,\n",
       " 'Lam': 645,\n",
       " 'mieng': 386,\n",
       " 'chang': 1735,\n",
       " 'tuyet': 2412,\n",
       " 'chan': 2780,\n",
       " 'nui': 341,\n",
       " 'cuop': 89,\n",
       " 'Kha': 94,\n",
       " 'Israel': 160,\n",
       " 'loan': 370,\n",
       " 'Me': 224,\n",
       " 'chau': 1066,\n",
       " 'Au': 418,\n",
       " 'nan': 815,\n",
       " 'kim': 363,\n",
       " 'cuong': 439,\n",
       " 'khau': 563,\n",
       " 'tron': 629,\n",
       " 'Luan': 45,\n",
       " 'im': 121,\n",
       " 'lang': 1734,\n",
       " 'Anne': 13,\n",
       " 'thoat': 309,\n",
       " 'Shoah': 1,\n",
       " 'Chien': 103,\n",
       " 'Will': 10,\n",
       " 'Leni': 1,\n",
       " 'Riefenstahl': 1,\n",
       " 'Nazi': 4,\n",
       " 'chiu': 708,\n",
       " 'ich': 1341,\n",
       " 'sot': 570,\n",
       " 'thoang': 200,\n",
       " 'xuyen': 463,\n",
       " 'Nguoi': 1050,\n",
       " 'thuan': 514,\n",
       " 'xua': 189,\n",
       " 'tho': 1428,\n",
       " 'thien': 1465,\n",
       " 'cuon': 994,\n",
       " 'Nhu': 671,\n",
       " 'Phu': 96,\n",
       " 'xu': 1073,\n",
       " 'Oz': 12,\n",
       " 'Hay': 1789,\n",
       " 'Kane': 4,\n",
       " 'Jane': 25,\n",
       " 'Austen': 6,\n",
       " 'Tennyson': 1,\n",
       " 'khung': 981,\n",
       " 'thau': 238,\n",
       " 'phoi': 552,\n",
       " 'gach': 59,\n",
       " 'Tom': 93,\n",
       " 'Hanks': 2,\n",
       " 'tru': 1692,\n",
       " 'Jim': 40,\n",
       " 'Lovell': 1,\n",
       " 'khuon': 412,\n",
       " 'Ben': 144,\n",
       " 'Kingley': 1,\n",
       " 'Gandhi': 32,\n",
       " 'Eve': 24,\n",
       " 'Harrington': 1,\n",
       " 'Howard': 51,\n",
       " 'Beale': 1,\n",
       " 'Mildred': 5,\n",
       " 'Pierce': 1,\n",
       " 'bot': 452,\n",
       " 'Shakespeare': 35,\n",
       " 'Elizabeth': 29,\n",
       " 'nhac': 2015,\n",
       " 'hung': 1002,\n",
       " 'phat': 4316,\n",
       " 'trien': 2401,\n",
       " 'mien': 543,\n",
       " 'Thanh': 199,\n",
       " 'thach': 739,\n",
       " 'nhi': 282,\n",
       " 'nac': 58,\n",
       " 'Boi': 1022,\n",
       " 'Cua': 44,\n",
       " 'toa': 797,\n",
       " 'kia': 1147,\n",
       " 'Ellen': 11,\n",
       " 'Jorgensen': 2,\n",
       " 'Hack': 1,\n",
       " 'vat': 3818,\n",
       " 'Genspace': 5,\n",
       " 'nhuan': 244,\n",
       " 'Brooklyn': 30,\n",
       " 'Khac': 25,\n",
       " 'xau': 658,\n",
       " 'Frankenstein': 6,\n",
       " 'dai': 1331,\n",
       " 'nhon': 83,\n",
       " 'DIYbio': 4,\n",
       " 'Thoi': 196,\n",
       " 'Viec': 265,\n",
       " 'DNA': 147,\n",
       " 'de': 1381,\n",
       " 're': 454,\n",
       " 'gen': 407,\n",
       " 'euro': 13,\n",
       " 'tiem': 580,\n",
       " 'khia': 207,\n",
       " 'Vay': 2661,\n",
       " 'yen': 231,\n",
       " 'Vao': 355,\n",
       " 'khuyen': 321,\n",
       " 'khich': 312,\n",
       " 'Y': 714,\n",
       " 'tot': 3790,\n",
       " 'ro': 1881,\n",
       " 'xac': 1314,\n",
       " 'Do': 508,\n",
       " 'Nen': 746,\n",
       " 'New': 550,\n",
       " 'York': 386,\n",
       " 'lop': 932,\n",
       " 'voc': 23,\n",
       " 'chuan': 713,\n",
       " 'Bao': 251,\n",
       " 'google': 12,\n",
       " 'buon': 493,\n",
       " 'duy': 1167,\n",
       " 'gang': 1389,\n",
       " 'hacker': 68,\n",
       " 'soi': 426,\n",
       " 'sap': 734,\n",
       " 'Moscow': 9,\n",
       " 'Han': 189,\n",
       " 'Quoc': 1144,\n",
       " 'rieng': 978,\n",
       " 'dua': 725,\n",
       " 'chut': 1363,\n",
       " 'ap': 1581,\n",
       " 'khuan': 330,\n",
       " 'hack': 15,\n",
       " 'mem': 391,\n",
       " 'uot': 43,\n",
       " 'nhay': 652,\n",
       " 'Tinh': 178,\n",
       " 'ninh': 139,\n",
       " 'luoi': 600,\n",
       " 'nano': 62,\n",
       " 'ha': 656,\n",
       " 'Hiep': 94,\n",
       " 'DYI': 1,\n",
       " 'My': 1717,\n",
       " 'Chau': 644,\n",
       " 'Gio': 698,\n",
       " 'li': 631,\n",
       " 'rac': 426,\n",
       " 'A': 709,\n",
       " 'sa': 164,\n",
       " 'Sahara': 65,\n",
       " 'An': 712,\n",
       " 'Gen': 13,\n",
       " 'Nhan': 178,\n",
       " 'Havard': 21,\n",
       " 'chap': 544,\n",
       " 'xuong': 1817,\n",
       " 'ADN': 206,\n",
       " 'Nao': 278,\n",
       " 'biohacker': 2,\n",
       " 'banh': 717,\n",
       " 'vot': 66,\n",
       " 'Nhat': 284,\n",
       " 'vach': 100,\n",
       " 'sushi': 7,\n",
       " 'coc': 235,\n",
       " 'tha': 291,\n",
       " 'duyet': 98,\n",
       " 'nhiem': 1330,\n",
       " 'pin': 144,\n",
       " 'ngoan': 140,\n",
       " 'Vai': 140,\n",
       " 'hao': 703,\n",
       " 'Van': 467,\n",
       " 'thieng': 53,\n",
       " 'lieng': 42,\n",
       " 'huyt': 48,\n",
       " 'Tai': 1098,\n",
       " 'TEDxRotterdam': 2,\n",
       " 'Geert': 3,\n",
       " 'Chatrou': 2,\n",
       " 'bieu': 1190,\n",
       " 'Eleonora': 1,\n",
       " 'Honhoff': 1,\n",
       " 'Fete': 2,\n",
       " 'Belle': 2,\n",
       " 'huyet': 171,\n",
       " 'tieng': 1937,\n",
       " 'tria': 3,\n",
       " 'xoan': 58,\n",
       " 'Ha': 144,\n",
       " 'Lan': 297,\n",
       " 'and': 129,\n",
       " 'quay': 1344,\n",
       " 'ray': 85,\n",
       " 'tiec': 275,\n",
       " 'giang': 293,\n",
       " 'Bai': 126,\n",
       " 'loc': 288,\n",
       " 'mui': 430,\n",
       " 'Rudolph': 1,\n",
       " 'bua': 391,\n",
       " 'Chi': 600,\n",
       " 'ruou': 192,\n",
       " 'thua': 465,\n",
       " 'Em': 211,\n",
       " 'chac': 1221,\n",
       " 'Louisburg': 1,\n",
       " 'bac': 1230,\n",
       " 'Carolina': 35,\n",
       " 'chien': 1853,\n",
       " 'judokas': 1,\n",
       " 'gianh': 171,\n",
       " 'Tokyo': 25,\n",
       " 'Rotterdam': 6,\n",
       " 'xinh': 127,\n",
       " 'ep': 1373,\n",
       " 'Okay': 54,\n",
       " 'Troi': 118,\n",
       " 'luyen': 429,\n",
       " 'nhe': 546,\n",
       " 'hua': 148,\n",
       " 'hen': 206,\n",
       " 'ga': 376,\n",
       " 'Oh': 94,\n",
       " 'hah': 1,\n",
       " 'Phan': 257,\n",
       " 'Max': 10,\n",
       " 'Westerman': 1,\n",
       " 'Chartrou': 1,\n",
       " 'Roberto': 2,\n",
       " 'D': 251,\n",
       " 'Angelo': 6,\n",
       " 'Francesca': 4,\n",
       " 'Fedeli': 3,\n",
       " 'Mario': 18,\n",
       " 'khoe': 937,\n",
       " 'khoan': 579,\n",
       " 'Be': 27,\n",
       " 'ven': 123,\n",
       " 'thia': 7,\n",
       " 'xoay': 293,\n",
       " 'ruoi': 267,\n",
       " 'duong': 1215,\n",
       " 'u': 1592,\n",
       " 'Apgar': 1,\n",
       " 'ton': 2163,\n",
       " 'Hau': 184,\n",
       " 'gui': 698,\n",
       " 'Giong': 219,\n",
       " 'hoach': 536,\n",
       " 'day': 2177,\n",
       " 'khuyet': 92,\n",
       " 'ne': 132,\n",
       " 'neuron': 18,\n",
       " 'Ve': 233,\n",
       " 'sup': 259,\n",
       " 'Video': 14,\n",
       " 'cot': 349,\n",
       " 'Tam': 130,\n",
       " 'Mark': 64,\n",
       " 'Shaw': 7,\n",
       " 'Cuc': 182,\n",
       " 'Kho': 48,\n",
       " 'O': 1188,\n",
       " 'Hom': 183,\n",
       " 'sua': 541,\n",
       " 'xi': 180,\n",
       " 'xit': 43,\n",
       " 'hut': 494,\n",
       " 'Rong': 12,\n",
       " 'Giot': 2,\n",
       " 'vua': 1180,\n",
       " 'sut': 41,\n",
       " 'nanomet': 4,\n",
       " 'ti': 1154,\n",
       " 'hat': 1195,\n",
       " 'son': 136,\n",
       " 'bun': 37,\n",
       " 'vuong': 314,\n",
       " 'ria': 89,\n",
       " 'nhuom': 43,\n",
       " 'xanh': 701,\n",
       " 'khe': 47,\n",
       " 'truot': 117,\n",
       " 'ri': 91,\n",
       " 'set': 81,\n",
       " 'Thu': 563,\n",
       " 'Se': 190,\n",
       " 'TED': 689,\n",
       " 'Hai': 324,\n",
       " 'Dan': 203,\n",
       " 'Ariely': 9,\n",
       " 'trom': 72,\n",
       " 'kheo': 83,\n",
       " 'leo': 242,\n",
       " 'Niem': 26,\n",
       " 'bong': 1221,\n",
       " 'gap': 1912,\n",
       " 'Cach': 280,\n",
       " 'boc': 252,\n",
       " 'giu': 1000,\n",
       " 'ghet': 191,\n",
       " 'khoanh': 273,\n",
       " 'toac': 5,\n",
       " 'thiep': 213,\n",
       " 'Hebrew': 5,\n",
       " 'truu': 75,\n",
       " 'Hoac': 221,\n",
       " 'kieu': 888,\n",
       " 'them': 1343,\n",
       " 'rut': 355,\n",
       " 'Hoa': 709,\n",
       " 'Le': 50,\n",
       " 'keo': 693,\n",
       " 'Tat': 843,\n",
       " 'Ly': 167,\n",
       " 'Enron': 7,\n",
       " 'bung': 289,\n",
       " 'Lieu': 367,\n",
       " 'rau': 169,\n",
       " 'dich': 1323,\n",
       " 'Nhet': 1,\n",
       " 'vun': 76,\n",
       " 'Xet': 22,\n",
       " 'cent': 43,\n",
       " 'The': 1529,\n",
       " 'uu': 247,\n",
       " 'Mat': 231,\n",
       " 'Ran': 12,\n",
       " 'Ket': 140,\n",
       " 'nguong': 206,\n",
       " 'ran': 141,\n",
       " 'Khoanh': 13,\n",
       " 'Kinh': 181,\n",
       " 'Danh': 26,\n",
       " 'MIT': 93,\n",
       " 'Thi': 134,\n",
       " 'Coke': 9,\n",
       " 'lanh': 1284,\n",
       " 'Nguoc': 45,\n",
       " 'X': 206,\n",
       " 'phieu': 262,\n",
       " 'ft': 14,\n",
       " 'truc': 1621,\n",
       " 'but': 115,\n",
       " 'iieu': 1,\n",
       " 'Sinh': 75,\n",
       " 'Nhiem': 27,\n",
       " 'Chuyen': 209,\n",
       " 'Carnegie': 24,\n",
       " 'Mellon': 12,\n",
       " 'Pittsburgh': 12,\n",
       " 'ngot': 188,\n",
       " 'meo': 203,\n",
       " ...}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<SOS>': 1,\n",
       " '<UNK>': 2,\n",
       " '<EOS>': 3,\n",
       " 'Rachel': 4,\n",
       " 'Pike': 5,\n",
       " 'The': 6,\n",
       " 'science': 7,\n",
       " 'behind': 8,\n",
       " 'a': 9,\n",
       " 'climate': 10,\n",
       " 'headline': 11,\n",
       " 'In': 12,\n",
       " 'minutes': 13,\n",
       " 'atmospheric': 14,\n",
       " 'chemist': 15,\n",
       " 'provides': 16,\n",
       " 'glimpse': 17,\n",
       " 'of': 18,\n",
       " 'the': 19,\n",
       " 'massive': 20,\n",
       " 'scientific': 21,\n",
       " 'effort': 22,\n",
       " 'bold': 23,\n",
       " 'headlines': 24,\n",
       " 'on': 25,\n",
       " 'change': 26,\n",
       " 'with': 27,\n",
       " 'her': 28,\n",
       " 'team': 29,\n",
       " 'one': 30,\n",
       " 'thousands': 31,\n",
       " 'who': 32,\n",
       " 'contributed': 33,\n",
       " 'taking': 34,\n",
       " 'risky': 35,\n",
       " 'flight': 36,\n",
       " 'over': 37,\n",
       " 'rainforest': 38,\n",
       " 'in': 39,\n",
       " 'pursuit': 40,\n",
       " 'data': 41,\n",
       " 'key': 42,\n",
       " 'molecule': 43,\n",
       " 'I': 44,\n",
       " 'apos': 45,\n",
       " 'd': 46,\n",
       " 'like': 47,\n",
       " 'to': 48,\n",
       " 'talk': 49,\n",
       " 'you': 50,\n",
       " 'today': 51,\n",
       " 'about': 52,\n",
       " 'scale': 53,\n",
       " 'that': 54,\n",
       " 'goes': 55,\n",
       " 'into': 56,\n",
       " 'making': 57,\n",
       " 'see': 58,\n",
       " 'paper': 59,\n",
       " 'Headlines': 60,\n",
       " 'look': 61,\n",
       " 'this': 62,\n",
       " 'when': 63,\n",
       " 'they': 64,\n",
       " 'have': 65,\n",
       " 'do': 66,\n",
       " 'and': 67,\n",
       " 'air': 68,\n",
       " 'quality': 69,\n",
       " 'or': 70,\n",
       " 'smog': 71,\n",
       " 'They': 72,\n",
       " 'are': 73,\n",
       " 'both': 74,\n",
       " 'two': 75,\n",
       " 'branches': 76,\n",
       " 'same': 77,\n",
       " 'field': 78,\n",
       " 'Recently': 79,\n",
       " 'looked': 80,\n",
       " 'Intergovernmental': 81,\n",
       " 'Panel': 82,\n",
       " 'Climate': 83,\n",
       " 'Change': 84,\n",
       " 'IPCC': 85,\n",
       " 'put': 86,\n",
       " 'out': 87,\n",
       " 'their': 88,\n",
       " 'report': 89,\n",
       " 'state': 90,\n",
       " 'understanding': 91,\n",
       " 'system': 92,\n",
       " 'That': 93,\n",
       " 'was': 94,\n",
       " 'written': 95,\n",
       " 'by': 96,\n",
       " 'scientists': 97,\n",
       " 'from': 98,\n",
       " 'countries': 99,\n",
       " 'wrote': 100,\n",
       " 'almost': 101,\n",
       " 'thousand': 102,\n",
       " 'pages': 103,\n",
       " 'topic': 104,\n",
       " 'And': 105,\n",
       " 'all': 106,\n",
       " 'those': 107,\n",
       " 'were': 108,\n",
       " 'reviewed': 109,\n",
       " 'another': 110,\n",
       " 'plus': 111,\n",
       " 'reviewers': 112,\n",
       " 'It': 113,\n",
       " 's': 114,\n",
       " 'big': 115,\n",
       " 'community': 116,\n",
       " 'such': 117,\n",
       " 'fact': 118,\n",
       " 'our': 119,\n",
       " 'annual': 120,\n",
       " 'gathering': 121,\n",
       " 'is': 122,\n",
       " 'largest': 123,\n",
       " 'meeting': 124,\n",
       " 'world': 125,\n",
       " 'Over': 126,\n",
       " 'go': 127,\n",
       " 'San': 128,\n",
       " 'Francisco': 129,\n",
       " 'every': 130,\n",
       " 'year': 131,\n",
       " 'for': 132,\n",
       " 'research': 133,\n",
       " 'group': 134,\n",
       " 'studies': 135,\n",
       " 'wide': 136,\n",
       " 'variety': 137,\n",
       " 'topics': 138,\n",
       " 'For': 139,\n",
       " 'us': 140,\n",
       " 'at': 141,\n",
       " 'Cambridge': 142,\n",
       " 'it': 143,\n",
       " 'as': 144,\n",
       " 'varied': 145,\n",
       " 'El': 146,\n",
       " 'Nino': 147,\n",
       " 'oscillation': 148,\n",
       " 'which': 149,\n",
       " 'affects': 150,\n",
       " 'weather': 151,\n",
       " 'assimilation': 152,\n",
       " 'satellite': 153,\n",
       " 'emissions': 154,\n",
       " 'crops': 155,\n",
       " 'produce': 156,\n",
       " 'biofuels': 157,\n",
       " 'what': 158,\n",
       " 'happen': 159,\n",
       " 'study': 160,\n",
       " 'each': 161,\n",
       " 'these': 162,\n",
       " 'areas': 163,\n",
       " 'there': 164,\n",
       " 'even': 165,\n",
       " 'more': 166,\n",
       " 'PhD': 167,\n",
       " 'students': 168,\n",
       " 'me': 169,\n",
       " 'we': 170,\n",
       " 'incredibly': 171,\n",
       " 'narrow': 172,\n",
       " 'things': 173,\n",
       " 'few': 174,\n",
       " 'processes': 175,\n",
       " 'molecules': 176,\n",
       " 'called': 177,\n",
       " 'isoprene': 178,\n",
       " 'here': 179,\n",
       " 'small': 180,\n",
       " 'organic': 181,\n",
       " 'You': 182,\n",
       " 've': 183,\n",
       " 'probably': 184,\n",
       " 'never': 185,\n",
       " 'heard': 186,\n",
       " 'weight': 187,\n",
       " 'clip': 188,\n",
       " 'approximately': 189,\n",
       " 'equal': 190,\n",
       " 'zeta': 191,\n",
       " 'illion': 192,\n",
       " 'st': 193,\n",
       " 'But': 194,\n",
       " 'despite': 195,\n",
       " 'its': 196,\n",
       " 'very': 197,\n",
       " 'enough': 198,\n",
       " 'emitted': 199,\n",
       " 'atmosphere': 200,\n",
       " 'people': 201,\n",
       " 'planet': 202,\n",
       " 'huge': 203,\n",
       " 'amount': 204,\n",
       " 'stuff': 205,\n",
       " 'methane': 206,\n",
       " 'because': 207,\n",
       " 'so': 208,\n",
       " 'much': 209,\n",
       " 'really': 210,\n",
       " 'important': 211,\n",
       " 'Because': 212,\n",
       " 'lengths': 213,\n",
       " 'thing': 214,\n",
       " 'We': 215,\n",
       " 'blow': 216,\n",
       " 'up': 217,\n",
       " 'pieces': 218,\n",
       " 'This': 219,\n",
       " 'EUPHORE': 220,\n",
       " 'Smog': 221,\n",
       " 'Chamber': 222,\n",
       " 'Spain': 223,\n",
       " 'Atmospheric': 224,\n",
       " 'explosions': 225,\n",
       " 'full': 226,\n",
       " 'combustion': 227,\n",
       " 'takes': 228,\n",
       " 'times': 229,\n",
       " 'longer': 230,\n",
       " 'than': 231,\n",
       " 'happens': 232,\n",
       " 'your': 233,\n",
       " 'car': 234,\n",
       " 'still': 235,\n",
       " 'run': 236,\n",
       " 'enormous': 237,\n",
       " 'models': 238,\n",
       " 'supercomputers': 239,\n",
       " 'Our': 240,\n",
       " 'hundreds': 241,\n",
       " 'grid': 242,\n",
       " 'boxes': 243,\n",
       " 'calculating': 244,\n",
       " 'variables': 245,\n",
       " 'minute': 246,\n",
       " 'timescales': 247,\n",
       " 'weeks': 248,\n",
       " 'perform': 249,\n",
       " 'integrations': 250,\n",
       " 'dozens': 251,\n",
       " 'order': 252,\n",
       " 'understand': 253,\n",
       " 'happening': 254,\n",
       " 'also': 255,\n",
       " 'fly': 256,\n",
       " 'looking': 257,\n",
       " 'recently': 258,\n",
       " 'joined': 259,\n",
       " 'campaign': 260,\n",
       " 'Malaysia': 261,\n",
       " 'There': 262,\n",
       " 'others': 263,\n",
       " 'found': 264,\n",
       " 'global': 265,\n",
       " 'watchtower': 266,\n",
       " 'middle': 267,\n",
       " 'hung': 268,\n",
       " 'dollars': 269,\n",
       " 'worth': 270,\n",
       " 'equipment': 271,\n",
       " 'off': 272,\n",
       " 'tower': 273,\n",
       " 'course': 274,\n",
       " 'other': 275,\n",
       " 'while': 276,\n",
       " 'above': 277,\n",
       " 'below': 278,\n",
       " 'part': 279,\n",
       " 'brought': 280,\n",
       " 'an': 281,\n",
       " 'aircraft': 282,\n",
       " 'plane': 283,\n",
       " 'model': 284,\n",
       " 'BA': 285,\n",
       " 'FAAM': 286,\n",
       " 'normally': 287,\n",
       " 'flies': 288,\n",
       " 'So': 289,\n",
       " 'maybe': 290,\n",
       " 'took': 291,\n",
       " 'similar': 292,\n",
       " 'get': 293,\n",
       " 'didn': 294,\n",
       " 't': 295,\n",
       " 'just': 296,\n",
       " 'flying': 297,\n",
       " 'meters': 298,\n",
       " 'top': 299,\n",
       " 'canopy': 300,\n",
       " 'measure': 301,\n",
       " 'dangerous': 302,\n",
       " 'special': 303,\n",
       " 'incline': 304,\n",
       " 'make': 305,\n",
       " 'measurements': 306,\n",
       " 'hire': 307,\n",
       " 'military': 308,\n",
       " 'test': 309,\n",
       " 'pilots': 310,\n",
       " 'maneuvering': 311,\n",
       " 'clearance': 312,\n",
       " 'come': 313,\n",
       " 'around': 314,\n",
       " 'banks': 315,\n",
       " 'valleys': 316,\n",
       " 'forces': 317,\n",
       " 'can': 318,\n",
       " 'Gs': 319,\n",
       " 'be': 320,\n",
       " 'completely': 321,\n",
       " 'harnessed': 322,\n",
       " 're': 323,\n",
       " 'board': 324,\n",
       " 'imagine': 325,\n",
       " 'inside': 326,\n",
       " 'doesn': 327,\n",
       " 'any': 328,\n",
       " 'would': 329,\n",
       " 'take': 330,\n",
       " 'vacation': 331,\n",
       " 'laboratory': 332,\n",
       " 'region': 333,\n",
       " 'chemistry': 334,\n",
       " 'student': 335,\n",
       " 'has': 336,\n",
       " 'some': 337,\n",
       " 'sort': 338,\n",
       " 'inclination': 339,\n",
       " 'write': 340,\n",
       " 'subject': 341,\n",
       " 'll': 342,\n",
       " 'dozen': 343,\n",
       " 'papers': 344,\n",
       " 'body': 345,\n",
       " 'knowledge': 346,\n",
       " 'builds': 347,\n",
       " 'will': 348,\n",
       " 'form': 349,\n",
       " 'subsection': 350,\n",
       " 'sub': 351,\n",
       " 'assessment': 352,\n",
       " 'although': 353,\n",
       " 'chapters': 354,\n",
       " 'six': 355,\n",
       " 'ten': 356,\n",
       " 'subsections': 357,\n",
       " 'assessments': 358,\n",
       " 'always': 359,\n",
       " 'tag': 360,\n",
       " 'summary': 361,\n",
       " 'non': 362,\n",
       " 'audience': 363,\n",
       " 'hand': 364,\n",
       " 'journalists': 365,\n",
       " 'policy': 366,\n",
       " 'makers': 367,\n",
       " 'Thank': 368,\n",
       " 'Christopher': 369,\n",
       " 'deCharms': 370,\n",
       " 'A': 371,\n",
       " 'brain': 372,\n",
       " 'real': 373,\n",
       " 'time': 374,\n",
       " 'Neuroscientist': 375,\n",
       " 'inventor': 376,\n",
       " 'demonstrates': 377,\n",
       " 'new': 378,\n",
       " 'way': 379,\n",
       " 'use': 380,\n",
       " 'fMRI': 381,\n",
       " 'show': 382,\n",
       " 'activity': 383,\n",
       " 'thoughts': 384,\n",
       " 'emotions': 385,\n",
       " 'pain': 386,\n",
       " 'words': 387,\n",
       " 'actually': 388,\n",
       " 'how': 389,\n",
       " 'feel': 390,\n",
       " 'Hi': 391,\n",
       " 'm': 392,\n",
       " 'going': 393,\n",
       " 'ask': 394,\n",
       " 'raise': 395,\n",
       " 'arms': 396,\n",
       " 'wave': 397,\n",
       " 'back': 398,\n",
       " 'am': 399,\n",
       " 'kind': 400,\n",
       " 'royal': 401,\n",
       " 'mimic': 402,\n",
       " 'program': 403,\n",
       " 'muscles': 404,\n",
       " 'arm': 405,\n",
       " 'Soon': 406,\n",
       " 'able': 407,\n",
       " 'control': 408,\n",
       " 'tell': 409,\n",
       " 'technology': 410,\n",
       " 'People': 411,\n",
       " 'wanted': 412,\n",
       " 'human': 413,\n",
       " 'mind': 414,\n",
       " 'years': 415,\n",
       " 'Well': 416,\n",
       " 'coming': 417,\n",
       " 'labs': 418,\n",
       " 'now': 419,\n",
       " 'generation': 420,\n",
       " 'possibility': 421,\n",
       " 'envision': 422,\n",
       " 'being': 423,\n",
       " 'difficult': 424,\n",
       " 'had': 425,\n",
       " 'spaceship': 426,\n",
       " 'shrink': 427,\n",
       " 'down': 428,\n",
       " 'inject': 429,\n",
       " 'bloodstream': 430,\n",
       " 'terribly': 431,\n",
       " 'could': 432,\n",
       " 'attacked': 433,\n",
       " 'white': 434,\n",
       " 'blood': 435,\n",
       " 'cells': 436,\n",
       " 'arteries': 437,\n",
       " 'my': 438,\n",
       " 'colleague': 439,\n",
       " 'Peter': 440,\n",
       " 'invasively': 441,\n",
       " 'using': 442,\n",
       " 'MRI': 443,\n",
       " 'don': 444,\n",
       " 'anything': 445,\n",
       " 'need': 446,\n",
       " 'radiation': 447,\n",
       " 'anatomy': 448,\n",
       " 'literally': 449,\n",
       " 'his': 450,\n",
       " 'but': 451,\n",
       " 'importantly': 452,\n",
       " 'When': 453,\n",
       " 'moves': 454,\n",
       " 'yellow': 455,\n",
       " 'spot': 456,\n",
       " 'interface': 457,\n",
       " 'functioning': 458,\n",
       " 'place': 459,\n",
       " 'Now': 460,\n",
       " 'seen': 461,\n",
       " 'before': 462,\n",
       " 'electrodes': 463,\n",
       " 'robotic': 464,\n",
       " 'imaging': 465,\n",
       " 'scanners': 466,\n",
       " 'insides': 467,\n",
       " 'brains': 468,\n",
       " 'What': 469,\n",
       " 'process': 470,\n",
       " 'typically': 471,\n",
       " 'taken': 472,\n",
       " 'days': 473,\n",
       " 'months': 474,\n",
       " 'analysis': 475,\n",
       " 'collapsed': 476,\n",
       " 'through': 477,\n",
       " 'milliseconds': 478,\n",
       " 'allows': 479,\n",
       " 'let': 480,\n",
       " 'he': 481,\n",
       " 'scanner': 482,\n",
       " 'He': 483,\n",
       " 'points': 484,\n",
       " 'activation': 485,\n",
       " 'per': 486,\n",
       " 'second': 487,\n",
       " 'If': 488,\n",
       " 'pattern': 489,\n",
       " 'own': 490,\n",
       " 'learn': 491,\n",
       " 'been': 492,\n",
       " 'three': 493,\n",
       " 'ways': 494,\n",
       " 'try': 495,\n",
       " 'impact': 496,\n",
       " 'therapist': 497,\n",
       " 'couch': 498,\n",
       " 'pills': 499,\n",
       " 'knife': 500,\n",
       " 'fourth': 501,\n",
       " 'alternative': 502,\n",
       " 'soon': 503,\n",
       " 'know': 504,\n",
       " 'deep': 505,\n",
       " 'channels': 506,\n",
       " 'minds': 507,\n",
       " 'Chronic': 508,\n",
       " 'example': 509,\n",
       " 'burn': 510,\n",
       " 'yourself': 511,\n",
       " 'pull': 512,\n",
       " 'away': 513,\n",
       " 'if': 514,\n",
       " 'circuits': 515,\n",
       " 'producing': 516,\n",
       " 'no': 517,\n",
       " 'helping': 518,\n",
       " 'D': 519,\n",
       " 'watch': 520,\n",
       " 'information': 521,\n",
       " 'then': 522,\n",
       " 'select': 523,\n",
       " 'flex': 524,\n",
       " 'bicep': 525,\n",
       " 'seeing': 526,\n",
       " 'selected': 527,\n",
       " 'pathways': 528,\n",
       " 'chronic': 529,\n",
       " 'patient': 530,\n",
       " 'may': 531,\n",
       " 'shock': 532,\n",
       " 'reading': 533,\n",
       " 'person': 534,\n",
       " 'watching': 535,\n",
       " 'controlling': 536,\n",
       " 'pathway': 537,\n",
       " 'produces': 538,\n",
       " 'learning': 539,\n",
       " 'releases': 540,\n",
       " 'endogenous': 541,\n",
       " 'opiates': 542,\n",
       " 'As': 543,\n",
       " 'upper': 544,\n",
       " 'left': 545,\n",
       " 'display': 546,\n",
       " 'yoked': 547,\n",
       " 'controlled': 548,\n",
       " 'investigational': 549,\n",
       " 'clinical': 550,\n",
       " 'trials': 551,\n",
       " 'percent': 552,\n",
       " 'decrease': 553,\n",
       " 'patients': 554,\n",
       " 'not': 555,\n",
       " 'quot': 556,\n",
       " 'Matrix': 557,\n",
       " 'only': 558,\n",
       " 'too': 559,\n",
       " 'want': 560,\n",
       " 'aspects': 561,\n",
       " 'experiences': 562,\n",
       " 'These': 563,\n",
       " 'working': 564,\n",
       " 'detail': 565,\n",
       " 'leave': 566,\n",
       " 'question': 567,\n",
       " 'first': 568,\n",
       " 'enter': 569,\n",
       " 'Where': 570,\n",
       " 'Beeban': 571,\n",
       " 'Kidron': 572,\n",
       " 'shared': 573,\n",
       " 'wonder': 574,\n",
       " 'film': 575,\n",
       " 'Movies': 576,\n",
       " 'power': 577,\n",
       " 'create': 578,\n",
       " 'narrative': 579,\n",
       " 'experience': 580,\n",
       " 'shape': 581,\n",
       " 'memories': 582,\n",
       " 'worldviews': 583,\n",
       " 'British': 584,\n",
       " 'director': 585,\n",
       " 'invokes': 586,\n",
       " 'iconic': 587,\n",
       " 'scenes': 588,\n",
       " 'amp': 589,\n",
       " 'lt': 590,\n",
       " 'em': 591,\n",
       " 'gt': 592,\n",
       " 'Miracle': 593,\n",
       " 'Milan': 594,\n",
       " 'Boyz': 595,\n",
       " 'n': 596,\n",
       " 'Hood': 597,\n",
       " 'she': 598,\n",
       " 'shows': 599,\n",
       " 'FILMCLUB': 600,\n",
       " 'shares': 601,\n",
       " 'great': 602,\n",
       " 'films': 603,\n",
       " 'kids': 604,\n",
       " 'Evidence': 605,\n",
       " 'suggests': 606,\n",
       " 'humans': 607,\n",
       " 'ages': 608,\n",
       " 'cultures': 609,\n",
       " 'identity': 610,\n",
       " 'From': 611,\n",
       " 'mother': 612,\n",
       " 'daughter': 613,\n",
       " 'preacher': 614,\n",
       " 'congregant': 615,\n",
       " 'teacher': 616,\n",
       " 'pupil': 617,\n",
       " 'storyteller': 618,\n",
       " 'Whether': 619,\n",
       " 'cave': 620,\n",
       " 'paintings': 621,\n",
       " 'latest': 622,\n",
       " 'uses': 623,\n",
       " 'Internet': 624,\n",
       " 'beings': 625,\n",
       " 'told': 626,\n",
       " 'histories': 627,\n",
       " 'truths': 628,\n",
       " 'parable': 629,\n",
       " 'fable': 630,\n",
       " 'inveterate': 631,\n",
       " 'storytellers': 632,\n",
       " 'where': 633,\n",
       " 'increasingly': 634,\n",
       " 'secular': 635,\n",
       " 'fragmented': 636,\n",
       " 'offer': 637,\n",
       " 'communality': 638,\n",
       " 'unmediated': 639,\n",
       " 'furious': 640,\n",
       " 'consumerism': 641,\n",
       " 'history': 642,\n",
       " 'moral': 643,\n",
       " 'code': 644,\n",
       " 'imparting': 645,\n",
       " 'young': 646,\n",
       " 'Cinema': 647,\n",
       " 'arguably': 648,\n",
       " 'th': 649,\n",
       " 'century': 650,\n",
       " 'most': 651,\n",
       " 'influential': 652,\n",
       " 'art': 653,\n",
       " 'Its': 654,\n",
       " 'artists': 655,\n",
       " 'stories': 656,\n",
       " 'across': 657,\n",
       " 'national': 658,\n",
       " 'boundaries': 659,\n",
       " 'many': 660,\n",
       " 'languages': 661,\n",
       " 'genres': 662,\n",
       " 'philosophies': 663,\n",
       " 'Indeed': 664,\n",
       " 'hard': 665,\n",
       " 'find': 666,\n",
       " 'yet': 667,\n",
       " 'tackle': 668,\n",
       " 'During': 669,\n",
       " 'last': 670,\n",
       " 'decade': 671,\n",
       " 'vast': 672,\n",
       " 'integration': 673,\n",
       " 'media': 674,\n",
       " 'dominated': 675,\n",
       " 'culture': 676,\n",
       " 'Hollywood': 677,\n",
       " 'blockbuster': 678,\n",
       " 'offered': 679,\n",
       " 'diet': 680,\n",
       " 'sensation': 681,\n",
       " 'story': 682,\n",
       " 'king': 683,\n",
       " 'common': 684,\n",
       " 'ago': 685,\n",
       " 'telling': 686,\n",
       " 'between': 687,\n",
       " 'generations': 688,\n",
       " 'rarified': 689,\n",
       " 'filmmaker': 690,\n",
       " 'worried': 691,\n",
       " 'puts': 692,\n",
       " 'fear': 693,\n",
       " 'God': 694,\n",
       " 'future': 695,\n",
       " 'build': 696,\n",
       " 'little': 697,\n",
       " 'grasp': 698,\n",
       " 'narratives': 699,\n",
       " 'possible': 700,\n",
       " 'irony': 701,\n",
       " 'palpable': 702,\n",
       " 'technical': 703,\n",
       " 'access': 704,\n",
       " 'greater': 705,\n",
       " 'cultural': 706,\n",
       " 'weaker': 707,\n",
       " 'set': 708,\n",
       " 'organization': 709,\n",
       " 'ran': 710,\n",
       " 'weekly': 711,\n",
       " 'screenings': 712,\n",
       " 'schools': 713,\n",
       " 'followed': 714,\n",
       " 'discussions': 715,\n",
       " 'raid': 716,\n",
       " 'annals': 717,\n",
       " 'deliver': 718,\n",
       " 'meaning': 719,\n",
       " 'restless': 720,\n",
       " 'Given': 721,\n",
       " 'school': 722,\n",
       " 'tiny': 723,\n",
       " 'rural': 724,\n",
       " 'hamlet': 725,\n",
       " 'project': 726,\n",
       " 'DVD': 727,\n",
       " 'onto': 728,\n",
       " 'nine': 729,\n",
       " 'clubs': 730,\n",
       " 'U': 731,\n",
       " '<EOS>K': 732,\n",
       " 'age': 733,\n",
       " 'groups': 734,\n",
       " 'five': 735,\n",
       " 'uninterrupted': 736,\n",
       " 'curated': 737,\n",
       " 'contextualized': 738,\n",
       " 'choice': 739,\n",
       " 'theirs': 740,\n",
       " 'quickly': 741,\n",
       " 'grew': 742,\n",
       " 'choose': 743,\n",
       " 'richest': 744,\n",
       " 'provide': 745,\n",
       " 'outcome': 746,\n",
       " 'immediate': 747,\n",
       " 'education': 748,\n",
       " 'profound': 749,\n",
       " 'transformative': 750,\n",
       " 'large': 751,\n",
       " 'discovered': 752,\n",
       " 'places': 753,\n",
       " 'perspectives': 754,\n",
       " 'By': 755,\n",
       " 'pilot': 756,\n",
       " 'finished': 757,\n",
       " 'names': 758,\n",
       " 'wished': 759,\n",
       " 'join': 760,\n",
       " 'changed': 761,\n",
       " 'life': 762,\n",
       " 'Vittorio': 763,\n",
       " 'De': 764,\n",
       " 'Sica': 765,\n",
       " '': 766,\n",
       " 'remarkable': 767,\n",
       " 'comment': 768,\n",
       " 'slums': 769,\n",
       " 'poverty': 770,\n",
       " 'aspiration': 771,\n",
       " 'occasion': 772,\n",
       " 'father': 773,\n",
       " 'birthday': 774,\n",
       " 'Technology': 775,\n",
       " 'meant': 776,\n",
       " 'viewing': 777,\n",
       " 'cinema': 778,\n",
       " 'pay': 779,\n",
       " 'print': 780,\n",
       " 'projectionist': 781,\n",
       " 'emotional': 782,\n",
       " 'artistic': 783,\n",
       " 'importance': 784,\n",
       " 'vision': 785,\n",
       " 'chose': 786,\n",
       " 'celebrate': 787,\n",
       " 'half': 788,\n",
       " 'teenage': 789,\n",
       " 'children': 790,\n",
       " 'friends': 791,\n",
       " 'said': 792,\n",
       " 'pass': 793,\n",
       " 'baton': 794,\n",
       " 'concern': 795,\n",
       " 'hope': 796,\n",
       " 'next': 797,\n",
       " 'shot': 798,\n",
       " 'slum': 799,\n",
       " 'dwellers': 800,\n",
       " 'float': 801,\n",
       " 'skyward': 802,\n",
       " 'brooms': 803,\n",
       " 'Sixty': 804,\n",
       " 'after': 805,\n",
       " 'made': 806,\n",
       " 'saw': 807,\n",
       " 'faces': 808,\n",
       " 'tilt': 809,\n",
       " 'awe': 810,\n",
       " 'incredulity': 811,\n",
       " 'matching': 812,\n",
       " 'mine': 813,\n",
       " 'speed': 814,\n",
       " 'associate': 815,\n",
       " 'Slumdog': 816,\n",
       " 'Millionaire': 817,\n",
       " 'favelas': 818,\n",
       " 'Rio': 819,\n",
       " 'speaks': 820,\n",
       " 'enduring': 821,\n",
       " 'nature': 822,\n",
       " 'season': 823,\n",
       " 'democracy': 824,\n",
       " 'government': 825,\n",
       " 'screened': 826,\n",
       " 'Mr': 827,\n",
       " 'Smith': 828,\n",
       " 'Goes': 829,\n",
       " 'Washington': 830,\n",
       " 'Made': 831,\n",
       " 'older': 832,\n",
       " 'members': 833,\n",
       " 'grandparents': 834,\n",
       " 'Frank': 835,\n",
       " 'Capra': 836,\n",
       " 'classic': 837,\n",
       " 'values': 838,\n",
       " 'independence': 839,\n",
       " 'propriety': 840,\n",
       " 'right': 841,\n",
       " 'heroically': 842,\n",
       " 'awkward': 843,\n",
       " 'expression': 844,\n",
       " 'faith': 845,\n",
       " 'political': 846,\n",
       " 'machine': 847,\n",
       " 'force': 848,\n",
       " 'honor': 849,\n",
       " 'Shortly': 850,\n",
       " 'became': 851,\n",
       " 'week': 852,\n",
       " 'night': 853,\n",
       " 'filibustering': 854,\n",
       " 'House': 855,\n",
       " 'Lords': 856,\n",
       " 'delight': 857,\n",
       " 'country': 858,\n",
       " 'explaining': 859,\n",
       " 'authority': 860,\n",
       " 'why': 861,\n",
       " 'might': 862,\n",
       " 'defy': 863,\n",
       " 'bedtime': 864,\n",
       " 'point': 865,\n",
       " 'principle': 866,\n",
       " 'After': 867,\n",
       " 'Jimmy': 868,\n",
       " 'Stewart': 869,\n",
       " 'filibustered': 870,\n",
       " 'entire': 871,\n",
       " 'reels': 872,\n",
       " 'choosing': 873,\n",
       " 'Hotel': 874,\n",
       " 'Rwanda': 875,\n",
       " 'explored': 876,\n",
       " 'genocide': 877,\n",
       " 'brutal': 878,\n",
       " 'provoked': 879,\n",
       " 'tears': 880,\n",
       " 'well': 881,\n",
       " 'incisive': 882,\n",
       " 'questions': 883,\n",
       " 'unarmed': 884,\n",
       " 'peace': 885,\n",
       " 'keeping': 886,\n",
       " 'double': 887,\n",
       " 'dealing': 888,\n",
       " 'Western': 889,\n",
       " 'society': 890,\n",
       " 'picks': 891,\n",
       " 'fights': 892,\n",
       " 'commodities': 893,\n",
       " 'Schindler': 894,\n",
       " 'List': 895,\n",
       " 'demanded': 896,\n",
       " 'forget': 897,\n",
       " 'child': 898,\n",
       " 'consciousness': 899,\n",
       " 'remarked': 900,\n",
       " 'already': 901,\n",
       " 'forgot': 902,\n",
       " 'otherwise': 903,\n",
       " 'did': 904,\n",
       " 'lives': 905,\n",
       " 'got': 906,\n",
       " 'palpably': 907,\n",
       " 'richer': 908,\n",
       " 'Pickpocket': 909,\n",
       " 'started': 910,\n",
       " 'debate': 911,\n",
       " 'criminality': 912,\n",
       " 'disenfranchisement': 913,\n",
       " 'To': 914,\n",
       " 'Sir': 915,\n",
       " 'Love': 916,\n",
       " 'ignited': 917,\n",
       " 'teen': 918,\n",
       " 'celebrated': 919,\n",
       " 'attitude': 920,\n",
       " 'towards': 921,\n",
       " 'Britons': 922,\n",
       " 'railed': 923,\n",
       " 'against': 924,\n",
       " 'does': 925,\n",
       " 'value': 926,\n",
       " 'collective': 927,\n",
       " 'unlike': 928,\n",
       " 'Sidney': 929,\n",
       " 'Poitier': 930,\n",
       " 'careful': 931,\n",
       " 'tutelage': 932,\n",
       " 'thoughtful': 933,\n",
       " 'opinionated': 934,\n",
       " 'curious': 935,\n",
       " 'thought': 936,\n",
       " 'nothing': 937,\n",
       " 'tackling': 938,\n",
       " 'forms': 939,\n",
       " 'black': 940,\n",
       " 'subtitled': 941,\n",
       " 'documentary': 942,\n",
       " 'fantasy': 943,\n",
       " 'writing': 944,\n",
       " 'detailed': 945,\n",
       " 'reviews': 946,\n",
       " 'competed': 947,\n",
       " 'favor': 948,\n",
       " 'passionate': 949,\n",
       " 'sophisticated': 950,\n",
       " 'prose': 951,\n",
       " 'Six': 952,\n",
       " 'vying': 953,\n",
       " 'review': 954,\n",
       " 'until': 955,\n",
       " 'nearly': 956,\n",
       " 'quarter': 957,\n",
       " 'million': 958,\n",
       " 'numbers': 959,\n",
       " 'continue': 960,\n",
       " 'extraordinary': 961,\n",
       " 'critical': 962,\n",
       " 'questioning': 963,\n",
       " 'translated': 964,\n",
       " 'Some': 965,\n",
       " 'talking': 966,\n",
       " 'parents': 967,\n",
       " 'teachers': 968,\n",
       " 'without': 969,\n",
       " 'them': 970,\n",
       " 'provided': 971,\n",
       " 'manner': 972,\n",
       " 'divide': 973,\n",
       " 'held': 974,\n",
       " 'Persepolis': 975,\n",
       " 'closer': 976,\n",
       " 'Iranian': 977,\n",
       " 'Jaws': 978,\n",
       " 'boy': 979,\n",
       " 'articulate': 980,\n",
       " 'experienced': 981,\n",
       " 'violence': 982,\n",
       " 'killed': 983,\n",
       " 'latter': 984,\n",
       " 'thrown': 985,\n",
       " 'overboard': 986,\n",
       " 'boat': 987,\n",
       " 'journey': 988,\n",
       " 'Who': 989,\n",
       " 'wrong': 990,\n",
       " 'under': 991,\n",
       " 'conditions': 992,\n",
       " 'Was': 993,\n",
       " 'tale': 994,\n",
       " 'hidden': 995,\n",
       " 'message': 996,\n",
       " 'How': 997,\n",
       " 'different': 998,\n",
       " 'tsunami': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<SOS>': 0,\n",
       " '<UNK>': 0,\n",
       " '<EOS>': 148908,\n",
       " 'Rachel': 20,\n",
       " 'Pike': 4,\n",
       " 'The': 6667,\n",
       " 'science': 763,\n",
       " 'behind': 417,\n",
       " 'a': 54259,\n",
       " 'climate': 314,\n",
       " 'headline': 26,\n",
       " 'In': 2758,\n",
       " 'minutes': 549,\n",
       " 'atmospheric': 12,\n",
       " 'chemist': 20,\n",
       " 'provides': 51,\n",
       " 'glimpse': 36,\n",
       " 'of': 60341,\n",
       " 'the': 103138,\n",
       " 'massive': 169,\n",
       " 'scientific': 237,\n",
       " 'effort': 137,\n",
       " 'bold': 65,\n",
       " 'headlines': 43,\n",
       " 'on': 13313,\n",
       " 'change': 1612,\n",
       " 'with': 13280,\n",
       " 'her': 2257,\n",
       " 'team': 420,\n",
       " 'one': 8316,\n",
       " 'thousands': 464,\n",
       " 'who': 5289,\n",
       " 'contributed': 27,\n",
       " 'taking': 550,\n",
       " 'risky': 26,\n",
       " 'flight': 107,\n",
       " 'over': 2632,\n",
       " 'rainforest': 36,\n",
       " 'in': 38689,\n",
       " 'pursuit': 43,\n",
       " 'data': 1194,\n",
       " 'key': 313,\n",
       " 'molecule': 141,\n",
       " 'I': 43694,\n",
       " 'apos': 64433,\n",
       " 'd': 1767,\n",
       " 'like': 8303,\n",
       " 'to': 65797,\n",
       " 'talk': 1759,\n",
       " 'you': 33041,\n",
       " 'today': 1583,\n",
       " 'about': 11111,\n",
       " 'scale': 370,\n",
       " 'that': 46612,\n",
       " 'goes': 704,\n",
       " 'into': 4150,\n",
       " 'making': 911,\n",
       " 'see': 5220,\n",
       " 'paper': 400,\n",
       " 'Headlines': 2,\n",
       " 'look': 2780,\n",
       " 'this': 21400,\n",
       " 'when': 5876,\n",
       " 'they': 13833,\n",
       " 'have': 14210,\n",
       " 'do': 9111,\n",
       " 'and': 56439,\n",
       " 'air': 413,\n",
       " 'quality': 256,\n",
       " 'or': 6579,\n",
       " 'smog': 5,\n",
       " 'They': 3703,\n",
       " 'are': 14443,\n",
       " 'both': 616,\n",
       " 'two': 3242,\n",
       " 'branches': 29,\n",
       " 'same': 2034,\n",
       " 'field': 321,\n",
       " 'Recently': 19,\n",
       " 'looked': 639,\n",
       " 'Intergovernmental': 2,\n",
       " 'Panel': 9,\n",
       " 'Climate': 19,\n",
       " 'Change': 12,\n",
       " 'IPCC': 8,\n",
       " 'put': 1802,\n",
       " 'out': 6219,\n",
       " 'their': 5294,\n",
       " 'report': 118,\n",
       " 'state': 510,\n",
       " 'understanding': 312,\n",
       " 'system': 1270,\n",
       " 'That': 2709,\n",
       " 'was': 15982,\n",
       " 'written': 186,\n",
       " 'by': 6001,\n",
       " 'scientists': 314,\n",
       " 'from': 8205,\n",
       " 'countries': 804,\n",
       " 'wrote': 316,\n",
       " 'almost': 607,\n",
       " 'thousand': 191,\n",
       " 'pages': 95,\n",
       " 'topic': 86,\n",
       " 'And': 23335,\n",
       " 'all': 9423,\n",
       " 'those': 2983,\n",
       " 'were': 5227,\n",
       " 'reviewed': 21,\n",
       " 'another': 1276,\n",
       " 'plus': 85,\n",
       " 'reviewers': 4,\n",
       " 'It': 8398,\n",
       " 's': 30143,\n",
       " 'big': 1464,\n",
       " 'community': 632,\n",
       " 'such': 756,\n",
       " 'fact': 1606,\n",
       " 'our': 7055,\n",
       " 'annual': 31,\n",
       " 'gathering': 48,\n",
       " 'is': 33470,\n",
       " 'largest': 216,\n",
       " 'meeting': 159,\n",
       " 'world': 4508,\n",
       " 'Over': 121,\n",
       " 'go': 3554,\n",
       " 'San': 111,\n",
       " 'Francisco': 72,\n",
       " 'every': 2019,\n",
       " 'year': 2048,\n",
       " 'for': 15441,\n",
       " 'research': 572,\n",
       " 'group': 580,\n",
       " 'studies': 214,\n",
       " 'wide': 96,\n",
       " 'variety': 80,\n",
       " 'topics': 36,\n",
       " 'For': 743,\n",
       " 'us': 4727,\n",
       " 'at': 9570,\n",
       " 'Cambridge': 49,\n",
       " 'it': 30677,\n",
       " 'as': 9481,\n",
       " 'varied': 15,\n",
       " 'El': 33,\n",
       " 'Nino': 7,\n",
       " 'oscillation': 1,\n",
       " 'which': 5154,\n",
       " 'affects': 48,\n",
       " 'weather': 93,\n",
       " 'assimilation': 2,\n",
       " 'satellite': 71,\n",
       " 'emissions': 89,\n",
       " 'crops': 53,\n",
       " 'produce': 283,\n",
       " 'biofuels': 7,\n",
       " 'what': 11484,\n",
       " 'happen': 752,\n",
       " 'study': 515,\n",
       " 'each': 1277,\n",
       " 'these': 6485,\n",
       " 'areas': 298,\n",
       " 'there': 8637,\n",
       " 'even': 2533,\n",
       " 'more': 5865,\n",
       " 'PhD': 9,\n",
       " 'students': 479,\n",
       " 'me': 6455,\n",
       " 'we': 28447,\n",
       " 'incredibly': 278,\n",
       " 'narrow': 45,\n",
       " 'things': 3873,\n",
       " 'few': 1237,\n",
       " 'processes': 89,\n",
       " 'molecules': 184,\n",
       " 'called': 1823,\n",
       " 'isoprene': 3,\n",
       " 'here': 4416,\n",
       " 'small': 873,\n",
       " 'organic': 99,\n",
       " 'You': 4407,\n",
       " 've': 4639,\n",
       " 'probably': 802,\n",
       " 'never': 1369,\n",
       " 'heard': 591,\n",
       " 'weight': 178,\n",
       " 'clip': 54,\n",
       " 'approximately': 24,\n",
       " 'equal': 123,\n",
       " 'zeta': 1,\n",
       " 'illion': 1,\n",
       " 'st': 97,\n",
       " 'But': 6585,\n",
       " 'despite': 121,\n",
       " 'its': 1462,\n",
       " 'very': 6308,\n",
       " 'enough': 862,\n",
       " 'emitted': 11,\n",
       " 'atmosphere': 129,\n",
       " 'people': 8558,\n",
       " 'planet': 510,\n",
       " 'huge': 522,\n",
       " 'amount': 370,\n",
       " 'stuff': 720,\n",
       " 'methane': 32,\n",
       " 'because': 5633,\n",
       " 'so': 7807,\n",
       " 'much': 3124,\n",
       " 'really': 5081,\n",
       " 'important': 1433,\n",
       " 'Because': 1120,\n",
       " 'lengths': 9,\n",
       " 'thing': 2860,\n",
       " 'We': 6642,\n",
       " 'blow': 46,\n",
       " 'up': 5912,\n",
       " 'pieces': 189,\n",
       " 'This': 4358,\n",
       " 'EUPHORE': 1,\n",
       " 'Smog': 1,\n",
       " 'Chamber': 4,\n",
       " 'Spain': 30,\n",
       " 'Atmospheric': 1,\n",
       " 'explosions': 14,\n",
       " 'full': 448,\n",
       " 'combustion': 9,\n",
       " 'takes': 448,\n",
       " 'times': 815,\n",
       " 'longer': 382,\n",
       " 'than': 3179,\n",
       " 'happens': 709,\n",
       " 'your': 5384,\n",
       " 'car': 578,\n",
       " 'still': 1363,\n",
       " 'run': 540,\n",
       " 'enormous': 128,\n",
       " 'models': 201,\n",
       " 'supercomputers': 4,\n",
       " 'Our': 350,\n",
       " 'hundreds': 291,\n",
       " 'grid': 89,\n",
       " 'boxes': 71,\n",
       " 'calculating': 25,\n",
       " 'variables': 22,\n",
       " 'minute': 254,\n",
       " 'timescales': 1,\n",
       " 'weeks': 342,\n",
       " 'perform': 77,\n",
       " 'integrations': 2,\n",
       " 'dozens': 43,\n",
       " 'order': 544,\n",
       " 'understand': 1114,\n",
       " 'happening': 515,\n",
       " 'also': 2953,\n",
       " 'fly': 278,\n",
       " 'looking': 1106,\n",
       " 'recently': 258,\n",
       " 'joined': 77,\n",
       " 'campaign': 112,\n",
       " 'Malaysia': 11,\n",
       " 'There': 2253,\n",
       " 'others': 483,\n",
       " 'found': 1165,\n",
       " 'global': 616,\n",
       " 'watchtower': 1,\n",
       " 'middle': 429,\n",
       " 'hung': 11,\n",
       " 'dollars': 789,\n",
       " 'worth': 253,\n",
       " 'equipment': 114,\n",
       " 'off': 1354,\n",
       " 'tower': 37,\n",
       " 'course': 1387,\n",
       " 'other': 3801,\n",
       " 'while': 697,\n",
       " 'above': 199,\n",
       " 'below': 143,\n",
       " 'part': 1348,\n",
       " 'brought': 333,\n",
       " 'an': 7357,\n",
       " 'aircraft': 41,\n",
       " 'plane': 95,\n",
       " 'model': 497,\n",
       " 'BA': 4,\n",
       " 'FAAM': 1,\n",
       " 'normally': 103,\n",
       " 'flies': 59,\n",
       " 'So': 11724,\n",
       " 'maybe': 920,\n",
       " 'took': 905,\n",
       " 'similar': 245,\n",
       " 'get': 4902,\n",
       " 'didn': 1569,\n",
       " 't': 11709,\n",
       " 'just': 6903,\n",
       " 'flying': 148,\n",
       " 'meters': 118,\n",
       " 'top': 634,\n",
       " 'canopy': 16,\n",
       " 'measure': 235,\n",
       " 'dangerous': 177,\n",
       " 'special': 243,\n",
       " 'incline': 3,\n",
       " 'make': 3610,\n",
       " 'measurements': 29,\n",
       " 'hire': 45,\n",
       " 'military': 166,\n",
       " 'test': 346,\n",
       " 'pilots': 21,\n",
       " 'maneuvering': 2,\n",
       " 'clearance': 8,\n",
       " 'come': 1955,\n",
       " 'around': 2333,\n",
       " 'banks': 58,\n",
       " 'valleys': 11,\n",
       " 'forces': 141,\n",
       " 'can': 12287,\n",
       " 'Gs': 7,\n",
       " 'be': 10401,\n",
       " 'completely': 518,\n",
       " 'harnessed': 8,\n",
       " 're': 8774,\n",
       " 'board': 169,\n",
       " 'imagine': 590,\n",
       " 'inside': 716,\n",
       " 'doesn': 1100,\n",
       " 'any': 1793,\n",
       " 'would': 5090,\n",
       " 'take': 2458,\n",
       " 'vacation': 30,\n",
       " 'laboratory': 71,\n",
       " 'region': 225,\n",
       " 'chemistry': 98,\n",
       " 'student': 227,\n",
       " 'has': 4245,\n",
       " 'some': 4198,\n",
       " 'sort': 1010,\n",
       " 'inclination': 1,\n",
       " 'write': 357,\n",
       " 'subject': 163,\n",
       " 'll': 2075,\n",
       " 'dozen': 63,\n",
       " 'papers': 65,\n",
       " 'body': 843,\n",
       " 'knowledge': 348,\n",
       " 'builds': 34,\n",
       " 'will': 3867,\n",
       " 'form': 594,\n",
       " 'subsection': 2,\n",
       " 'sub': 87,\n",
       " 'assessment': 22,\n",
       " 'although': 131,\n",
       " 'chapters': 14,\n",
       " 'six': 623,\n",
       " 'ten': 46,\n",
       " 'subsections': 1,\n",
       " 'assessments': 5,\n",
       " 'always': 1073,\n",
       " 'tag': 37,\n",
       " 'summary': 19,\n",
       " 'non': 233,\n",
       " 'audience': 309,\n",
       " 'hand': 744,\n",
       " 'journalists': 29,\n",
       " 'policy': 187,\n",
       " 'makers': 59,\n",
       " 'Thank': 1494,\n",
       " 'Christopher': 22,\n",
       " 'deCharms': 2,\n",
       " 'A': 1356,\n",
       " 'brain': 1551,\n",
       " 'real': 1113,\n",
       " 'time': 4475,\n",
       " 'Neuroscientist': 8,\n",
       " 'inventor': 24,\n",
       " 'demonstrates': 24,\n",
       " 'new': 2412,\n",
       " 'way': 4205,\n",
       " 'use': 1994,\n",
       " 'fMRI': 14,\n",
       " 'show': 1330,\n",
       " 'activity': 211,\n",
       " 'thoughts': 91,\n",
       " 'emotions': 135,\n",
       " 'pain': 215,\n",
       " 'words': 653,\n",
       " 'actually': 4204,\n",
       " 'how': 5291,\n",
       " 'feel': 996,\n",
       " 'Hi': 59,\n",
       " 'm': 4612,\n",
       " 'going': 5787,\n",
       " 'ask': 815,\n",
       " 'raise': 188,\n",
       " 'arms': 101,\n",
       " 'wave': 73,\n",
       " 'back': 2767,\n",
       " 'am': 787,\n",
       " 'kind': 2247,\n",
       " 'royal': 9,\n",
       " 'mimic': 32,\n",
       " 'program': 291,\n",
       " 'muscles': 52,\n",
       " 'arm': 132,\n",
       " 'Soon': 17,\n",
       " 'able': 1366,\n",
       " 'control': 614,\n",
       " 'tell': 1657,\n",
       " 'technology': 1098,\n",
       " 'People': 387,\n",
       " 'wanted': 1096,\n",
       " 'human': 1543,\n",
       " 'mind': 716,\n",
       " 'years': 3996,\n",
       " 'Well': 2007,\n",
       " 'coming': 702,\n",
       " 'labs': 46,\n",
       " 'now': 3838,\n",
       " 'generation': 291,\n",
       " 'possibility': 116,\n",
       " 'envision': 20,\n",
       " 'being': 2088,\n",
       " 'difficult': 416,\n",
       " 'had': 5446,\n",
       " 'spaceship': 15,\n",
       " 'shrink': 13,\n",
       " 'down': 2110,\n",
       " 'inject': 11,\n",
       " 'bloodstream': 6,\n",
       " 'terribly': 27,\n",
       " 'could': 3637,\n",
       " 'attacked': 31,\n",
       " 'white': 289,\n",
       " 'blood': 281,\n",
       " 'cells': 745,\n",
       " 'arteries': 21,\n",
       " 'my': 8756,\n",
       " 'colleague': 50,\n",
       " 'Peter': 61,\n",
       " 'invasively': 5,\n",
       " 'using': 956,\n",
       " 'MRI': 58,\n",
       " 'don': 4048,\n",
       " 'anything': 715,\n",
       " 'need': 2656,\n",
       " 'radiation': 72,\n",
       " 'anatomy': 24,\n",
       " 'literally': 302,\n",
       " 'his': 2725,\n",
       " 'but': 7319,\n",
       " 'importantly': 134,\n",
       " 'When': 1302,\n",
       " 'moves': 111,\n",
       " 'yellow': 110,\n",
       " 'spot': 105,\n",
       " 'interface': 96,\n",
       " 'functioning': 27,\n",
       " 'place': 1190,\n",
       " 'Now': 3895,\n",
       " 'seen': 663,\n",
       " 'before': 1434,\n",
       " 'electrodes': 35,\n",
       " 'robotic': 58,\n",
       " 'imaging': 61,\n",
       " 'scanners': 8,\n",
       " 'insides': 6,\n",
       " 'brains': 223,\n",
       " 'What': 2594,\n",
       " 'process': 701,\n",
       " 'typically': 80,\n",
       " 'taken': 388,\n",
       " 'days': 669,\n",
       " 'months': 558,\n",
       " 'analysis': 101,\n",
       " 'collapsed': 41,\n",
       " 'through': 2162,\n",
       " 'milliseconds': 15,\n",
       " 'allows': 202,\n",
       " 'let': 1395,\n",
       " 'he': 5443,\n",
       " 'scanner': 48,\n",
       " 'He': 1982,\n",
       " 'points': 202,\n",
       " 'activation': 10,\n",
       " 'per': 419,\n",
       " 'second': 878,\n",
       " 'If': 1942,\n",
       " 'pattern': 201,\n",
       " 'own': 1609,\n",
       " 'learn': 766,\n",
       " 'been': 3477,\n",
       " 'three': 1947,\n",
       " 'ways': 729,\n",
       " 'try': 970,\n",
       " 'impact': 276,\n",
       " 'therapist': 23,\n",
       " 'couch': 23,\n",
       " 'pills': 33,\n",
       " 'knife': 30,\n",
       " 'fourth': 92,\n",
       " 'alternative': 83,\n",
       " 'soon': 273,\n",
       " 'know': 5711,\n",
       " 'deep': 282,\n",
       " 'channels': 34,\n",
       " 'minds': 183,\n",
       " 'Chronic': 1,\n",
       " 'example': 1149,\n",
       " 'burn': 65,\n",
       " 'yourself': 430,\n",
       " 'pull': 175,\n",
       " 'away': 1017,\n",
       " 'if': 6432,\n",
       " 'circuits': 45,\n",
       " 'producing': 91,\n",
       " 'no': 3138,\n",
       " 'helping': 139,\n",
       " 'D': 303,\n",
       " 'watch': 322,\n",
       " 'information': 997,\n",
       " 'then': 4371,\n",
       " 'select': 22,\n",
       " 'flex': 5,\n",
       " 'bicep': 1,\n",
       " 'seeing': 387,\n",
       " 'selected': 28,\n",
       " 'pathways': 28,\n",
       " 'chronic': 23,\n",
       " 'patient': 290,\n",
       " 'may': 1011,\n",
       " 'shock': 61,\n",
       " 'reading': 198,\n",
       " 'person': 1046,\n",
       " 'watching': 219,\n",
       " 'controlling': 53,\n",
       " 'pathway': 21,\n",
       " 'produces': 53,\n",
       " 'learning': 452,\n",
       " 'releases': 27,\n",
       " 'endogenous': 1,\n",
       " 'opiates': 2,\n",
       " 'As': 672,\n",
       " 'upper': 65,\n",
       " 'left': 853,\n",
       " 'display': 71,\n",
       " 'yoked': 1,\n",
       " 'controlled': 93,\n",
       " 'investigational': 1,\n",
       " 'clinical': 98,\n",
       " 'trials': 85,\n",
       " 'percent': 1776,\n",
       " 'decrease': 25,\n",
       " 'patients': 291,\n",
       " 'not': 9446,\n",
       " 'quot': 18923,\n",
       " 'Matrix': 4,\n",
       " 'only': 2636,\n",
       " 'too': 1174,\n",
       " 'want': 3717,\n",
       " 'aspects': 59,\n",
       " 'experiences': 146,\n",
       " 'These': 835,\n",
       " 'working': 1029,\n",
       " 'detail': 113,\n",
       " 'leave': 404,\n",
       " 'question': 1196,\n",
       " 'first': 3065,\n",
       " 'enter': 72,\n",
       " 'Where': 254,\n",
       " 'Beeban': 2,\n",
       " 'Kidron': 2,\n",
       " 'shared': 182,\n",
       " 'wonder': 173,\n",
       " 'film': 318,\n",
       " 'Movies': 2,\n",
       " 'power': 938,\n",
       " 'create': 984,\n",
       " 'narrative': 93,\n",
       " 'experience': 774,\n",
       " 'shape': 268,\n",
       " 'memories': 85,\n",
       " 'worldviews': 3,\n",
       " 'British': 129,\n",
       " 'director': 60,\n",
       " 'invokes': 2,\n",
       " 'iconic': 32,\n",
       " 'scenes': 31,\n",
       " 'amp': 684,\n",
       " 'lt': 236,\n",
       " 'em': 228,\n",
       " 'gt': 236,\n",
       " 'Miracle': 4,\n",
       " 'Milan': 6,\n",
       " 'Boyz': 2,\n",
       " 'n': 19,\n",
       " 'Hood': 7,\n",
       " 'she': 2763,\n",
       " 'shows': 497,\n",
       " 'FILMCLUB': 5,\n",
       " 'shares': 152,\n",
       " 'great': 1522,\n",
       " 'films': 77,\n",
       " 'kids': 986,\n",
       " 'Evidence': 3,\n",
       " 'suggests': 105,\n",
       " 'humans': 310,\n",
       " 'ages': 59,\n",
       " 'cultures': 96,\n",
       " 'identity': 151,\n",
       " 'From': 168,\n",
       " 'mother': 466,\n",
       " 'daughter': 161,\n",
       " 'preacher': 10,\n",
       " 'congregant': 1,\n",
       " 'teacher': 244,\n",
       " 'pupil': 3,\n",
       " 'storyteller': 22,\n",
       " 'Whether': 46,\n",
       " 'cave': 74,\n",
       " 'paintings': 57,\n",
       " 'latest': 65,\n",
       " 'uses': 180,\n",
       " 'Internet': 528,\n",
       " 'beings': 177,\n",
       " 'told': 774,\n",
       " 'histories': 15,\n",
       " 'truths': 33,\n",
       " 'parable': 8,\n",
       " 'fable': 4,\n",
       " 'inveterate': 1,\n",
       " 'storytellers': 11,\n",
       " 'where': 3645,\n",
       " 'increasingly': 109,\n",
       " 'secular': 29,\n",
       " 'fragmented': 16,\n",
       " 'offer': 119,\n",
       " 'communality': 4,\n",
       " 'unmediated': 2,\n",
       " 'furious': 4,\n",
       " 'consumerism': 6,\n",
       " 'history': 579,\n",
       " 'moral': 227,\n",
       " 'code': 230,\n",
       " 'imparting': 1,\n",
       " 'young': 653,\n",
       " 'Cinema': 3,\n",
       " 'arguably': 10,\n",
       " 'th': 416,\n",
       " 'century': 452,\n",
       " 'most': 2564,\n",
       " 'influential': 11,\n",
       " 'art': 545,\n",
       " 'Its': 34,\n",
       " 'artists': 152,\n",
       " 'stories': 565,\n",
       " 'across': 754,\n",
       " 'national': 210,\n",
       " 'boundaries': 55,\n",
       " 'many': 2484,\n",
       " 'languages': 114,\n",
       " 'genres': 4,\n",
       " 'philosophies': 6,\n",
       " 'Indeed': 53,\n",
       " 'hard': 739,\n",
       " 'find': 1604,\n",
       " 'yet': 684,\n",
       " 'tackle': 39,\n",
       " 'During': 32,\n",
       " 'last': 1446,\n",
       " 'decade': 157,\n",
       " 'vast': 126,\n",
       " 'integration': 27,\n",
       " 'media': 395,\n",
       " 'dominated': 32,\n",
       " 'culture': 424,\n",
       " 'Hollywood': 40,\n",
       " 'blockbuster': 5,\n",
       " 'offered': 64,\n",
       " 'diet': 99,\n",
       " 'sensation': 26,\n",
       " 'story': 1378,\n",
       " 'king': 57,\n",
       " 'common': 352,\n",
       " 'ago': 1240,\n",
       " 'telling': 296,\n",
       " 'between': 1258,\n",
       " 'generations': 101,\n",
       " 'rarified': 1,\n",
       " 'filmmaker': 30,\n",
       " 'worried': 89,\n",
       " 'puts': 99,\n",
       " 'fear': 335,\n",
       " 'God': 448,\n",
       " 'future': 888,\n",
       " 'build': 735,\n",
       " 'little': 2774,\n",
       " 'grasp': 31,\n",
       " 'narratives': 32,\n",
       " 'possible': 635,\n",
       " 'irony': 23,\n",
       " 'palpable': 5,\n",
       " 'technical': 87,\n",
       " 'access': 312,\n",
       " 'greater': 188,\n",
       " 'cultural': 163,\n",
       " 'weaker': 14,\n",
       " 'set': 612,\n",
       " 'organization': 185,\n",
       " 'ran': 157,\n",
       " 'weekly': 12,\n",
       " 'screenings': 2,\n",
       " 'schools': 263,\n",
       " 'followed': 109,\n",
       " 'discussions': 28,\n",
       " 'raid': 6,\n",
       " 'annals': 2,\n",
       " 'deliver': 119,\n",
       " 'meaning': 242,\n",
       " 'restless': 7,\n",
       " 'Given': 15,\n",
       " 'school': 1075,\n",
       " 'tiny': 263,\n",
       " 'rural': 106,\n",
       " 'hamlet': 2,\n",
       " 'project': 645,\n",
       " 'DVD': 29,\n",
       " 'onto': 234,\n",
       " 'nine': 246,\n",
       " 'clubs': 20,\n",
       " 'U': 664,\n",
       " '<EOS>K': 138,\n",
       " 'age': 616,\n",
       " 'groups': 267,\n",
       " 'five': 976,\n",
       " 'uninterrupted': 6,\n",
       " 'curated': 4,\n",
       " 'contextualized': 2,\n",
       " 'choice': 260,\n",
       " 'theirs': 15,\n",
       " 'quickly': 374,\n",
       " 'grew': 217,\n",
       " 'choose': 229,\n",
       " 'richest': 28,\n",
       " 'provide': 203,\n",
       " 'outcome': 65,\n",
       " 'immediate': 44,\n",
       " 'education': 555,\n",
       " 'profound': 114,\n",
       " 'transformative': 22,\n",
       " 'large': 488,\n",
       " 'discovered': 288,\n",
       " 'places': 501,\n",
       " 'perspectives': 28,\n",
       " 'By': 289,\n",
       " 'pilot': 51,\n",
       " 'finished': 109,\n",
       " 'names': 115,\n",
       " 'wished': 11,\n",
       " 'join': 143,\n",
       " 'changed': 452,\n",
       " 'life': 2884,\n",
       " 'Vittorio': 1,\n",
       " 'De': 8,\n",
       " 'Sica': 2,\n",
       " '': 7789,\n",
       " 'remarkable': 173,\n",
       " 'comment': 38,\n",
       " 'slums': 41,\n",
       " 'poverty': 210,\n",
       " 'aspiration': 11,\n",
       " 'occasion': 23,\n",
       " 'father': 407,\n",
       " 'birthday': 85,\n",
       " 'Technology': 33,\n",
       " 'meant': 214,\n",
       " 'viewing': 21,\n",
       " 'cinema': 19,\n",
       " 'pay': 343,\n",
       " 'print': 82,\n",
       " 'projectionist': 1,\n",
       " 'emotional': 166,\n",
       " 'artistic': 39,\n",
       " 'importance': 91,\n",
       " 'vision': 244,\n",
       " 'chose': 80,\n",
       " 'celebrate': 58,\n",
       " 'half': 759,\n",
       " 'teenage': 36,\n",
       " 'children': 1053,\n",
       " 'friends': 539,\n",
       " 'said': 3356,\n",
       " 'pass': 142,\n",
       " 'baton': 1,\n",
       " 'concern': 45,\n",
       " 'hope': 599,\n",
       " 'next': 1299,\n",
       " 'shot': 150,\n",
       " 'slum': 20,\n",
       " 'dwellers': 5,\n",
       " 'float': 21,\n",
       " 'skyward': 1,\n",
       " 'brooms': 1,\n",
       " 'Sixty': 10,\n",
       " 'after': 1456,\n",
       " 'made': 1750,\n",
       " 'saw': 729,\n",
       " 'faces': 136,\n",
       " 'tilt': 12,\n",
       " 'awe': 33,\n",
       " 'incredulity': 1,\n",
       " 'matching': 19,\n",
       " 'mine': 214,\n",
       " 'speed': 187,\n",
       " 'associate': 32,\n",
       " 'Slumdog': 1,\n",
       " 'Millionaire': 1,\n",
       " 'favelas': 27,\n",
       " 'Rio': 53,\n",
       " 'speaks': 40,\n",
       " 'enduring': 14,\n",
       " 'nature': 502,\n",
       " 'season': 57,\n",
       " 'democracy': 262,\n",
       " 'government': 556,\n",
       " 'screened': 7,\n",
       " 'Mr': 129,\n",
       " 'Smith': 65,\n",
       " 'Goes': 4,\n",
       " 'Washington': 105,\n",
       " 'Made': 7,\n",
       " 'older': 179,\n",
       " 'members': 136,\n",
       " 'grandparents': 46,\n",
       " 'Frank': 51,\n",
       " 'Capra': 1,\n",
       " 'classic': 64,\n",
       " 'values': 161,\n",
       " 'independence': 37,\n",
       " 'propriety': 3,\n",
       " 'right': 2974,\n",
       " 'heroically': 2,\n",
       " 'awkward': 36,\n",
       " 'expression': 107,\n",
       " 'faith': 104,\n",
       " 'political': 395,\n",
       " 'machine': 438,\n",
       " 'force': 291,\n",
       " 'honor': 86,\n",
       " 'Shortly': 9,\n",
       " 'became': 547,\n",
       " 'week': 360,\n",
       " 'night': 521,\n",
       " 'filibustering': 2,\n",
       " 'House': 49,\n",
       " 'Lords': 2,\n",
       " 'delight': 19,\n",
       " 'country': 978,\n",
       " 'explaining': 52,\n",
       " 'authority': 78,\n",
       " 'why': 1750,\n",
       " 'might': 1416,\n",
       " 'defy': 10,\n",
       " 'bedtime': 5,\n",
       " 'point': 1048,\n",
       " 'principle': 87,\n",
       " 'After': 246,\n",
       " 'Jimmy': 14,\n",
       " 'Stewart': 41,\n",
       " 'filibustered': 1,\n",
       " 'entire': 469,\n",
       " 'reels': 2,\n",
       " 'choosing': 45,\n",
       " 'Hotel': 11,\n",
       " 'Rwanda': 56,\n",
       " 'explored': 27,\n",
       " 'genocide': 33,\n",
       " 'brutal': 23,\n",
       " 'provoked': 9,\n",
       " 'tears': 52,\n",
       " 'well': 1786,\n",
       " 'incisive': 2,\n",
       " 'questions': 508,\n",
       " 'unarmed': 5,\n",
       " 'peace': 216,\n",
       " 'keeping': 112,\n",
       " 'double': 115,\n",
       " 'dealing': 81,\n",
       " 'Western': 181,\n",
       " 'society': 540,\n",
       " 'picks': 25,\n",
       " 'fights': 21,\n",
       " 'commodities': 18,\n",
       " 'Schindler': 3,\n",
       " 'List': 6,\n",
       " 'demanded': 13,\n",
       " 'forget': 148,\n",
       " 'child': 615,\n",
       " 'consciousness': 137,\n",
       " 'remarked': 3,\n",
       " 'already': 580,\n",
       " 'forgot': 37,\n",
       " 'otherwise': 96,\n",
       " 'did': 2389,\n",
       " 'lives': 853,\n",
       " 'got': 2723,\n",
       " 'palpably': 2,\n",
       " 'richer': 53,\n",
       " 'Pickpocket': 1,\n",
       " 'started': 1503,\n",
       " 'debate': 90,\n",
       " 'criminality': 2,\n",
       " 'disenfranchisement': 1,\n",
       " 'To': 358,\n",
       " 'Sir': 41,\n",
       " 'Love': 52,\n",
       " 'ignited': 2,\n",
       " 'teen': 8,\n",
       " 'celebrated': 25,\n",
       " 'attitude': 44,\n",
       " 'towards': 241,\n",
       " 'Britons': 1,\n",
       " 'railed': 1,\n",
       " 'against': 495,\n",
       " 'does': 1224,\n",
       " 'value': 392,\n",
       " 'collective': 119,\n",
       " 'unlike': 67,\n",
       " 'Sidney': 3,\n",
       " 'Poitier': 1,\n",
       " 'careful': 65,\n",
       " 'tutelage': 1,\n",
       " 'thoughtful': 23,\n",
       " 'opinionated': 1,\n",
       " 'curious': 94,\n",
       " 'thought': 1448,\n",
       " 'nothing': 622,\n",
       " 'tackling': 12,\n",
       " 'forms': 207,\n",
       " 'black': 405,\n",
       " 'subtitled': 1,\n",
       " 'documentary': 31,\n",
       " 'fantasy': 30,\n",
       " 'writing': 310,\n",
       " 'detailed': 49,\n",
       " 'reviews': 18,\n",
       " 'competed': 9,\n",
       " 'favor': 44,\n",
       " 'passionate': 99,\n",
       " 'sophisticated': 73,\n",
       " 'prose': 7,\n",
       " 'Six': 49,\n",
       " 'vying': 1,\n",
       " 'review': 47,\n",
       " 'until': 545,\n",
       " 'nearly': 137,\n",
       " 'quarter': 59,\n",
       " 'million': 957,\n",
       " 'numbers': 317,\n",
       " 'continue': 213,\n",
       " 'extraordinary': 200,\n",
       " 'critical': 191,\n",
       " 'questioning': 21,\n",
       " 'translated': 46,\n",
       " 'Some': 350,\n",
       " 'talking': 792,\n",
       " 'parents': 367,\n",
       " 'teachers': 277,\n",
       " 'without': 795,\n",
       " 'them': 6141,\n",
       " 'provided': 58,\n",
       " 'manner': 28,\n",
       " 'divide': 64,\n",
       " 'held': 136,\n",
       " 'Persepolis': 2,\n",
       " 'closer': 144,\n",
       " 'Iranian': 44,\n",
       " 'Jaws': 4,\n",
       " 'boy': 233,\n",
       " 'articulate': 18,\n",
       " 'experienced': 107,\n",
       " 'violence': 239,\n",
       " 'killed': 207,\n",
       " 'latter': 13,\n",
       " 'thrown': 56,\n",
       " 'overboard': 10,\n",
       " 'boat': 77,\n",
       " 'journey': 261,\n",
       " 'Who': 212,\n",
       " 'wrong': 672,\n",
       " 'under': 562,\n",
       " 'conditions': 207,\n",
       " 'Was': 36,\n",
       " 'tale': 36,\n",
       " 'hidden': 113,\n",
       " 'message': 262,\n",
       " 'How': 1211,\n",
       " 'different': 2278,\n",
       " 'tsunami': 24,\n",
       " ...}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the train Dataset of zh to en\n",
      "\n",
      "Example of Language #1 sentence: 我 现在 明白   我 从来 从来不 不是 一个   也 不是 另 一个  \n",
      "Example of Language #2 sentence: I see now , I never was one and not the other .\n",
      "\n",
      "Number of sentences in Language #1 = 213376\n",
      "Number of sentences in Language #2 = 213376\n",
      "\n",
      "Vocabulary sizes:\n",
      "zh 89203\n",
      "en 59328\n",
      "\n",
      "number of sentence pairs = 213376\n",
      "number of <EOS> tags in the source sentences = 212842\n",
      "number of <EOS> tags in the target sentences = 240005\n",
      "\n",
      "Random preprocessed sentence pair:\n",
      "['接下 接下来 下来 发生 的 是    她 说    请坐下 请坐下来 坐下 下来    安静 点   听 我 说 的 做   遵守 守规矩 规矩    抓紧 抓紧时间 时间   注意 一下    要 乖 点 像 个 女孩   <EOS>', 'And what happens is she says quot Please sit down be quiet do what you apos re told follow the rules manage your time focus be a girl <EOS> quot ']\n",
      "['当 这些 被 测量    反而   我们 发现 这个 速度 基本 基本上 是 一个 常数    作为 距离 的 函数  <EOS>', 'When those measurements are made instead what we find is that the speed is basically constant as a function of distance <EOS>']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData(lang1 = 'zh', \n",
    "                                             lang2 = 'en', \n",
    "                                             reverse = False, \n",
    "                                             dataset = \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vietnamese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the train Dataset of vi to en\n",
      "\n",
      "Example of Language #1 sentence: Điều này chính là 33 % còn thiếu của việc thành công bình_đẳng trong công_việc cho phụ_nữ chứ không phải là_vì chúng_ta thiếu năng_lực hay khả_năng cũng không phải vì chúng_ta thiếu lời_khuyên .\n",
      "Example of Language #2 sentence: This is that missing 33 percent of the career success equation for women , not because it &apos;s missing in our capabilities or abilities , but because it &apos;s missing in the advice that we &apos;re given .\n",
      "\n",
      "Number of sentences in Language #1 = 133317\n",
      "Number of sentences in Language #2 = 133317\n",
      "\n",
      "Vocabulary sizes:\n",
      "vi 16143\n",
      "en 47567\n",
      "\n",
      "number of sentence pairs = 133317\n",
      "number of <EOS> tags in the source sentences = 138953\n",
      "number of <EOS> tags in the target sentences = 148908\n",
      "\n",
      "Random preprocessed sentence pair:\n",
      "['Mot trong nhung nha tien phong vi du la tien sy Anthony Atala va ong ay ang lam cong viec phan tich te bao e tao nen cac bo phan co the ruot than <EOS> <EOS> <EOS>', 'So one of the pioneers for example is Dr <EOS> Anthony Atala and he has been working on layering cells to create body parts bladders valves kidneys <EOS>']\n",
      "\n",
      "Preparing the dev Dataset of vi to en\n",
      "\n",
      "Example of Language #1 sentence: Hiện_tại chúng_tôi dạy về kinh_doanh cho thiếu_niên lứa_tuổi 16 tại Northumberland , chúng_tôi khởi_đầu lớp_học bằng cách đưa cho chúng hai tờ đầu_tiên về tiểu_sử của Richard Branson , và công_việc cho những ở lứa_tuổi 16 này là gạch dưới , trong 2 trang đầu_tiên về tiểu_sử của Richard Branson bao_nhiêu lần Richard sử_dụng từ \" Tôi \" và bao_nhiêu lần ông sử_dụng từ \" Chúng_tôi . \"\n",
      "Example of Language #2 sentence: Now we teach entrepreneurship to 16-year-olds in Northumberland , and we start the class by giving them the first two pages of Richard Branson &apos;s autobiography , and the task of the 16-year-olds is to underline , in the first two pages of Richard Branson &apos;s autobiography how many times Richard uses the word &quot; I &quot; and how many times he uses the word &quot; we . &quot;\n",
      "\n",
      "Number of sentences in Language #1 = 1268\n",
      "Number of sentences in Language #2 = 1268\n",
      "\n",
      "Vocabulary sizes:\n",
      "vi 1369\n",
      "en 3815\n",
      "\n",
      "number of sentence pairs = 1268\n",
      "number of <EOS> tags in the source sentences = 1338\n",
      "number of <EOS> tags in the target sentences = 1408\n",
      "\n",
      "Random preprocessed sentence pair:\n",
      "['Co mot buc anh no chup nhieu the he phu nu trong nha tu gia en tre quay quan xung quanh mot em be a anh ong long toi vi nha toi cung co mot buc anh tuong tu ba ngoai me toi toi va ua con gai moi sinh tam anh treo tren tuong nha <EOS>', 'One in particular a photo of women of all ages from grandmother to little girl gathered around a baby struck a chord because a similar photo from my family my grandmother and mother myself and newborn daughter hangs on our wall <EOS>']\n",
      "\n",
      "Preparing the test Dataset of vi to en\n",
      "\n",
      "Example of Language #1 sentence: Trong một thế_giới , tôi là học_sinh gốc Á điển_hình , đòi_hỏi khắc_nghiệt từ chính mình .\n",
      "Example of Language #2 sentence: In one , I was the classic Asian student , relentless in the demands that I made on myself .\n",
      "\n",
      "Number of sentences in Language #1 = 1553\n",
      "Number of sentences in Language #2 = 1553\n",
      "\n",
      "Vocabulary sizes:\n",
      "vi 1324\n",
      "en 3618\n",
      "\n",
      "number of sentence pairs = 1553\n",
      "number of <EOS> tags in the source sentences = 1290\n",
      "number of <EOS> tags in the target sentences = 1651\n",
      "\n",
      "Random preprocessed sentence pair:\n",
      "['Voi nhung ieu o sai lam la bat kha khang <EOS>', 'Given all of that mistakes are inevitable <EOS>']\n"
     ]
    }
   ],
   "source": [
    "# Format: languagepair_language_dataset\n",
    "# Train \n",
    "vien_vi_train, vien_en_train, vi_en_train_pairs = prepareData('vi', 'en', False, dataset=\"train\")\n",
    "# Dev \n",
    "vien_vi_dev, vien_en_dev, vi_en_dev_pairs = prepareData('vi', 'en', False, dataset=\"dev\")\n",
    "# Test\n",
    "vien_vi_test, vien_en_test, vi_en_test_pairs = prepareData('vi', 'en', False, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Chinese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the train Dataset of zh to en\n",
      "\n",
      "Example of Language #1 sentence: 直到 午夜    接着 再次 入睡    大约 从 凌晨 2 点 直到 日出  \n",
      "Example of Language #2 sentence: until midnight and then again , they sleep from about 2 : 00 a.m. until sunrise .\n",
      "\n",
      "Number of sentences in Language #1 = 213376\n",
      "Number of sentences in Language #2 = 213376\n",
      "\n",
      "Vocabulary sizes:\n",
      "zh 89203\n",
      "en 59328\n",
      "\n",
      "number of sentence pairs = 213376\n",
      "number of <EOS> tags in the source sentences = 212842\n",
      "number of <EOS> tags in the target sentences = 240005\n",
      "\n",
      "Random preprocessed sentence pair:\n",
      "['然后 把 这些 喷 桩 从 沙丘 里 拔出 拔出来 出来    以 沙 为 模子    我们 几乎 能 在 沙丘 内部 创造 任何 我们 想要 的 形状  <EOS>', 'We then pull the piles up through the dune and we apos re able to create almost any conceivable shape inside of the sand with the sand acting as a mold as we go up <EOS>']\n",
      "\n",
      "Preparing the dev Dataset of zh to en\n",
      "\n",
      "Example of Language #1 sentence:   我 最后 爬出 了 矿井 得以 回家   而 这些 被 奴役 的 矿工 可能 永远 没有 回家 的 那 一天  \n",
      "Example of Language #2 sentence: I got to climb out of that hole , and I got to go home , but they likely never will , because they &apos;re trapped in slavery .\n",
      "\n",
      "Number of sentences in Language #1 = 1261\n",
      "Number of sentences in Language #2 = 1261\n",
      "\n",
      "Vocabulary sizes:\n",
      "zh 6135\n",
      "en 3915\n",
      "\n",
      "number of sentence pairs = 1261\n",
      "number of <EOS> tags in the source sentences = 1260\n",
      "number of <EOS> tags in the target sentences = 1398\n",
      "\n",
      "Random preprocessed sentence pair:\n",
      "['  你们 应该 看看 这些 垃圾     掌声      你们 应该 看看 我们 塞 了 多少 垃圾 给   这些 满怀 信任 的 非洲 非洲人 人民   <EOS>', 'You should see the rubbish You should see the rubbish that we have bestowed on unsuspecting African people <EOS>']\n",
      "\n",
      "Preparing the test Dataset of zh to en\n",
      "\n",
      "Example of Language #1 sentence:   但 我 还是 照常 的 继续 工作   \n",
      "Example of Language #2 sentence: But I carried on with my work .\n",
      "\n",
      "Number of sentences in Language #1 = 1397\n",
      "Number of sentences in Language #2 = 1397\n",
      "\n",
      "Vocabulary sizes:\n",
      "zh 5217\n",
      "en 3422\n",
      "\n",
      "number of sentence pairs = 1397\n",
      "number of <EOS> tags in the source sentences = 1396\n",
      "number of <EOS> tags in the target sentences = 1510\n",
      "\n",
      "Random preprocessed sentence pair:\n",
      "['  然后 我会 过去   只是 倾听  <EOS>', 'And I apos d go over there and I would I would just listen <EOS>']\n"
     ]
    }
   ],
   "source": [
    "# Format: languagepair_language_dataset\n",
    "# Train \n",
    "zhen_zh_train, zhen_en_train, zh_en_train_pairs = prepareData('zh', 'en', False, dataset=\"train\")\n",
    "# Dev \n",
    "zhen_zh_dev, zhen_en_dev, zh_en_dev_pairs = prepareData('zh', 'en', False, dataset=\"dev\")\n",
    "# Test\n",
    "zhen_zh_test, zhen_en_test, zh_en_test_pairs = prepareData('zh', 'en', False, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chinese word2index & index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10482"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_zh_train.word2index[\"格\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'格'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_zh_train.index2word[10482]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vietnamese word2index & index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6751"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_vi_train.word2index[\"Hamburger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hamburger'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_vi_train.index2word[6751]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English (Chinese) word2index & index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1450"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_en_train.word2index[\"translate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'translate'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_en_train.index2word[1450]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English (Vietnamese) word2index & index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "847"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_train.word2index[\"machine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_train.index2word[847]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Prepare Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"we\" in zhen_en_train.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_en_train.word2index[\"we\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_IDX = PAD_index # 0 \n",
    "SOS_IDX = SOS_index # 1\n",
    "UNK_IDX = UNK_index # 2\n",
    "EOS_IDX = EOS_index # 3\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(paired_tokens, \n",
    "                        lang1_token2id_vocab,\n",
    "                        lang2_token2id_vocab):\n",
    "    \"\"\"Takes as input:\n",
    "    - paired_tokens: a list of sentence pairs that consist of \n",
    "                    source & target lang sentences.\n",
    "    - lang1_token2id_vocab: token2index vocabulary for the first language. \n",
    "                            Get by method Lang_dataset.word2index\n",
    "    - lang2_token2id_vocab: token2index vocabulary for the second language. \n",
    "                            Get by method Lang_dataset.word2index\n",
    "                            \n",
    "    Returns:\n",
    "    - indices_data_lang_1, indices_data_lang2: A list of lists where each sub-list holds corresponding indices for each\n",
    "                                               token in the sentence.\"\"\"\n",
    "    indices_data_lang_1, indices_data_lang_2 = [], []\n",
    "    vocabs = [lang1_token2id_vocab, lang2_token2id_vocab]\n",
    "    \n",
    "    # lang1\n",
    "    for t in range(len(paired_tokens)):\n",
    "        # replaces token with UNK_IDX if the token is not in vocab\n",
    "        index_list = [vocabs[0][token] if token in vocabs[0]\\\n",
    "                                    else UNK_IDX for token in paired_tokens[t][0]] \n",
    "        indices_data_lang_1.append(index_list)\n",
    "    # lang2\n",
    "    for t in range(len(paired_tokens)):\n",
    "        index_list =  [vocabs[1][token] if token in vocabs[1] \\\n",
    "                                    else UNK_IDX for token in paired_tokens[t][1]] \n",
    "        indices_data_lang_2.append(index_list)\n",
    "     \n",
    "    # Documenting the transformation\n",
    "    random_sentence = random.randint(0, len(paired_tokens)-1)\n",
    "    print(\"Random sentence pair = \" + str(random_sentence))\n",
    "    print(paired_tokens[random_sentence])\n",
    "    print(\"Random token2indexized sentence pair:\")\n",
    "    print(\"Indexed source sentence:\" + str(indices_data_lang_1[random_sentence]))\n",
    "    print(\"Indexed target sentence:\" + str(indices_data_lang_2[random_sentence]))\n",
    "       \n",
    "    return indices_data_lang_1, indices_data_lang_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Datasets\n",
      "Zh -> En\n",
      "Random sentence pair = 106426\n",
      "['三天 后   我 从 我 的 朋友 兼 伙伴 兼 同事 比尔 尔克 克林顿 哪里 收到 一封   手写   精美 兼 很 长 的 一封 一封信 封信   内容 是    艾 尔   祝 新 餐馆 开张   <EOS>', 'Three days later I got a nice long handwritten letter from my friend and partner and colleague Bill Clinton saying quot Congratulations on the new restaurant Al <EOS> quot ']\n",
      "Random token2indexized sentence pair:\n",
      "Indexed source sentence:[4911, 4265, 2, 619, 2, 2, 2, 50, 2, 309, 2, 50, 2, 6, 2, 46940, 41876, 2, 13091, 2, 70631, 31598, 2, 13091, 2, 11658, 1997, 2, 161, 8108, 2, 8108, 6732, 2, 6732, 7771, 11418, 2, 237, 116, 2, 9557, 348, 2, 679, 8620, 2, 2, 2, 706, 6348, 2, 2, 2, 44009, 13454, 2, 13091, 2, 329, 2, 246, 2, 6, 2, 679, 8620, 2, 679, 8620, 9653, 2, 8620, 9653, 2, 2, 2, 2477, 12897, 2, 32, 2, 2, 2, 2, 6333, 2, 8108, 2, 2, 2, 8521, 2, 215, 2, 24277, 10850, 2, 5210, 397, 2, 2, 2, 2, 5869, 16125, 5790, 2]\n",
      "Indexed target sentence:[5690, 2, 46403, 8296, 8296, 2, 1736, 159, 8786, 25, 2, 14281, 159, 269, 8296, 46403, 2, 47, 2, 18566, 1264, 269, 2, 159, 2, 2108, 16910, 28418, 8296, 2, 14281, 1264, 2108, 18566, 2, 2, 159, 2108, 1736, 57480, 46403, 16910, 269, 269, 8296, 2108, 2, 14281, 8296, 269, 269, 8296, 46403, 2, 49097, 46403, 1264, 48, 2, 48, 8786, 2, 49097, 46403, 16910, 8296, 2108, 1736, 2, 159, 2108, 1736, 2, 10559, 159, 46403, 269, 2108, 8296, 46403, 2, 159, 2108, 1736, 2, 28418, 1264, 14281, 14281, 8296, 159, 18566, 2, 8296, 2, 5428, 16910, 14281, 14281, 2, 470, 14281, 16910, 2108, 269, 1264, 2108, 2, 25, 159, 8786, 16910, 2108, 18566, 2, 2, 2, 1264, 269, 2, 470, 1264, 2108, 18566, 46403, 159, 269, 2, 14281, 159, 269, 16910, 1264, 2108, 25, 2, 1264, 2108, 2, 269, 2, 8296, 2, 2108, 8296, 57480, 2, 46403, 8296, 25, 269, 159, 2, 46403, 159, 2108, 269, 2, 489, 14281, 2, 2, 836, 8251, 14222, 2, 2, 2, 2, 1264, 269, 2]\n",
      "\n",
      "Vi -> En\n",
      "Random sentence pair = 67940\n",
      "['Boi vi Small Hadron Collider a tung la mot thu quan trong <EOS>', 'Because the Small Hadron Collider once was the big thing <EOS>']\n",
      "Random token2indexized sentence pair:\n",
      "Indexed source sentence:[1506, 138, 435, 2, 1594, 435, 2, 2012, 1929, 46, 12702, 12702, 2, 2244, 46, 3053, 2, 138, 1831, 2, 1391, 138, 12702, 12702, 435, 3053, 10, 2, 2, 46, 2, 2683, 889, 1831, 5220, 2, 12702, 46, 2, 1929, 138, 2683, 2, 2683, 2844, 889, 2, 2, 889, 46, 1831, 2, 2683, 2, 138, 1831, 5220, 2, 2, 1578, 909, 2012, 2]\n",
      "Indexed target sentence:[4193, 10627, 22912, 9, 2, 114, 10627, 2, 295, 2, 10627, 2, 10698, 392, 9, 10761, 10761, 2, 13881, 9, 46, 2, 4151, 596, 2, 3598, 4151, 10761, 10761, 14361, 46, 10627, 2, 2, 4151, 596, 22912, 10627, 2, 46293, 9, 114, 2, 295, 2, 10627, 2, 22437, 14361, 14332, 2, 295, 2, 14361, 596, 14332, 2, 2, 4574, 7464, 10698, 2]\n",
      "\n",
      "\n",
      "Dev Datasets\n",
      "Zh -> En\n",
      "Random sentence pair = 95\n",
      "['  居高临下 意味 意味着 我 把 另 一文 文化 文化背景 背景 的 人   当做 我 的 仆人 对待   <EOS>', 'Patronizing I treat everybody from another culture as if they were my servants <EOS>']\n",
      "Random token2indexized sentence pair:\n",
      "Indexed source sentence:[2, 2, 2, 379, 2, 474, 2, 779, 2, 2, 779, 2, 53, 2, 4, 2, 123, 2, 787, 2, 767, 2, 2, 2, 2, 2, 2, 2, 2269, 2, 2, 2269, 2, 2, 18, 2, 108, 2, 2, 2, 348, 518, 2, 4, 2, 18, 2, 2, 108, 2, 242, 3846, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Indexed target sentence:[2, 31, 52, 2, 2, 2, 1212, 2, 1212, 2, 2, 2, 5, 2, 52, 2, 2, 31, 52, 2, 2, 2, 2, 2, 2, 2, 2, 1110, 2, 2, 2, 2, 2, 420, 2, 31, 2, 2, 52, 2, 2, 2, 2, 2, 2, 2, 52, 2, 2, 2, 2, 31, 200, 2, 1212, 2, 2, 52, 2, 2, 2, 2, 2, 2, 2, 2, 2, 420, 2, 2, 200, 2, 2, 2, 31, 2, 52, 200, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "Vi -> En\n",
      "Random sentence pair = 527\n",
      "['Trong suot bon thap ki nen chuyen che tan bao cua Gaddafi a pha huy co so ha tang cung nhu nen van hoa va nen tang ao uc cua xa hoi Libya <EOS>', 'For four decades Gaddafi apos s tyrannical regime destroyed the infrastructure as well as the culture and the moral fabric of Libyan society <EOS>']\n",
      "Random token2indexized sentence pair:\n",
      "Indexed source sentence:[2, 2, 84, 2, 2, 2, 2, 395, 84, 2, 2, 2, 84, 2, 2, 2, 2, 34, 2, 2, 2, 161, 2, 2, 46, 2, 2, 2, 2, 395, 409, 46, 2, 2, 2, 2, 46, 2, 2, 34, 2, 2, 2, 34, 84, 2, 2, 395, 34, 2, 2, 34, 2, 2, 34, 2, 161, 2, 34, 2, 2, 2, 34, 2, 2, 395, 409, 2, 2, 84, 2, 2, 84, 2, 2, 34, 2, 2, 34, 2, 2, 2, 2, 395, 2, 2, 2, 2, 2, 395, 2, 2, 46, 2, 2, 2, 34, 2, 2, 2, 84, 34, 2, 2, 34, 2, 2, 46, 2, 2, 2, 34, 2, 2, 2, 34, 84, 2, 395, 2, 2, 2, 395, 34, 2, 851, 34, 2, 2, 84, 161, 2, 1144, 161, 2, 409, 34, 2, 2, 2, 39, 2, 2]\n",
      "Indexed target sentence:[2, 2, 2, 2, 2, 2, 2, 2, 2, 585, 2, 2, 19, 585, 2, 96, 2, 2, 19, 585, 585, 19, 2, 2, 2, 19, 2, 2, 96, 2, 96, 2, 109, 2, 2, 19, 2, 2, 2, 2, 19, 2, 2, 2, 2, 2, 2, 533, 2, 2, 585, 2, 96, 109, 2, 2, 2, 2, 585, 2, 109, 2, 2, 2, 2, 2, 2, 2, 19, 96, 109, 2, 2, 2, 109, 2, 2, 2, 2, 19, 96, 2, 2, 2, 2, 2, 2, 19, 96, 2, 109, 2, 2, 2, 2, 2, 2, 109, 2, 2, 2, 2, 19, 2, 585, 2, 109, 2, 2, 2, 533, 2, 2, 19, 2, 2, 2, 19, 2, 2, 2, 2, 2, 2, 2, 2, 2836, 2, 2, 2, 19, 2, 2, 96, 2, 2, 2, 2, 109, 2, 2, 2, 1777, 2, 2, 2]\n",
      "\n",
      "\n",
      "Test Datasets\n",
      "Zh -> En\n",
      "Random sentence pair = 892\n",
      "['  除了 给 你 写 支票   我们 还 能 做 什么   <EOS>', 'Other than writing a check what could we do <EOS>']\n",
      "Random token2indexized sentence pair:\n",
      "Indexed source sentence:[2, 2, 2, 138, 2, 1197, 2, 230, 2, 416, 2, 2, 2, 2, 2, 2, 6, 629, 2, 180, 2, 412, 2, 68, 2, 2, 1223, 2, 2, 2, 2, 2, 2, 2322, 2]\n",
      "Indexed target sentence:[1419, 154, 2, 1087, 2, 2, 154, 2, 28, 2, 2, 2, 2, 2, 154, 2, 2, 2, 2, 28, 2, 2, 2, 1087, 2, 2, 2, 2, 2, 28, 154, 2, 2, 2, 2, 2, 1497, 2, 2, 1087, 2, 1497, 2, 2, 2, 2, 1419, 1646, 2]\n",
      "\n",
      "Vi -> En\n",
      "Random sentence pair = 1260\n",
      "['Nhung tren Citizens Connect moi thu eu uoc cong khai e tat ca co the thay <EOS>', 'But on Citizens Connect everything is public so everybody can see this <EOS>']\n",
      "Random token2indexized sentence pair:\n",
      "Indexed source sentence:[2, 2, 445, 2, 2, 2, 2, 2, 92, 2, 2, 2, 69, 2, 69, 2, 92, 2, 849, 2, 2, 70, 2, 2, 92, 2, 2, 2, 2, 70, 69, 2, 2, 2, 445, 2, 92, 445, 2, 445, 70, 2, 2, 2, 70, 2, 2, 2, 2, 2, 34, 69, 2, 92, 2, 2, 34, 2, 2, 2, 34, 2, 2, 70, 2, 2, 2, 92, 2, 2, 2, 34, 472, 2, 2, 2, 600, 1120, 2]\n",
      "Indexed target sentence:[2277, 2, 219, 2, 2, 2, 2, 3564, 2, 219, 2, 2, 3221, 2, 86, 2, 3564, 2, 2, 2, 3221, 2, 219, 2, 3221, 2, 3221, 2, 1503, 219, 2, 2, 2, 2, 2, 2, 86, 2, 2, 2, 2, 2, 2, 2, 2, 86, 2, 2, 3221, 2, 3221, 2, 1503, 2, 2, 738, 1503, 2, 2, 25, 2, 2, 86, 3221, 3221, 2, 219, 2, 2, 86, 2, 2, 2, 3121, 2788, 2]\n"
     ]
    }
   ],
   "source": [
    "# train indices\n",
    "print(\"\\n\\nTraining Datasets\")\n",
    "print(\"Zh -> En\")\n",
    "zhen_zh_train_indices, zhen_en_train_indices = token2index_dataset(zh_en_train_pairs,\n",
    "                                                                   zhen_zh_train.word2index,\n",
    "                                                                   zhen_en_train.word2index)\n",
    "\n",
    "print(\"\\nVi -> En\")\n",
    "vien_vi_train_indices, vien_en_train_indices = token2index_dataset(vi_en_train_pairs,\n",
    "                                                                   vien_vi_train.word2index,\n",
    "                                                                   vien_en_train.word2index)\n",
    "\n",
    "# dev indices\n",
    "print(\"\\n\\nDev Datasets\")\n",
    "print(\"Zh -> En\")\n",
    "zhen_zh_dev_indices, zhen_en_dev_indices = token2index_dataset(zh_en_dev_pairs,\n",
    "                                                               zhen_zh_dev.word2index,\n",
    "                                                               zhen_en_dev.word2index)\n",
    "print(\"\\nVi -> En\")\n",
    "vien_vi_dev_indices, vien_en_dev_indices = token2index_dataset(vi_en_dev_pairs,\n",
    "                                                               vien_vi_dev.word2index,\n",
    "                                                               vien_en_dev.word2index)\n",
    "\n",
    "# test indices\n",
    "print(\"\\n\\nTest Datasets\")\n",
    "print(\"Zh -> En\")\n",
    "zhen_zh_test_indices, zhen_en_test_indices = token2index_dataset(zh_en_test_pairs,\n",
    "                                                                 zhen_zh_test.word2index,\n",
    "                                                                 zhen_en_test.word2index)\n",
    "print(\"\\nVi -> En\")\n",
    "vien_vi_test_indices, vien_en_test_indices = token2index_dataset(vi_en_test_pairs,\n",
    "                                                                 vien_vi_test.word2index,\n",
    "                                                                 vien_en_test.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index2token(language,index_list):\n",
    "    word_sentence = []\n",
    "    for index in index_list:\n",
    "        word_sentence.append(language.index2word[index])\n",
    "        \n",
    "    return word_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gamer', '<UNK>', 'This', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'Singer', '<UNK>', 'This', '<UNK>', '<UNK>', 'certified', '<UNK>', 'put', '<UNK>', 'Singer', '<UNK>', '<UNK>', '<UNK>', 'certified', '<UNK>', 'This', '<UNK>', 'certified', '<UNK>', 'certified', '<UNK>', 'send', 'This', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'put', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'put', '<UNK>', '<UNK>', 'certified', '<UNK>', 'certified', '<UNK>', 'send', '<UNK>', '<UNK>', 'contextualized', 'send', '<UNK>', '<UNK>', 'on', '<UNK>', '<UNK>', 'put', 'certified', 'certified', '<UNK>', 'This', '<UNK>', '<UNK>', 'put', '<UNK>', '<UNK>', '<UNK>', 'Hawaiian', 'accepting', '<UNK>']\n"
     ]
    }
   ],
   "source": [
    "try_index = [2277, 2, 219, 2, 2, 2, 2, 3564, 2, 219, 2, 2, 3221, 2, 86, 2, 3564, 2, 2, 2, 3221, 2, 219, 2, 3221, 2, 3221, 2, 1503, 219, 2, 2, 2, 2, 2, 2, 86, 2, 2, 2, 2, 2, 2, 2, 2, 86, 2, 2, 3221, 2, 3221, 2, 1503, 2, 2, 738, 1503, 2, 2, 25, 2, 2, 86, 3221, 3221, 2, 219, 2, 2, 86, 2, 2, 2, 3121, 2788, 2]\n",
    "\n",
    "print(index2token(language = vien_en_train,\n",
    "            index_list = try_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese training sentence count = 213376\n",
      "Chinese-English (En) training sentence count = 213376\n",
      "\n",
      "Vietnamese training sentence count = 133317\n",
      "Vietnamese-English (En) training sentence count = 133317\n",
      "\n",
      "Chinese dev sentence count = 1261\n",
      "Chinese-English (En) dev sentence count = 1261\n",
      "\n",
      "Vietnamese dev sentence count = 1268\n",
      "Vietnamese-English (En) dev sentence count = 1268\n",
      "\n",
      "Chinese test sentence count = 1397\n",
      "Chinese-English (En) test sentence count = 1397\n",
      "\n",
      "Vietnamese test sentence count = 1553\n",
      "Vietnamese-English (En) test sentence count = 1553\n"
     ]
    }
   ],
   "source": [
    "# check length\n",
    "# train\n",
    "print (\"Chinese training sentence count = \"+str(len(zhen_zh_train_indices)))\n",
    "print (\"Chinese-English (En) training sentence count = \"+str(len(zhen_en_train_indices)))\n",
    "print (\"\\nVietnamese training sentence count = \"+str(len(vien_vi_train_indices)))\n",
    "print (\"Vietnamese-English (En) training sentence count = \"+str(len(vien_en_train_indices)))\n",
    "# dev\n",
    "print (\"\\nChinese dev sentence count = \"+str(len(zhen_zh_dev_indices)))\n",
    "print (\"Chinese-English (En) dev sentence count = \"+str(len(zhen_en_dev_indices)))\n",
    "print (\"\\nVietnamese dev sentence count = \"+str(len(vien_vi_dev_indices)))\n",
    "print (\"Vietnamese-English (En) dev sentence count = \"+str(len(vien_en_dev_indices)))\n",
    "# test\n",
    "print (\"\\nChinese test sentence count = \"+str(len(zhen_zh_test_indices)))\n",
    "print (\"Chinese-English (En) test sentence count = \"+str(len(zhen_en_test_indices)))\n",
    "print (\"\\nVietnamese test sentence count = \"+str(len(vien_vi_test_indices)))\n",
    "print (\"Vietnamese-English (En) test sentence count = \"+str(len(vien_en_test_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading zhen_train_dataset:\n",
      "self.source_sentences = [321, 7912, 2, 7912, 310, 2, 4, 2, 1586, 23701, 2, 2, 2, 275, 49581, 2, 2, 2, 5915, 6331, 2, 2, 5868, 16124, 5789, 2]\n",
      "self.target_sentences = [4216, 16909, 49096, 8295, 2, 16909, 2107, 2, 268, 2, 8295, 2, 1735, 8295, 8295, 10558, 2, 1263, 28417, 8295, 158, 2107, 23]\n",
      "\n",
      "Loading zhen_dev_dataset:\n",
      "self.source_sentences = [2, 2, 1232, 1232, 2, 4, 2, 55, 91, 2, 2, 2, 2, 2695, 650, 2, 650, 14, 2, 766, 678, 2, 4782, 2, 2, 2, 281, 2, 2, 2, 27, 2802, 2, 430, 729, 2, 14, 2, 2, 2, 2, 16, 2, 4733, 5952, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "self.target_sentences = [2, 2, 2, 2, 2, 3, 2, 2, 30, 199, 2, 3, 2, 2, 2, 419, 2, 419, 2, 2, 2, 2, 2, 30, 2, 1211, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 419, 2, 2, 2, 1211, 2, 2, 2, 51, 2, 2, 51, 2, 2, 2, 199, 2, 2, 2, 1109, 2, 2, 2, 2, 2, 2, 2, 2, 1211, 2, 2, 419, 2, 2, 2, 2, 2, 199, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "Loading vien_train_dataset:\n",
      "self.source_sentences = [3018, 2843, 137, 44, 2, 2843, 137, 1836, 2, 44, 1830, 5219, 2, 1082, 44, 888, 2, 1928, 137, 2682, 2, 2682, 434, 8, 888, 2, 8, 2, 1593, 8, 2, 8953, 2843, 434, 2, 2843, 44, 888]\n",
      "self.target_sentences = [7417, 7, 22911, 2, 10626, 10760, 2, 6843, 14360, 35190, 10626, 2, 4493, 2, 10626, 2, 113, 22911, 14360, 10626, 595, 22911, 10626, 2, 22436, 10626, 2, 14360, 595, 45, 2, 7, 2, 22911, 10760, 14360, 391, 7, 294, 10626, 2, 2, 10626, 7, 45, 10760, 14360, 595, 10626]\n",
      "\n",
      "Loading vien_dev_dataset:\n",
      "self.source_sentences = [2, 2, 160, 2, 2, 83, 160, 2, 2, 83, 2, 2, 2, 2, 83, 2, 2, 83, 160, 2, 2, 2, 2, 160, 2, 2, 33, 2, 2, 2, 2, 33, 2, 2, 2, 160, 45, 394, 2, 2, 160, 45, 2, 2, 2, 33, 2, 33, 2, 2, 2, 394, 83, 2, 2, 2, 83, 2, 2, 2, 2, 33, 2, 2, 2, 2, 45, 2, 2, 2, 2, 45, 2, 2, 160, 83, 160, 2, 2, 33, 2, 2, 83, 160, 2, 2, 2, 394, 83, 2, 2, 2, 2, 33, 2, 2, 2, 33, 160, 2, 1325, 2, 394, 2, 2, 2, 2, 33, 2, 2, 2, 33, 2, 2, 2, 2, 83, 2, 2, 160, 2, 2, 2, 33, 160, 2, 2, 2, 45, 2, 2, 2, 160, 2, 2, 2, 38, 2, 2, 2]\n",
      "self.target_sentences = [2, 2, 2, 2, 2, 3, 2, 2, 17, 95, 2, 2, 2, 108, 108, 2, 2, 2, 3, 2, 108, 2, 2, 2, 2, 2, 108, 2, 532, 2, 2, 2, 2, 2, 2, 108, 2, 2, 2, 2, 17, 95, 2, 108, 2, 2, 2, 2, 2, 95, 108, 2, 2, 2, 2, 108, 2, 2, 2, 2, 2, 17, 2, 2, 108, 2, 17, 2, 584, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 95, 2, 2, 2, 2, 2, 2, 2, 17, 2, 95, 2, 2, 2, 2, 2, 17, 2, 2, 2, 584, 2, 2, 2, 2, 108, 2, 2, 2, 108, 2, 2, 2, 2, 2, 2, 2, 2, 1776, 2, 2, 2, 2, 2, 1776, 2, 2, 2, 2, 2, 2, 2, 108, 2]\n"
     ]
    }
   ],
   "source": [
    "## TODO \n",
    "\n",
    "MAX_SENTENCE_LENGTH = 15\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# zhen token2index vocabs\n",
    "zhen_zh_train_token2id = zhen_zh_train.word2index\n",
    "zhen_en_train_token2id = zhen_en_train.word2index\n",
    "\n",
    "# vien token2index vocabs\n",
    "vien_vi_train_token2id = vien_vi_train.word2index\n",
    "vien_en_train_token2id = vien_en_train.word2index\n",
    "\n",
    "class TranslationDataset():\n",
    "    \"\"\"\n",
    "    Class that represents a train/dev/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 data_source, # training indices data of the source language\n",
    "                 data_target, # training indices data of the target language\n",
    "                 token2id_source=None, # token2id dict of the source language\n",
    "                 token2id_target=None  # token2id dict of the target language\n",
    "                ):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.source_sentences = data_source\n",
    "        self.target_sentences = data_target\n",
    "        print(\"self.source_sentences = \" + str(self.source_sentences[0]))\n",
    "        print(\"self.target_sentences = \" + str(self.target_sentences[0]))\n",
    "        \n",
    "        self.token2id_source = token2id_source\n",
    "        self.token2id_target = token2id_target\n",
    "        # prints the mandarin token -> # dictionary\n",
    "        \"\"\"\n",
    "        It also contains wrong tokenized words & Latin Alphabet words too\n",
    "        '11<EOS>': 13275, '为啥': 13276, '指责': 13277, '超载': 13278, \n",
    "        '复杂度': 13279, '小玩意': 13280, '小玩意儿': 13281, '此行': 13282, \n",
    "        '断面': 13283, '强制': 13284, '制发': 13285, '花瓶': 13286,\n",
    "        '糖': 13287, '年缴': 13288, '缴纳': 13289, '会费': 13290,\n",
    "        '99': 13291, 'Photoshop': 13292, '4000<EOS>': 13293, \n",
    "        '升级': 13294, '佯': 13295, '谬': 13296, 'Microsoft': 13297, \n",
    "        'Word': 13298, '文字': 13299,\n",
    "        \"\"\"\n",
    "#         print(\"self.token2id_source = \" + str(self.token2id_source))\n",
    "        # prints the english token -> # dictionary\n",
    "#         print(\"self.token2id_target = \" + str(self.token2id_target))        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def __getitem__(self, batch_index):\n",
    "\n",
    "#         source_word_idx, target_word_idx = [], []\n",
    "        source_mask, target_mask = [], []\n",
    "        \n",
    "        for index in self.source_sentences[batch_index][:MAX_SENTENCE_LENGTH]:\n",
    "            if index != UNK_IDX:\n",
    "                source_mask.append(0)\n",
    "            else:\n",
    "                source_mask.append(1)\n",
    "                \n",
    "        for index in self.target_sentences[batch_index][:MAX_SENTENCE_LENGTH]:\n",
    "            if index != UNK_IDX:\n",
    "                target_mask.append(0)\n",
    "            else:\n",
    "                target_mask.append(1)\n",
    "        \n",
    "        source_indices = self.source_sentences[batch_index][:MAX_SENTENCE_LENGTH]\n",
    "        target_indices = self.target_sentences[batch_index][:MAX_SENTENCE_LENGTH]\n",
    "        \n",
    "        source_list = [source_indices, source_mask, len(source_indices)]\n",
    "        target_list = [target_indices, target_mask, len(target_indices)]\n",
    "        \n",
    "        return source_list + target_list\n",
    "\n",
    "    \n",
    "def translation_collate(batch, max_sentence_length):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the \n",
    "    batch so that all data have the same length\n",
    "    \"\"\"\n",
    "    source_data, target_data = [], []\n",
    "    source_mask, target_mask = [], []\n",
    "    source_lengths, target_lengths = [], []\n",
    "\n",
    "    for datum in batch:\n",
    "        source_lengths.append(datum[2])\n",
    "        target_lengths.append(datum[5])\n",
    "        \n",
    "        # PAD\n",
    "        source_data_padded = np.pad(np.array(datum[0]), \n",
    "                                    pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", \n",
    "                                    constant_values=0)\n",
    "        source_data.append(source_data_padded)\n",
    "        \n",
    "        source_mask_padded = np.pad(np.array(datum[1]), \n",
    "                                    pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", \n",
    "                                    constant_values=0)\n",
    "        source_mask.append(source_mask_padded)\n",
    "        \n",
    "        target_data_padded = np.pad(np.array(datum[3]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        target_data.append(target_data_padded)\n",
    "        \n",
    "        target_mask_padded = np.pad(np.array(datum[4]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])),\n",
    "                               mode=\"constant\", constant_values=0)\n",
    "        target_mask.append(target_mask_padded)\n",
    "        \n",
    "    ind_dec_order = np.argsort(source_lengths)[::-1]\n",
    "    source_data = np.array(source_data)[ind_dec_order]\n",
    "    target_data = np.array(target_data)[ind_dec_order]\n",
    "    source_mask = np.array(source_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    target_mask = np.array(target_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    source_lengths = np.array(source_lengths)[ind_dec_order]\n",
    "    target_lengths = np.array(target_lengths)[ind_dec_order]\n",
    "    \n",
    "    source_list = [torch.from_numpy(source_data), \n",
    "               torch.from_numpy(source_mask).float(), source_lengths]\n",
    "    target_list = [torch.from_numpy(target_data), \n",
    "               torch.from_numpy(target_mask).float(), target_lengths]\n",
    "        \n",
    "    return source_list + target_list\n",
    "\n",
    "print(\"Loading zhen_train_dataset:\")\n",
    "zhen_train_dataset = TranslationDataset(zhen_zh_train_indices,\n",
    "                                       zhen_en_train_indices,\n",
    "                                       token2id_source=zhen_zh_train_token2id,\n",
    "                                       token2id_target=zhen_en_train_token2id)\n",
    "\n",
    "zhen_train_loader = torch.utils.data.DataLoader(dataset=zhen_train_dataset,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                               shuffle=False)\n",
    "\n",
    "print(\"\\nLoading zhen_dev_dataset:\")\n",
    "zhen_dev_dataset = TranslationDataset(zhen_zh_dev_indices,\n",
    "                                       zhen_en_dev_indices,\n",
    "                                       token2id_source=zhen_zh_train_token2id,\n",
    "                                       token2id_target=zhen_en_train_token2id)\n",
    "\n",
    "zhen_dev_loader = torch.utils.data.DataLoader(dataset=zhen_dev_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                             shuffle=False)\n",
    "\n",
    "print(\"\\nLoading vien_train_dataset:\")\n",
    "vien_train_dataset = TranslationDataset(vien_vi_train_indices,\n",
    "                                       vien_en_train_indices,\n",
    "                                       token2id_source=vien_vi_train_token2id,\n",
    "                                       token2id_target=vien_en_train_token2id)\n",
    "\n",
    "vien_train_loader = torch.utils.data.DataLoader(dataset=vien_train_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                             shuffle=False)\n",
    "\n",
    "print(\"\\nLoading vien_dev_dataset:\")\n",
    "vien_dev_dataset = TranslationDataset(vien_vi_dev_indices,\n",
    "                                       vien_en_dev_indices,\n",
    "                                       token2id_source=vien_vi_train_token2id,\n",
    "                                       token2id_target=vien_en_train_token2id)\n",
    "\n",
    "vien_dev_loader = torch.utils.data.DataLoader(dataset=vien_dev_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                             shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "#### SAMPLE DATASET - CLEAR OUT LATER ####\n",
    "##########################################\n",
    "\n",
    "# zhen_train_dataset = TranslationDataset(zhen_zh_train_indices[:480], # 15 batches\n",
    "#                                        zhen_en_train_indices[:480],\n",
    "#                                        token2id_source=zhen_zh_train_token2id,\n",
    "#                                        token2id_target=zhen_en_train_token2id)\n",
    "\n",
    "# zhen_train_loader = torch.utils.data.DataLoader(dataset=zhen_train_dataset,\n",
    "#                                batch_size=BATCH_SIZE,\n",
    "#                                collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "#                                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Evaluation Metric\n",
    "\n",
    "We use BLEU as the evaluation metric. Specifically, we focus on the corpus-level BLEU function. \n",
    "\n",
    "The code for BLEU is taken from https://github.com/mjpost/sacreBLEU/blob/master/sacrebleu.py#L1022-L1080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /Users/atakanokan/anaconda/lib/python3.6/site-packages (1.2.12)\n",
      "Requirement already satisfied: typing in /Users/atakanokan/anaconda/lib/python3.6/site-packages (from sacrebleu) (3.6.6)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Beam Search Algorithm\n",
    "\n",
    "In this section, we implement the Beam Search algorithm in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model\n",
    "\n",
    "1. Recurrent neural network based encoder-decoder without attention\n",
    "2. Recurrent neural network based encoder-decoder with attention\n",
    "2. Replace the recurrent encoder with either convolutional or self-attention based encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reconstruction loss = binary cross entropy between two (vocab_size x 1) vectors\n",
    "# used during training, since we can compare the real Y and and the generated Y\n",
    "# still at each time step of the decoder, we compare up to and including\n",
    "# the real t-th token and the generated t-th, then optimize\n",
    "\n",
    "def loss_function(y_hat, y):\n",
    "    \n",
    "    \"\"\"Takes as input;\n",
    "    - y: correct \"log-softmax\"(binary vector) that represents the correct t-th token in the target sentence,\n",
    "                 (vocab_size x 1) vector\n",
    "    - y_hat: predicted LogSoftmax for the predicted t-th token in the target sentence.\n",
    "             (vocab_size x 1) vector\n",
    "    Returns;\n",
    "    - NLL Loss in training time\"\"\"\n",
    "#     y_hat = torch.log(y_hat) # log softmax\n",
    "    loss = nn.functional.binary_cross_entropy(y_hat,y)\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "# generation/inference time - validation loss = BLEU\n",
    "\n",
    "def compute_BLEU(corpus_hat,corpus):\n",
    "    ## TODO\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([7., 5., 4.]), tensor([3, 4, 1]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([3,4,2,7,5,3,2]).topk(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "class BeamSearch(nn.Module):\n",
    "    \n",
    "    \"\"\"network that conducts beam search over the outputs of\n",
    "     any translator network. The translator networks that can \n",
    "     be passed are:\n",
    "     \n",
    "     - Translate (for RNN-enc-dec),\n",
    "     - AttnTranslate (for RNN-enc-dec with attention),\n",
    "     - CNNtranslate (for CNN-encoder based translation).\n",
    "     \n",
    "     The translation networks take care of the encoder-decoder\n",
    "     choices specific to each task. Please see in below sections.\"\"\"\n",
    "\n",
    "    def __init__(self, translator_network, beam_size):\n",
    "        super().__init__()\n",
    "        # translator network that returns the logsoftmax\n",
    "        # over vocabulary size:(vocab_size, 1)\n",
    "        self.translator_network = translator_network\n",
    "        self.beam_size = beam_size\n",
    "        \n",
    "    def init_search_tree(self, batch_size):\n",
    "        beam_size = self.beam_size\n",
    "        self.search_tree = torch.empty(batch_size, beam_size, 1)\n",
    "        return self\n",
    "    \n",
    "    def init_score_tree(self, batch_size):\n",
    "        beam_size = self.beam_size\n",
    "        search_tree = self.search_tree\n",
    "        self.score_tree = torch.zeros(search_tree.size())\n",
    "        return self\n",
    "    \n",
    "    def forward(source_sentence, source_mask, source_lengths,\n",
    "                target_sentence, target_mask, target_lengths):\n",
    "        \n",
    "        self.init_search_tree(BATCH_SIZE)\n",
    "        self.init_score_tree(BATCH_SIZE)\n",
    "        \n",
    "        # at each time step the decoder will give us the logsoftmax\n",
    "        # of one token (batch_size, vocab_size). \n",
    "        output = model(source_sentence, target_sentence,source_mask, \n",
    "                       target_mask, source_lengths,target_lengths)\n",
    "        \n",
    "        # for each sentence in the batch we get the top k predictions\n",
    "        # for each token and append it to the search and score trees. \n",
    "        for i in range(BATCH_SIZE):\n",
    "            beam = output[i].topk(beam_size) # (token scores, token indices)\n",
    "            # cat instead\n",
    "            self.search_tree[i] = self.search_tree.cat(beam[1]) # cat the indices to the search tree\n",
    "            self.score_tree[i,:] = beam[0] # append the scores to the score tree \n",
    "        \n",
    "        # we will sum the logs \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: RNN-based Encoder-Decoder without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_softmax(tensor_of_indices,\n",
    "                       batch_size,\n",
    "                       vocab_size = len(zhen_en_train_token2id)):\n",
    "    \"\"\"\n",
    "    - takes as input a time_step vector of the batch (t-th token of each sentence in the batch)\n",
    "      size: (batch_size, 1)\n",
    "    - converts it to softmax of (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    index_tensor_ = tensor_of_indices.view(-1,1).long()\n",
    "        \n",
    "    one_hot = torch.FloatTensor(batch_size, vocab_size).zero_()\n",
    "    one_hot.scatter_(1, index_tensor_.detach().cpu(), 1)\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully self-attention Translation System / Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "\n",
    "    \n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)    \n",
    "\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "    \n",
    "    \n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    \n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "    \n",
    "    \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "    \n",
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "#             nn.init.xavier_uniform_(p)\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(10, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(10, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model = make_model(10, 10, 2)\n",
    "tmp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        \n",
    "        # creates a copy of the Variable\n",
    "        true_dist = x.data.clone()\n",
    "        print(\"true_dist = \" + str(true_dist))\n",
    "        \n",
    "        # Fills self tensor with the specified value\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2.0))\n",
    "        print(\"true_dist = \" + str(true_dist))\n",
    "        \n",
    "        # scatter_(dim, index, src) → Tensor\n",
    "        print(\"self.confidence = \" + str(self.confidence))\n",
    "        true_dist.scatter_(dim = 1, \n",
    "                           index = target.data.unsqueeze(1), \n",
    "                           src = self.confidence)\n",
    "        \n",
    "        true_dist[:, self.padding_idx] = 0.\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "#         if mask.dim() > 0:\n",
    "        if mask.sum() > 0 and len(mask) > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atakanokan/anaconda/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "crit = LabelSmoothing(5, 0, 0.1)\n",
    "def loss(x):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d],\n",
    "                                 ])\n",
    "    #print(predict)\n",
    "    return crit(Variable(predict.log()),\n",
    "                 Variable(torch.LongTensor([1]))).data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        data[:, 0] = 1\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        x_loss = x.contiguous().view(-1, x.size(-1)).float()\n",
    "        y_loss = y.contiguous().view(-1)\n",
    "        loss = self.criterion(x_loss, y_loss) / norm\n",
    "                              \n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.data[0] * norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atakanokan/anaconda/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_dist = tensor([[-1.9340, -4.1510, -2.6695,  ..., -2.6970, -3.7252, -1.2385],\n",
      "        [-2.3561, -1.1062, -3.0696,  ..., -2.7260, -3.8816, -1.6418],\n",
      "        [-3.7725, -2.3680, -4.6145,  ..., -4.4205, -4.0762, -1.8362],\n",
      "        ...,\n",
      "        [-2.7200, -1.7991, -4.6643,  ..., -1.3054, -5.5390, -1.0155],\n",
      "        [-3.3538, -1.7163, -2.9694,  ..., -2.2880, -3.2859, -2.1620],\n",
      "        [-4.2577, -1.2764, -2.6886,  ..., -2.4744, -2.3833, -1.3999]])\n",
      "true_dist = tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "self.confidence = 1.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "scatter_() received an invalid combination of arguments - got (src=float, index=Tensor, dim=int, ), but expected one of:\n * (int dim, Tensor index, Tensor src)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mdim=int\u001b[0m, \u001b[32;1mindex=Tensor\u001b[0m, \u001b[31;1msrc=float\u001b[0m, )\n * (int dim, Tensor index, Number value)\n      didn't match because some of the keywords were incorrect: src\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-85d1304a84b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m               loss_compute = SimpleLossCompute(generator = model.generator, \n\u001b[1;32m     22\u001b[0m                                                \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                                                opt = model_opt))\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     print(run_epoch(data_iter = data_gen(V, 30, 5), \n",
      "\u001b[0;32m<ipython-input-51-f8ba8985578c>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, model, loss_compute)\u001b[0m\n\u001b[1;32m      8\u001b[0m         out = model.forward(batch.src, batch.trg, \n\u001b[1;32m      9\u001b[0m                             batch.src_mask, batch.trg_mask)\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-9381dbd08ad8>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y, norm)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-7ead1d9e8554>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, target)\u001b[0m\n\u001b[1;32m     25\u001b[0m         true_dist.scatter_(dim = 1, \n\u001b[1;32m     26\u001b[0m                            \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                            src = self.confidence)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtrue_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: scatter_() received an invalid combination of arguments - got (src=float, index=Tensor, dim=int, ), but expected one of:\n * (int dim, Tensor index, Tensor src)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mdim=int\u001b[0m, \u001b[32;1mindex=Tensor\u001b[0m, \u001b[31;1msrc=float\u001b[0m, )\n * (int dim, Tensor index, Number value)\n      didn't match because some of the keywords were incorrect: src\n"
     ]
    }
   ],
   "source": [
    "V = 11\n",
    "\n",
    "criterion = LabelSmoothing(size=V, \n",
    "                           padding_idx=0, \n",
    "                           smoothing=0.0)\n",
    "\n",
    "model = make_model(V, V, N=2)\n",
    "\n",
    "model_opt = NoamOpt(model.src_embed[0].d_model, \n",
    "                    1, \n",
    "                    400,\n",
    "                    torch.optim.Adam(model.parameters(), \n",
    "                                     lr=0, \n",
    "                                     betas=(0.9, 0.98), \n",
    "                                     eps=1e-9))\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    run_epoch(data_iter = data_gen(V, 30, 20), \n",
    "              model = model, \n",
    "              loss_compute = SimpleLossCompute(generator = model.generator, \n",
    "                                               criterion = criterion, \n",
    "                                               opt = model_opt))\n",
    "    model.eval()\n",
    "    print(run_epoch(data_iter = data_gen(V, 30, 5), \n",
    "                    model = model, \n",
    "                    loss_compute = SimpleLossCompute(generator = model.generator, \n",
    "                                                     criterion = criterion, \n",
    "                                                     opt = None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
