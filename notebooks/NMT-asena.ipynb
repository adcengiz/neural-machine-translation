{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Spring 2018 NLP Class Project: Neural Machine Translation</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pdb\n",
    "import os\n",
    "from underthesea import word_tokenize\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# running on cpu\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install spacy && python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Project Overview\n",
    "\n",
    "The goal of this project is to build a neural machine translation system and experience how recent advances have made their way. Each team will build the following sequence of neural translation systems for two language pairs, __Vietnamese (Vi)→English (En)__ and __Chinese (Zh)→En__ (prepared corpora is be provided):\n",
    "\n",
    "1. Recurrent neural network based encoder-decoder without attention\n",
    "2. Recurrent neural network based encoder-decoder with attention\n",
    "2. Replace the recurrent encoder with either convolutional or self-attention based encoder.\n",
    "4. [Optional] Build either or both fully self-attention translation system or/and multilingual translation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Upload & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of sentence\n",
    "SOS_token = 1\n",
    "\n",
    "# end of sentence\n",
    "EOS_token = 3\n",
    "\n",
    "## 2 = unk\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"<PAD>\",1: \"<SOS>\", 2:\"<UNK>\", 3: \"<EOS>\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    \"\"\"About \"NFC\" and \"NFD\": \n",
    "    \n",
    "    For each character, there are two normal forms: normal form C \n",
    "    and normal form D. Normal form D (NFD) is also known as canonical \n",
    "    decomposition, and translates each character into its decomposed form. \n",
    "    Normal form C (NFC) first applies a canonical decomposition, then composes \n",
    "    pre-combined characters again.\n",
    "    \n",
    "    About unicodedata.category: \n",
    "    \n",
    "    Returns the general category assigned to the Unicode character \n",
    "    unichr as string.\"\"\"\n",
    "    \n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Trim\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False,\n",
    "             dataset=\"train\"):\n",
    "    \n",
    "    \"\"\"Takes as input;\n",
    "    - lang1, lang2: either (vi, en) or (zh, en)\n",
    "    - dataset: one of (\"train\",\"dev\",\"test\")\"\"\"\n",
    "    print(\"Reading lines...\")\n",
    "    eos = [\".\",\"?\",\"!\",\"\\n\"]\n",
    "    # Read the pretokenized lang1 file and split into lines\n",
    "    lang1_lines = open(\"../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-%s-%s-processed/%s.tok.%s\" % (lang1, lang2, dataset, lang1), encoding=\"utf-8\").\\\n",
    "        read().strip().split(\"\\n\")\n",
    "    # Read the lang2 file and split into lines\n",
    "    lang2_lines = open(\"../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-%s-%s-processed/%s.tok.%s\" % (lang1, lang2, dataset, lang2), encoding=\"utf-8\").\\\n",
    "        read().strip().split(\"\\n\")\n",
    "    \n",
    "    # create sentence pairs (lists of length 2 that consist of string pairs)\n",
    "    # e.g. [\"And we &apos;re going to tell you some stories from the sea here in video .\",\n",
    "    #       \"我们 将 用 一些 影片 来讲 讲述 一些 深海 海里 的 故事  \"]\n",
    "    # check if there are the same number of sentences in each set\n",
    "    assert len(lang1_lines) == len(lang2_lines), \"Two languages must have the same number of sentences. \"+ str(len(lang1_lines)) + \" sentences were passed for \" + str(lang1) + \".\" + str(len(lang2_lines)) + \" sentences were passed for \" + str(lang2)+\".\"\n",
    "    # normalize if not Chinese, Chinese normalization is already handeled\n",
    "    if lang1 == \"zh\":\n",
    "        lang1_lines = [s + \"<EOS>\" for s in lang1_lines]\n",
    "    else:\n",
    "        lang1_lines = [normalizeString(s).replace(\".\",\"<EOS>\").\\\n",
    "                       replace(\"?\",\"<EOS>\").replace(\"!\",\"<EOS>\").replace(\"\\n\",\"<EOS>\") for s in lang1_lines]\n",
    "    lang2_lines = [normalizeString(s).replace(\".\",\"<EOS>\").\\\n",
    "                       replace(\"?\",\"<EOS>\").replace(\"!\",\"<EOS>\").replace(\"\\n\",\"<EOS>\") for s in lang2_lines]\n",
    "    # construct pairs\n",
    "    pair_ran = range(len(lang1_lines))\n",
    "    pairs = [[lang1_lines[i]] + [lang2_lines[i]] for i in pair_ran]\n",
    "    \n",
    "#     # Split every line into pairs and normalize\n",
    "#     pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 133317 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 16142\n",
      "en 47566\n",
      "['Khoa hoc va ky thuat uoc cai thien du chung ta co muon hay khong se cho chung ta ngay cang nhieu hieu ung on bay e tac ong toi hanh tinh <EOS> Nham muc ich ieu khien hanh tinh <EOS> Cho chung ta kha nang ieu khien thoi tiet va khi hau khong han vi chung ta len ke hoach khong phai vi chung ta muon ma chi boi vi khoa hoc a cung cap no cho chung ta voi hieu biet nhieu hon ve cach ma he thong lam viec va voi cac thiet bi ky thuat tien tien hon <EOS>', 'This improved science and engineering will whether we like it or not give us more and more leverage to affect the planet to control the planet to give us weather and climate control not because we plan it not because we want it just because science delivers it to us bit by bit with better knowledge of the way the system works and better engineering tools to effect it <EOS>']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False, dataset=\"train\"):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse, dataset=dataset)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# example\n",
    "input_lang, output_lang, pairs = prepareData('vi', 'en', False, dataset=\"train\")\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213376 sentence pairs\n",
      "Trimmed to 213376 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 89202\n",
      "en 59327\n",
      "['马丁   赛 利 格 曼 来 和 大家 谈一谈 谈心 心理 心理学 理学   心理 心理学 理学 既是 一门 学科   也 可以 体现 为 病人   医师 之间 一对 一对一 的 交流 交流平台 平台   而 在 关于 疾病 的 研究 以外   现代 心理 心理学 理学 还 可以 在 哪些 哪些方面 方面 帮助 我们 呢  <EOS>', 'Martin Seligman talks about psychology as a field of study and as it works one on one with each patient and each practitioner <EOS> As it moves beyond a focus on disease what can modern psychology help us to become <EOS>']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('zh', 'en', False, dataset=\"train\")\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vietnamese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Please find the original tokenizing code provided by Elman Mansimov in the following link:\n",
    "# # https://github.com/derincen/neural-machine-translation/tree/master/data/tokens_and_preprocessing_em/preprocess_translation\n",
    "\n",
    "# def tokenize_vi(f_names, f_out_names):\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             tok_lines.write(word_tokenize(sentence, format=\"text\") + '\\n')\n",
    "#         tok_lines.close()\n",
    "\n",
    "# def tokenize_en(f_names, f_out_names):\n",
    "#     tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             # replaced tokenizer(sentence) with str(tokenizer(sentence)) to avoid \n",
    "#             # type error while joining\n",
    "#             tok_lines.write(' '.join(str(tokenizer(sentence))) + '\\n')\n",
    "#         tok_lines.close()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     root = '../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-vi-en-processed/'\n",
    "#     tokenize_vi([os.path.join(root, 'train.vi'), os.path.join(root, 'dev.vi'), \n",
    "#                  os.path.join(root, 'test.vi')],\\\n",
    "#                [os.path.join(root, 'train.tok.vi'), os.path.join(root, 'dev.tok.vi'), \n",
    "#                 os.path.join(root, 'test.tok.vi')])\n",
    "\n",
    "#     tokenize_en([os.path.join(root, 'train.en'), os.path.join(root, 'dev.en'), \n",
    "#                  os.path.join(root, 'test.en')],\\\n",
    "#                 [os.path.join(root, 'train.tok.en'), os.path.join(root, 'dev.tok.en'), \n",
    "#                  os.path.join(root, 'test.tok.en')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 133317 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 16142\n",
      "en 47566\n",
      "Reading lines...\n",
      "Read 1268 sentence pairs\n",
      "Trimmed to 1268 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 1368\n",
      "en 3814\n",
      "Reading lines...\n",
      "Read 1553 sentence pairs\n",
      "Trimmed to 1553 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 1323\n",
      "en 3617\n"
     ]
    }
   ],
   "source": [
    "# Format: languagepair_language_dataset\n",
    "# Train \n",
    "vien_vi_train, vien_en_train, vi_en_train_pairs = prepareData('vi', 'en', False, dataset=\"train\")\n",
    "# Dev \n",
    "vien_vi_dev, vien_en_dev, vi_en_dev_pairs = prepareData('vi', 'en', False, dataset=\"dev\")\n",
    "# Test\n",
    "vien_vi_test, vien_en_test, vi_en_test_pairs = prepareData('vi', 'en', False, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Chinese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Please find the original tokenizing code provided by Elman Mansimov in the following link:\n",
    "# # https://github.com/derincen/neural-machine-translation/tree/master/data/tokens_and_preprocessing_em/preprocess_translation\n",
    "\n",
    "# def tokenize_zh(f_names, f_out_names):\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             tok_lines.write(' '.join(jieba.cut(sentence, cut_all=True)))\n",
    "#         tok_lines.close()\n",
    "\n",
    "# def tokenize_en(f_names, f_out_names):\n",
    "#     tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             # replaced tokenizer(sentence) with str(tokenizer(sentence)) to avoid \n",
    "#             # type error while joining\n",
    "#             tok_lines.write(' '.join(str(tokenizer(sentence))) + '\\n')\n",
    "#         tok_lines.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     root = '../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-zh-en-processed/'\n",
    "#     tokenize_zh([os.path.join(root, 'dev.zh'), os.path.join(root, 'test.zh'), os.path.join(root, 'train.zh')],\\\n",
    "#                 [os.path.join(root, 'dev.tok.zh'), os.path.join(root, 'test.tok.zh'), os.path.join(root, 'train.tok.zh')])\n",
    "\n",
    "# #     tokenize_en([os.path.join(root, 'dev.en'), os.path.join(root, 'test.en'), os.path.join(root, 'train.en')],\\\n",
    "# #                [os.path.join(root, 'dev.tok.en'), os.path.join(root, 'test.tok.en'), os.path.join(root, 'train.tok.en')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213376 sentence pairs\n",
      "Trimmed to 213376 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 89202\n",
      "en 59327\n",
      "Reading lines...\n",
      "Read 1261 sentence pairs\n",
      "Trimmed to 1261 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 6134\n",
      "en 3914\n",
      "Reading lines...\n",
      "Read 1397 sentence pairs\n",
      "Trimmed to 1397 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 5216\n",
      "en 3421\n"
     ]
    }
   ],
   "source": [
    "# Format: languagepair_language_dataset\n",
    "# Train \n",
    "zhen_zh_train, zhen_en_train, zh_en_train_pairs = prepareData('zh', 'en', False, dataset=\"train\")\n",
    "# Dev \n",
    "zhen_zh_dev, zhen_en_dev, zh_en_dev_pairs = prepareData('zh', 'en', False, dataset=\"dev\")\n",
    "# Test\n",
    "zhen_zh_test, zhen_en_test, zh_en_test_pairs = prepareData('zh', 'en', False, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['深海 海中 的 生命   大卫   盖罗 <EOS>', 'Life in the deep oceans']"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_en_train_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Check Source & Target Vocabs\n",
    "\n",
    "Since the source and target languages can have very different table lookup layers, it's good practice to have separate vocabularies for each. Thus, we build vocabularies for each language that we will be using. \n",
    "\n",
    "In the first class (Lang) of this section, we have already defined vocabularies for all languages. So, there is no need to redefine another function. We chech each vocabulary below.\n",
    "\n",
    "#### Chinese Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in Chinese training corpus is 89202\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in Chinese training corpus is \" + str(zhen_zh_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10481"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_zh_train.word2index[\"格\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'哈利'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_zh_train.index2word[10479]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vietnamese Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in Vietnamese training corpus is 16142\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in Vietnamese training corpus is \" + str(vien_vi_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6750"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_vi_train.word2index[\"Hamburger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Enlightened'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_vi_train.index2word[6752]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Vocabulary for Zh-En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in English training corpus for Zh-En is 59327\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in English training corpus for Zh-En is \" + str(zhen_en_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1449"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_en_train.word2index[\"translate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'directly'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_en_train.index2word[1451]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Vocabulary for Vi-En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in English training corpus for Vi-En is 47566\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in English training corpus for Vi-En is \" + str(vien_en_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "846"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_train.word2index[\"machine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'force'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_train.index2word[847]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Prepare Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_dev.word2index[\"<EOS>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<EOS>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_dev.index2word[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "SOS_IDX = 1\n",
    "UNK_IDX = 2\n",
    "EOS_IDX = 3\n",
    "# EOS_IDX = 3\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(paired_tokens, \n",
    "                        lang1_token2id_vocab,\n",
    "                        lang2_token2id_vocab):\n",
    "    \"\"\"Takes as input:\n",
    "    - paired_tokens: a list of sentence pairs that consist of source & target lang sentences.\n",
    "    - lang1_token2id_vocab: token2index vocabulary for the first language. \n",
    "                            Get by method Lang_dataset.word2index\n",
    "    - lang2_token2id_vocab: token2index vocabulary for the second language. \n",
    "                            Get by method Lang_dataset.word2index\n",
    "                            \n",
    "    Returns:\n",
    "    - indices_data_lang_1, indices_data_lang2: A list of lists where each sub-list holds corresponding indices for each\n",
    "                                               token in the sentence.\"\"\"\n",
    "    indices_data_lang_1, indices_data_lang_2 = [], []\n",
    "    vocabs = [lang1_token2id_vocab, lang2_token2id_vocab]\n",
    "    \n",
    "    # lang1\n",
    "    for t in range(len(paired_tokens)):\n",
    "        index_list = [vocabs[0][token] if token in vocabs[0]\\\n",
    "                                    else UNK_IDX for token in paired_tokens[t][0].split(\" \")] \n",
    "        indices_data_lang_1.append(index_list)\n",
    "    # lang2\n",
    "    for t in range(len(paired_tokens)):\n",
    "        index_list =  [vocabs[1][token] if token in vocabs[1] \\\n",
    "                                    else UNK_IDX for token in paired_tokens[t][1].split(\" \")] \n",
    "        indices_data_lang_2.append(index_list)\n",
    "        \n",
    "    return indices_data_lang_1, indices_data_lang_2\n",
    "\n",
    "# train indices\n",
    "zhen_zh_train_indices, zhen_en_train_indices = token2index_dataset(zh_en_train_pairs,\n",
    "                                                                   zhen_zh_train.word2index,\n",
    "                                                                   zhen_en_train.word2index)\n",
    "\n",
    "vien_vi_train_indices, vien_en_train_indices = token2index_dataset(vi_en_train_pairs,\n",
    "                                                                   vien_vi_train.word2index,\n",
    "                                                                   vien_en_train.word2index)\n",
    "\n",
    "# dev indices\n",
    "zhen_zh_dev_indices, zhen_en_dev_indices = token2index_dataset(zh_en_dev_pairs,\n",
    "                                                               zhen_zh_dev.word2index,\n",
    "                                                               zhen_en_dev.word2index)\n",
    "\n",
    "vien_vi_dev_indices, vien_en_dev_indices = token2index_dataset(vi_en_dev_pairs,\n",
    "                                                               vien_vi_dev.word2index,\n",
    "                                                               vien_en_dev.word2index)\n",
    "\n",
    "# test indices\n",
    "zhen_zh_test_indices, zhen_en_test_indices = token2index_dataset(zh_en_test_pairs,\n",
    "                                                                 zhen_zh_test.word2index,\n",
    "                                                                 zhen_en_test.word2index)\n",
    "\n",
    "vien_vi_test_indices, vien_en_test_indices = token2index_dataset(vi_en_test_pairs,\n",
    "                                                                 vien_vi_test.word2index,\n",
    "                                                                 vien_en_test.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And', 'what', 'you', 'apos', 're', 'seeing']"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"And what you apos re seeing\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese training set length = 213376\n",
      "Chinese-English (En) training set length = 213376\n",
      "\n",
      "Vietnamese training set length = 133317\n",
      "Vietnamese-English (En) training set length = 133317\n",
      "\n",
      "Chinese dev set length = 1261\n",
      "Chinese-English (En) dev set length = 1261\n",
      "\n",
      "Vietnamese dev set length = 1268\n",
      "Vietnamese-English (En) dev set length = 1268\n",
      "\n",
      "Chinese test set length = 1397\n",
      "Chinese-English (En) test set length = 1397\n",
      "\n",
      "Vietnamese test set length = 1553\n",
      "Vietnamese-English (En) test set length = 1553\n"
     ]
    }
   ],
   "source": [
    "# check length\n",
    "# train\n",
    "print (\"Chinese training set length = \"+str(len(zhen_zh_train_indices)))\n",
    "print (\"Chinese-English (En) training set length = \"+str(len(zhen_en_train_indices)))\n",
    "print (\"\\nVietnamese training set length = \"+str(len(vien_vi_train_indices)))\n",
    "print (\"Vietnamese-English (En) training set length = \"+str(len(vien_en_train_indices)))\n",
    "# dev\n",
    "print (\"\\nChinese dev set length = \"+str(len(zhen_zh_dev_indices)))\n",
    "print (\"Chinese-English (En) dev set length = \"+str(len(zhen_en_dev_indices)))\n",
    "print (\"\\nVietnamese dev set length = \"+str(len(vien_vi_dev_indices)))\n",
    "print (\"Vietnamese-English (En) dev set length = \"+str(len(vien_en_dev_indices)))\n",
    "# test\n",
    "print (\"\\nChinese test set length = \"+str(len(zhen_zh_test_indices)))\n",
    "print (\"Chinese-English (En) test set length = \"+str(len(zhen_en_test_indices)))\n",
    "print (\"\\nVietnamese test set length = \"+str(len(vien_vi_test_indices)))\n",
    "print (\"Vietnamese-English (En) test set length = \"+str(len(vien_en_test_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO \n",
    "\n",
    "MAX_SENTENCE_LENGTH = 15\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# zhen token2index vocabs\n",
    "zhen_zh_train_token2id = zhen_zh_train.word2index\n",
    "zhen_en_train_token2id = zhen_en_train.word2index\n",
    "\n",
    "# vien token2index vocabs\n",
    "vien_vi_train_token2id = vien_vi_train.word2index\n",
    "vien_en_train_token2id = vien_en_train.word2index\n",
    "\n",
    "class TranslationDataset():\n",
    "    \"\"\"\n",
    "    Class that represents a train/dev/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 data_source, # training indices data of the source language\n",
    "                 data_target, # training indices data of the target language\n",
    "                 token2id_source=None, # token2id dict of the source language\n",
    "                 token2id_target=None  # token2id dict of the target language\n",
    "                ):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.source_sentences, self.target_sentences =  data_source, data_target\n",
    "        \n",
    "        self.token2id_source = token2id_source\n",
    "        self.token2id_target = token2id_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def __getitem__(self, batch_index):\n",
    "\n",
    "#         source_word_idx, target_word_idx = [], []\n",
    "        source_mask, target_mask = [], []\n",
    "        \n",
    "        for index in self.source_sentences[batch_index][:MAX_SENTENCE_LENGTH]:\n",
    "            if index != UNK_IDX:\n",
    "                source_mask.append(0)\n",
    "            else:\n",
    "                source_mask.append(1)\n",
    "                \n",
    "        for index in self.target_sentences[batch_index][:MAX_SENTENCE_LENGTH]:\n",
    "            if index != UNK_IDX:\n",
    "                target_mask.append(0)\n",
    "            else:\n",
    "                target_mask.append(1)\n",
    "        \n",
    "        source_indices = self.source_sentences[batch_index][:MAX_SENTENCE_LENGTH]\n",
    "        target_indices = self.target_sentences[batch_index][:MAX_SENTENCE_LENGTH]\n",
    "        \n",
    "        source_list = [source_indices, source_mask, len(source_indices)]\n",
    "        target_list = [target_indices, target_mask, len(target_indices)]\n",
    "        \n",
    "        return source_list + target_list\n",
    "\n",
    "    \n",
    "def translation_collate(batch, max_sentence_length):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    source_data, target_data = [], []\n",
    "    source_mask, target_mask = [], []\n",
    "    source_lengths, target_lengths = [], []\n",
    "\n",
    "    for datum in batch:\n",
    "        source_lengths.append(datum[2])\n",
    "        target_lengths.append(datum[5])\n",
    "        \n",
    "        # PAD\n",
    "        source_data_padded = np.pad(np.array(datum[0]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        source_data.append(source_data_padded)\n",
    "        \n",
    "        source_mask_padded = np.pad(np.array(datum[1]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        source_mask.append(source_mask_padded)\n",
    "        \n",
    "        target_data_padded = np.pad(np.array(datum[3]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        target_data.append(target_data_padded)\n",
    "        \n",
    "        target_mask_padded = np.pad(np.array(datum[4]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])),\n",
    "                               mode=\"constant\", constant_values=0)\n",
    "        target_mask.append(target_mask_padded)\n",
    "        \n",
    "    ind_dec_order = np.argsort(source_lengths)[::-1]\n",
    "    source_data = np.array(source_data)[ind_dec_order]\n",
    "    target_data = np.array(target_data)[ind_dec_order]\n",
    "    source_mask = np.array(source_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    target_mask = np.array(target_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    source_lengths = np.array(source_lengths)[ind_dec_order]\n",
    "    target_lengths = np.array(target_lengths)[ind_dec_order]\n",
    "    \n",
    "    source_list = [torch.from_numpy(source_data), \n",
    "               torch.from_numpy(source_mask).float(), source_lengths]\n",
    "    target_list = [torch.from_numpy(target_data), \n",
    "               torch.from_numpy(target_mask).float(), target_lengths]\n",
    "        \n",
    "    return source_list + target_list\n",
    "\n",
    "\n",
    "zhen_train_dataset = TranslationDataset(zhen_zh_train_indices,\n",
    "                                       zhen_en_train_indices,\n",
    "                                       token2id_source=zhen_zh_train_token2id,\n",
    "                                       token2id_target=zhen_en_train_token2id)\n",
    "\n",
    "zhen_train_loader = torch.utils.data.DataLoader(dataset=zhen_train_dataset,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                               shuffle=False)\n",
    "\n",
    "zhen_dev_dataset = TranslationDataset(zhen_zh_dev_indices,\n",
    "                                       zhen_en_dev_indices,\n",
    "                                       token2id_source=zhen_zh_train_token2id,\n",
    "                                       token2id_target=zhen_en_train_token2id)\n",
    "\n",
    "zhen_dev_loader = torch.utils.data.DataLoader(dataset=zhen_dev_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                             shuffle=False)\n",
    "\n",
    "vien_train_dataset = TranslationDataset(vien_vi_train_indices,\n",
    "                                       vien_en_train_indices,\n",
    "                                       token2id_source=vien_vi_train_token2id,\n",
    "                                       token2id_target=vien_en_train_token2id)\n",
    "\n",
    "vien_train_loader = torch.utils.data.DataLoader(dataset=vien_train_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                             shuffle=False)\n",
    "\n",
    "vien_dev_dataset = TranslationDataset(vien_vi_dev_indices,\n",
    "                                       vien_en_dev_indices,\n",
    "                                       token2id_source=vien_vi_train_token2id,\n",
    "                                       token2id_target=vien_en_train_token2id)\n",
    "\n",
    "vien_dev_loader = torch.utils.data.DataLoader(dataset=vien_dev_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                             shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "#### SAMPLE DATASET - CLEAR OUT LATER ####\n",
    "##########################################\n",
    "\n",
    "zhen_train_dataset = TranslationDataset(zhen_zh_train_indices[:128], \n",
    "                                       zhen_en_train_indices[:128],\n",
    "                                       token2id_source=zhen_zh_train_token2id,\n",
    "                                       token2id_target=zhen_en_train_token2id)\n",
    "\n",
    "zhen_train_loader = torch.utils.data.DataLoader(dataset=zhen_train_dataset,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And',\n",
       " 'it',\n",
       " 'apos',\n",
       " 's',\n",
       " 'got',\n",
       " 'these',\n",
       " 'jet',\n",
       " 'thrusters',\n",
       " 'up',\n",
       " 'in',\n",
       " 'front',\n",
       " 'that',\n",
       " 'it',\n",
       " 'apos',\n",
       " 'll']"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[zhen_en_train.index2word[x] for x in [ 49,  71,  22,  23,  61, 220, 235, 236, 211,   3, 237,  64,  71,  22,\n",
    "          238]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Evaluation Metric\n",
    "\n",
    "We use BLEU as the evaluation metric. Specifically, we focus on the corpus-level BLEU function. \n",
    "\n",
    "The code for BLEU is taken from https://github.com/mjpost/sacreBLEU/blob/master/sacrebleu.py#L1022-L1080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /Users/derin/anaconda/lib/python3.6/site-packages (1.2.12)\n",
      "Requirement already satisfied: typing in /Users/derin/anaconda/lib/python3.6/site-packages (from sacrebleu) (3.6.6)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Beam Search Algorithm\n",
    "\n",
    "In this section, we implement the Beam Search algorithm in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize k-many score lists\n",
    "# start only with the whole x\n",
    "# initialize k-many prev y's lists\n",
    "# choose top-k for y1 from the whole vocab\n",
    "# choose top-k for the second time step by expanding the first time step\n",
    "# compute scores by adding log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_size_k = 10\n",
    "\n",
    "# class BeamSearch:\n",
    "    \n",
    "#     \"\"\"RECURSE\"\"\"\n",
    "    \n",
    "#     def __init__(self,\n",
    "#                  beam_size=beam_size_k, ## insert num \n",
    "#                  softmax_out\n",
    "#                 ):\n",
    "#         \"\"\"\n",
    "#         Class that holds beam information, and search & score functions\n",
    "#         - beam_size = beam size\n",
    "#         - softmax_out = the softmax over the vocabulary at time step t, as computed by the RNN decoder,\n",
    "#                         given the source sequence X and the previously decoded y_<t tokens.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         self.beam_size = beam_size\n",
    "#         self.softmax_out = softmax_out\n",
    "        \n",
    "#         # initialize paths\n",
    "#         self.paths = np.empty((self.beam_size))\n",
    "        \n",
    "#         # initialize the dictionary that will hold the path scores \n",
    "#         # and update the scores at each time step\n",
    "#         self.path_score_dict = {}\n",
    "#         # we will later use each i < k as a key and populate this\n",
    "#         # dict with scores\n",
    "\n",
    "        \n",
    "#     def search():\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "#     def score(prev_ys = None):\n",
    "#         \"\"\"- prev_ys = previously decoded tokens (previously generated target language tokens)\n",
    "#         \"\"\"\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model\n",
    "\n",
    "1. Recurrent neural network based encoder-decoder without attention\n",
    "2. Recurrent neural network based encoder-decoder with attention\n",
    "2. Replace the recurrent encoder with either convolutional or self-attention based encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction loss = binary cross entropy between two (vocab_size x 1) vectors\n",
    "# used during training, since we can compare the real Y and and the generated Y\n",
    "# still at each time step of the decoder, we compare up to and including\n",
    "# the real t-th token and the generated t-th, then optimize\n",
    "\n",
    "def loss_function(y_hat, y):\n",
    "    \n",
    "    \"\"\"Takes as input;\n",
    "    - y: correct \"log-softmax\"(binary vector) that represents the correct t-th token in the target sentence,\n",
    "                 (vocab_size x 1) vector\n",
    "    - y_hat: predicted LogSoftmax for the predicted t-th token in the target sentence.\n",
    "             (vocab_size x 1) vector\n",
    "    Returns;\n",
    "    - NLL Loss in training time\"\"\"\n",
    "#     y_hat = torch.log(y_hat) # log softmax\n",
    "    loss = nn.functional.binary_cross_entropy(y_hat,y)\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "# generation/inference time - validation loss = BLEU\n",
    "\n",
    "def compute_BLEU(corpus_hat,corpus):\n",
    "    ## TODO\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_PATH_LENGTH = 400 # make changeable later !!!\n",
    "\n",
    "# class TargetOut:\n",
    "#     def __init__(self,\n",
    "#                  beam_size=5,\n",
    "#                  source_sentence_length=400,\n",
    "#                  time_step=0):\n",
    "#         \"\"\"\n",
    "#         - beam: the tensor that will be populated with beam_size-many paths in each timestep\n",
    "#         - beam_size: the width of the beam, top k tokens to include in the beam search,\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # initialized again for each timestep\n",
    "#         self.beam = torch.empty(beam_size)\n",
    "#         self.beam_size = beam_size\n",
    "#         self.beam_seq = beam_seq\n",
    "#         self.time_step = time_step\n",
    "        \n",
    "#         self.max_target_length = source_sentence_length*(1.5)\n",
    "#         # path is kept by hold_path\n",
    "#         self.path = torch.empty(beam_size, max_target_length)\n",
    "    \n",
    "#     def _add_and_score_paths(self, \n",
    "#              top_k_tokens):\n",
    "        \n",
    "#         \"\"\"top_k_tokens: torch.FloatTensor of indices according to logSoftmax \n",
    "#         (not embeddings - embedding matrix indices or vocab indices)\"\"\"\n",
    "        \n",
    "#         time_step = self.time_step\n",
    "#         self.path[:,time_step] = top_k_tokens\n",
    "        \n",
    "#         return self\n",
    "            \n",
    "#     def _score_paths(self,gru_out):\n",
    "        \n",
    "#         \"\"\"For each path, computes log(P(Y_i|Y_i-1,..,Y_i-n,X)) + log(P(Y_i-1|Y_i-2,..,Y_i-n,X)) + ...\n",
    "#         -gru_out is a softmax over the vocabulary for each timestep, so \n",
    "#         we need to take its log to obtain the scores\"\"\"\n",
    "#         if self.time_step = 0:\n",
    "            \n",
    "        \n",
    "    \n",
    "#     def _hold_path_score(self):\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([7., 5., 4.]), tensor([3, 4, 1]))"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([3,4,2,7,5,3,2]).topk(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "class BeamSearch(nn.Module):\n",
    "    \n",
    "    \"\"\"network that conducts beam search over the outputs of\n",
    "     any translator network. The translator networks that can \n",
    "     be passed are:\n",
    "     \n",
    "     - Translate (for RNN-enc-dec),\n",
    "     - AttnTranslate (for RNN-enc-dec with attention),\n",
    "     - CNNtranslate (for CNN-encoder based translation).\n",
    "     \n",
    "     The translation networks take care of the encoder-decoder\n",
    "     choices specific to each task. Please see in below sections.\"\"\"\n",
    "\n",
    "    def __init__(self, translator_network, beam_size):\n",
    "        super().__init__()\n",
    "        # translator network that returns the logsoftmax\n",
    "        # over vocabulary size:(vocab_size, 1)\n",
    "        self.translator_network = translator_network\n",
    "        self.beam_size = beam_size\n",
    "        \n",
    "    def init_search_tree(self, batch_size):\n",
    "        beam_size = self.beam_size\n",
    "        self.search_tree = torch.empty(batch_size, beam_size, 1)\n",
    "        return self\n",
    "    \n",
    "    def init_score_tree(self, batch_size):\n",
    "        beam_size = self.beam_size\n",
    "        search_tree = self.search_tree\n",
    "        self.score_tree = torch.zeros(search_tree.size())\n",
    "        return self\n",
    "    \n",
    "    def forward(source_sentence, source_mask, source_lengths,\n",
    "                target_sentence, target_mask, target_lengths):\n",
    "        \n",
    "        self.init_search_tree(BATCH_SIZE)\n",
    "        self.init_score_tree(BATCH_SIZE)\n",
    "        \n",
    "        # at each time step the decoder will give us the logsoftmax\n",
    "        # of one token (batch_size, vocab_size). \n",
    "        output = model(source_sentence, target_sentence,source_mask, \n",
    "                       target_mask, source_lengths,target_lengths)\n",
    "        \n",
    "        # for each sentence in the batch we get the top k predictions\n",
    "        # for each token and append it to the search and score trees. \n",
    "        for i in range(BATCH_SIZE):\n",
    "            beam = output[i].topk(beam_size) # (token scores, token indices)\n",
    "            # cat instead\n",
    "            self.search_tree[i] = self.search_tree.cat(beam[1]) # cat the indices to the search tree\n",
    "            self.score_tree[i,:] = beam[0] # append the scores to the score tree \n",
    "        \n",
    "        # we will sum the logs \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: RNN-based Encoder-Decoder without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "# same as 1st model's RNN encoder\n",
    "# the different part is the attention decoder in model 2\n",
    "\n",
    "class RNNencoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=len(zhen_zh_train_token2id), # for chinese\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=16,\n",
    "                 max_sentence_len=15):\n",
    "        \n",
    "        super(RNNencoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_gru_layers\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embedding_size\n",
    "        self.dropout = percent_dropout\n",
    "        self.embed_source = nn.Embedding(self.vocab_size,\n",
    "                                         self.embed_size,\n",
    "                                         padding_idx=0\n",
    "                                        )\n",
    "        \n",
    "        self.max_sentence_len = max_sentence_len\n",
    "        \n",
    "        self.GRU = nn.GRU(self.embed_size, \n",
    "                          self.hidden_size, \n",
    "                          self.num_layers, \n",
    "                          batch_first=True, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "        self.drop_out_function = nn.Dropout(self.dropout)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        hidden_ = torch.zeros(self.num_layers*self.num_directions, \n",
    "                             batch_size, self.hidden_size).to(device)\n",
    "        return hidden_\n",
    "\n",
    "    def forward(self, source_sentence, source_mask, source_lengths):\n",
    "        \"\"\"Returns source lengths to feed into the decoder, since we do not want\n",
    "        the translation length to be above/below a certain treshold*source sentence length.\"\"\"\n",
    "        \n",
    "        sort_original_source = sorted(range(len(source_lengths)), \n",
    "                             key=lambda sentence: -source_lengths[sentence])\n",
    "        unsort_to_original_source = sorted(range(len(source_lengths)), \n",
    "                             key=lambda sentence: sort_original_source[sentence])\n",
    "        \n",
    "        source_sentence = source_sentence[sort_original_source]\n",
    "        _source_mask = source_mask[sort_original_source]\n",
    "        source_lengths = source_lengths[sort_original_source]\n",
    "        batch_size, seq_len_source = source_sentence.size()\n",
    "        \n",
    "        # init hidden\n",
    "        if self.GRU.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        \n",
    "        self.hidden_source = self.init_hidden(batch_size)\n",
    "        # (self.num_layers*self.num_directions, batch_size, self.hidden_size)\n",
    "        # (1, 32, 256)\n",
    "        # https://pytorch.org/docs/stable/nn.html\n",
    "#         print (\"self hidden size. = \"+str(self.hidden_source.size()))\n",
    "        \n",
    "        # If batch_first == True, then the input and output tensors are provided as \n",
    "        # (batch_size, seq_len, feature)\n",
    "        # https://pytorch.org/docs/stable/nn.html\n",
    "#         print (\"seq len source = \"+str(seq_len_source))\n",
    "        embeds_source = self.embed_source(source_sentence).view(batch_size, seq_len_source,\n",
    "                                                               self.embed_size)\n",
    "        \n",
    "#         print (\"embeds source size = \"+str(embeds_source.size()))\n",
    "        \n",
    "        embeds_source = source_mask*embeds_source + (1-_source_mask)*embeds_source.clone().detach()\n",
    "        \n",
    "#         print (\"embeds source after mask size = \"+str(embeds_source.size()))\n",
    "        \n",
    "        embeds_source = torch.nn.utils.rnn.pack_padded_sequence(embeds_source, \n",
    "                                                                source_lengths, \n",
    "                                                                batch_first=True)\n",
    "        \n",
    "        gru_out_source, self.hidden_source = self.GRU(embeds_source, self.hidden_source)\n",
    "        \n",
    "#         print (\"hidden source size = \"+str(self.hidden_source.size()))\n",
    "        \n",
    "        \n",
    "        # ref: pytorch documentation\n",
    "        # hidden source : h_n of shape \n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "#         print (\"hidden source size = \"+str(self.hidden_source.size()))\n",
    "        \n",
    "        # ref: pytorch documentation\n",
    "        # Like output, the layers can be separated using \n",
    "        # h_n.view(num_layers, num_directions, batch_size, hidden_size)\n",
    "        hidden_source = self.hidden_source.view(self.num_layers, self.num_directions, \n",
    "                                                batch_size, self.hidden_size)\n",
    "        # the following should print (1, 1, 32, 256) for this config\n",
    "#         print (\"hidden source size after view = \"+str(hidden_source.size()))\n",
    "        \n",
    "        # get the mean along 0th axis (over layers)\n",
    "        hidden_source = torch.mean(hidden_source, dim=0) ## mean instead of sum for source representation as suggested in the class\n",
    "        # the following should print (1, 32, 256)\n",
    "#         print (\"hidden source size after mean = \"+str(hidden_source.size()))\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            hidden_source = torch.cat([hidden_source[:,i,:] for i in range(self.num_directions)], dim=1)\n",
    "            gru_out_source = gru_out_source\n",
    "        else:\n",
    "            hidden_source = hidden_source\n",
    "            gru_out_source = gru_out_source\n",
    "            \n",
    "        # view before unsort\n",
    "        hidden_source = hidden_source.view(batch_size, self.hidden_size)\n",
    "        \n",
    "        # the following should print (32, 256)\n",
    "        # print(\"hidden source size before unsort = \"+str(hidden_source.size()))\n",
    "        # UNSORT HIDDEN\n",
    "        hidden_source = hidden_source[unsort_to_original_source] ## back to original indices\n",
    "        \n",
    "        gru_out_source, _ = torch.nn.utils.rnn.pad_packed_sequence(gru_out_source,\n",
    "                                                                  batch_first=True)\n",
    "        \n",
    "#         ### UNSORT GRU OUT\n",
    "#         # get the mean for the GRU output (batch_size, output size, hidden_size)\n",
    "#         gru_out_source = torch.mean(gru_out_source, dim=1).view(batch_size, 1, self.hidden_size)\n",
    "#         gru_out_source = gru_out_source[unsort_to_original_source]\n",
    "# #         print (\"gru_out_source size = \"+str(gru_out_source.size()))\n",
    "        \n",
    "        source_lengths = source_lengths[unsort_to_original_source]\n",
    "        \n",
    "        # here we return both hidden and out since we will pass both to\n",
    "        # the attention decoder\n",
    "        return hidden_source, source_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNdecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=len(zhen_en_train_token2id), # for chinese-english's english\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=1,\n",
    "                 max_sentence_len=15):\n",
    "        \n",
    "        super(RNNdecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embedding_size\n",
    "        self.dropout = percent_dropout\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_gru_layers\n",
    "        \n",
    "        self.GRU = nn.GRU(self.embed_size, \n",
    "                          self.hidden_size, \n",
    "                          self.num_layers, \n",
    "                          batch_first=True, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "        self.GRUcell = nn.GRUCell(self.embed_size, \n",
    "                          self.hidden_size)\n",
    "        \n",
    "        self.ReLU = nn.ReLU\n",
    "        \n",
    "        self.drop_out_function = nn.Dropout(self.dropout)\n",
    "        \n",
    "        self.embed_target = nn.Embedding(self.vocab_size,\n",
    "                                         self.embed_size, padding_idx=0)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # *2 because we are concating hidden with embedding plus context\n",
    "        self.linear_layer = nn.Linear(self.hidden_size*2, self.vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=0)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, \n",
    "                             batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "    def forward(self,\n",
    "                decoder_hidden, ## decoder_hidden = encoder_hidden at first time_step\n",
    "                input_, # input\n",
    "                target_lengths,\n",
    "                target_mask,\n",
    "                time_step):\n",
    "        \n",
    "        # input (batch_size, seq_len_target = 1)\n",
    "        # hidden (self.num_layers*self.num_directions, batch_size, self.hidden_size)\n",
    "        \n",
    "        self.input = input_\n",
    "#         print (\"self.input size = \"+str(self.input.size()))\n",
    "        \n",
    "        sort_original_target = sorted(range(len(target_lengths)), \n",
    "                             key=lambda sentence: -target_lengths[sentence])\n",
    "        unsort_to_original_target = sorted(range(len(target_lengths)), \n",
    "                             key=lambda sentence: sort_original_target[sentence])\n",
    "        \n",
    "        self.input = self.input[sort_original_target]\n",
    "        _target_mask = target_mask[sort_original_target]\n",
    "        target_lengths = target_lengths[sort_original_target]\n",
    "        \n",
    "        # seq_len_target is always 1 in the decoder since we are \n",
    "        # passing the tokens for only 1 time_step at a time\n",
    "        batch_size, seq_len_target = self.input.size()\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        \n",
    "        # hidden => initial hidden will be the same as the context\n",
    "        # vector, which is the hidden_source tensor\n",
    "        # then as we update the hidden state at each time step, this will be \n",
    "        # updated as well\n",
    "        self.hidden = decoder_hidden.view(self.num_layers*self.num_directions,\n",
    "                                          batch_size, self.hidden_size)\n",
    "        \n",
    "        # the following should print (1, 32, 256) for this config\n",
    "#         print (\"self.hidden size = \"+str(self.hidden.size()))\n",
    "        \n",
    "        self.input = self.input.unsqueeze(1)\n",
    "        \n",
    "        embeds_target = self.drop_out_function(self.embed_target(self.input.long())).view(batch_size,\n",
    "                                                                                   seq_len_target,\n",
    "                                                                                   -1)\n",
    "    \n",
    "#         embeds_target = target_mask*embeds_target + (1-_target_mask)*embeds_target.clone().detach()\n",
    "        embeds_target = target_mask[:,time_step,:].unsqueeze(1)*embeds_target + \\\n",
    "                        (1-_target_mask[:,time_step,:].unsqueeze(1))*embeds_target.clone().detach()\n",
    "\n",
    "#         print (\"embeds_target size = \"+str(embeds_target.size()))    \n",
    "        \n",
    "#         embeds_target = torch.nn.utils.rnn.pack_padded_sequence(embeds_target,\n",
    "#                                                         target_lengths,\n",
    "#                                                         batch_first=True)\n",
    "        \n",
    "#         print (\"type embeds target = \"+str(type(embeds_target)))\n",
    "\n",
    "        gru_out_target, self.hidden = self.GRU(embeds_target.data.view(batch_size, 1, self.embed_size),\n",
    "                                               self.hidden)\n",
    "        \n",
    "        # ref: pytorch documentation\n",
    "        # hidden source : h_n of shape \n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "        # the following should print (1, 32, 256) for this config\n",
    "#         print (\"hidden size after GRU = \"+str(self.hidden.size()))\n",
    "        \n",
    "        # undo packing \n",
    "#         gru_out_target, _ = torch.nn.utils.rnn.pad_packed_sequence(gru_out_target,\n",
    "#                                                                    batch_first=True)\n",
    "        \n",
    "#         print (\"out size after GRU = \"+str(gru_out_target.size()))\n",
    "\n",
    "\n",
    "        hidden = self.hidden.view(self.num_layers, self.num_directions,\n",
    "                                  batch_size, self.hidden_size)\n",
    "        hidden = torch.sum(hidden, dim=0) # we don't divide here, just sum\n",
    "        hidden = torch.tanh(hidden)\n",
    "#         print (\"hidden size = \"+str(hidden.size()))\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            # separate layers\n",
    "            gru_out_target = gru_out_target.contiguous().view(seq_len_target,\n",
    "                                                              batch_size,\n",
    "                                                              self.num_directions,\n",
    "                                                              self.hidden_size)\n",
    "        else:\n",
    "            gru_out_target = gru_out_target\n",
    "        \n",
    "#         print (\"gru out size = \"+str(gru_out_target.size()))\n",
    "        \n",
    "        # sum along sequence\n",
    "        gru_out_target = torch.sum(gru_out_target, dim=1) # we don't divide here, just sum\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            hidden = torch.cat([hidden[:,i,:] for i in range(self.num_directions)], \n",
    "                               dim=0)\n",
    "            gru_out_target = torch.cat([gru_out_target[:,i,:] for i in range(self.num_directions)], \n",
    "                                       dim=1)\n",
    "        else:\n",
    "            hidden = hidden.view(batch_size, \n",
    "                                 self.num_directions, self.hidden_size)\n",
    "            gru_out_target = gru_out_target.view(batch_size,\n",
    "                                                 self.num_directions, self.hidden_size)\n",
    "        \n",
    "        hidden = hidden[unsort_to_original_target] ## back to original indices\n",
    "        gru_out_target = gru_out_target[unsort_to_original_target] ## back to original indices\n",
    "\n",
    "#         gru_out_target = self.sigmoid(gru_out_target)\n",
    "        gru_out_target = torch.tanh(gru_out_target)\n",
    "        # concating embedding + context = gru_out_target with hidden\n",
    "        out = torch.cat([gru_out_target,hidden], dim=2)\n",
    "        \n",
    "#         print (\"out size after concat = \"+str(out.size()))\n",
    "        \n",
    "        out = self.linear_layer(out)\n",
    "        \n",
    "        # softmax over vocabulary\n",
    "        pred = self.log_softmax(out)\n",
    "\n",
    "        return pred, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(32,1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_softmax(tensor_of_indices,\n",
    "                       batch_size,\n",
    "                       vocab_size = len(zhen_en_train_token2id)):\n",
    "    \"\"\"\n",
    "    - takes as input a time_step vector of the batch (t-th token of each sentence in the batch)\n",
    "      size: (batch_size, 1)\n",
    "    - converts it to softmax of (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    index_tensor_ = tensor_of_indices.view(-1,1).long()\n",
    "        \n",
    "    one_hot = torch.FloatTensor(batch_size, vocab_size).zero_()\n",
    "    one_hot.scatter_(1, index_tensor_.detach().cpu(), 1)\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 59325])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_softmax(torch.FloatTensor([2,3,4]), 3).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_softmax(torch.ones(32), 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.],\n",
       "         [6.]]), tensor([[3],\n",
       "         [3]]))"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([[2,3,4,4],[4,5,6,6]]).topk(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.3333, 0.2500])"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/torch.FloatTensor([2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chinese -> english\n",
    "enc = RNNencoder(vocab_size=len(zhen_zh_train_token2id), # for chinese\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=16).to(device)\n",
    "\n",
    "dec = RNNdecoder(vocab_size=len(zhen_en_train_token2id), # for chinese-english's english\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=1).to(device)\n",
    "\n",
    "# model = Translate(enc, dec).to(device)\n",
    "\n",
    "loss_hist = []\n",
    "# train\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "def train(encoder, decoder, loader=zhen_train_loader,\n",
    "          optimizer = torch.optim.Adam([*enc.parameters()] + [*dec.parameters()], lr=1e-4),\n",
    "          epoch=None, teacher_forcing=False, criterion = torch.nn.NLLLoss()):\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for batch_idx, (source_sentence, source_mask, source_lengths, \n",
    "                    target_sentence, target_mask, target_lengths)\\\n",
    "                    in enumerate(loader):\n",
    "        \n",
    "        source_sentence, source_mask = source_sentence.to(device), source_mask.to(device) \n",
    "        target_sentence, target_mask = target_sentence.to(device), target_mask.to(device)\n",
    "        \n",
    "        encoder_hidden, source_lengths = encoder(source_sentence,\n",
    "                                               source_mask,\n",
    "                                               source_lengths)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden.to(device)\n",
    "        \n",
    "        # decoder should start with SOS tokens \n",
    "        # ref: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "        input_ = SOS_token*torch.ones(BATCH_SIZE,1).view(-1,1).to(device)\n",
    "        \n",
    "        \n",
    "        if teacher_forcing:\n",
    "            \n",
    "            decoder_outputs = torch.zeros(BATCH_SIZE, torch.max(torch.from_numpy(target_lengths)), decoder.vocab_size)\n",
    "            \n",
    "            for t in range(0, target_sentence.size(1)):\n",
    "\n",
    "                decoder_out, decoder_hidden = decoder(decoder_hidden, # = gru_out_source - instead of encoded_source[0]\n",
    "                                                     input_, # instead of target sentence up to t \n",
    "                                                     target_lengths,  # target lengths\n",
    "                                                     target_mask,\n",
    "                                                     t)\n",
    "                \n",
    "                decoder_outputs[:,t,:] = decoder_out.view(BATCH_SIZE, decoder.vocab_size)\n",
    "            \n",
    "                input_ = target_sentence[:,t].view(-1,1)\n",
    "               \n",
    "            loss_tensor = torch.zeros(BATCH_SIZE, 1)\n",
    "            \n",
    "            for i in range(BATCH_SIZE):\n",
    "                loss_tensor[i] = torch.sum(criterion(decoder_outputs[i], target_sentence[i]))/torch.from_numpy(target_lengths).float()[i]\n",
    "                \n",
    "                \n",
    "            loss = torch.sum(loss_tensor)\n",
    "            loss.backward(retain_graph = True)\n",
    "#             loss += criterion(F.sigmoid(decoder_out), target_tokens)\n",
    "            \n",
    "\n",
    "            print (\"loss = \"+str('{0:.16f}'.format(loss)))\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1)\n",
    "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), 1)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            decoder_outputs = torch.zeros(BATCH_SIZE, torch.max(torch.from_numpy(target_lengths)), decoder.vocab_size)\n",
    "            \n",
    "            for t in range(0, target_sentence.size(1)):\n",
    "\n",
    "                decoder_out, decoder_hidden = decoder(decoder_hidden, # = gru_out_source - instead of encoded_source[0]\n",
    "                                                     input_, # instead of target sentence up to t \n",
    "                                                     target_lengths,  # target lengths\n",
    "                                                     target_mask,\n",
    "                                                     t)\n",
    "\n",
    "                decoder_outputs[:,t,:] = decoder_out.view(BATCH_SIZE, decoder.vocab_size)\n",
    "                input_ = decoder_out.topk(1)[1].view(BATCH_SIZE, 1)\n",
    "#                 print (\"input_ = \"+ str(input_))\n",
    "#                 print (\"input_ size = \"+str(input_.size()))\n",
    "\n",
    "#                 loss += criterion(F.sigmoid(decoder_out), target_tokens)\n",
    "\n",
    "            loss_tensor = torch.zeros(BATCH_SIZE, 1)\n",
    "    \n",
    "            for i in range(BATCH_SIZE):\n",
    "                loss_tensor[i] = torch.sum(criterion(decoder_outputs[i], target_sentence[i]))/torch.from_numpy(target_lengths).float()[i]\n",
    "                \n",
    "            loss = torch.sum(loss_tensor)/BATCH_SIZE\n",
    "            loss.backward(retain_graph = True)\n",
    "#             loss += criterion(F.sigmoid(decoder_out), target_tokens)\n",
    "            \n",
    "\n",
    "            print (\"loss = \"+str('{0:.16f}'.format(loss)))\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1)\n",
    "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), 1)\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "    torch.save(encoder.state_dict(), \"rnn_encoder_state_dict\")\n",
    "    torch.save(decoder.state_dict(), \"rnn_decoder_state_dict\")\n",
    "            \n",
    "    return loss\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "loss = 10.1101493835449219\n",
      "loss = 8.8678016662597656\n",
      "loss = 9.1416530609130859\n",
      "loss = 10.0279464721679688\n",
      "epoch = 1\n",
      "loss = 9.1953191757202148\n",
      "loss = 8.3463144302368164\n",
      "loss = 8.7451848983764648\n",
      "loss = 9.7287149429321289\n",
      "epoch = 2\n",
      "loss = 8.3533220291137695\n",
      "loss = 7.8721103668212891\n",
      "loss = 8.3658161163330078\n",
      "loss = 9.4254112243652344\n",
      "epoch = 3\n",
      "loss = 7.4470486640930176\n",
      "loss = 7.4347085952758789\n",
      "loss = 8.0304088592529297\n",
      "loss = 9.1026630401611328\n",
      "epoch = 4\n",
      "loss = 6.6134114265441895\n",
      "loss = 6.9392967224121094\n",
      "loss = 7.7838072776794434\n",
      "loss = 8.9833803176879883\n",
      "epoch = 5\n",
      "loss = 5.8876662254333496\n",
      "loss = 6.5269632339477539\n",
      "loss = 7.4741396903991699\n",
      "loss = 8.8287343978881836\n",
      "epoch = 6\n",
      "loss = 5.3274607658386230\n",
      "loss = 6.0963082313537598\n",
      "loss = 7.1998119354248047\n",
      "loss = 8.6789226531982422\n",
      "epoch = 7\n",
      "loss = 4.8310236930847168\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-469-86cbf83decc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                  \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzhen_train_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                  \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                  epoch = epoch, teacher_forcing=True)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#     loss_train.append(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-468-b97bde669f06>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, loader, optimizer, epoch, teacher_forcing, criterion)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;31m#             loss += criterion(F.sigmoid(decoder_out), target_tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 150\n",
    "lr = 1e-3\n",
    "# batch_\n",
    "\n",
    "loss_train = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print (\"epoch = \"+str(epoch))\n",
    "\n",
    "    loss = train(enc, dec,\n",
    "                 loader = zhen_train_loader,\n",
    "                 optimizer = torch.optim.Adam([*enc.parameters()] + [*dec.parameters()], lr=lr, weight_decay=1e-6),\n",
    "                 epoch = epoch, teacher_forcing=True)\n",
    "    \n",
    "#     loss_train.append(loss)\n",
    "    \n",
    "#     print (loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 RNN-based Encoder-Decoder with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 RNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "# same as 1st model's RNN encoder except that works on one token at a time\n",
    "# the different part is the attention decoder in model 2\n",
    "\n",
    "class attnRNNencoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=len(zhen_zh_train_token2id), # for chinese\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=4,\n",
    "                 max_sentence_len=50):\n",
    "        \n",
    "        super(attnRNNencoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_gru_layers\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embedding_size\n",
    "        self.dropout = percent_dropout\n",
    "        self.embed_source = nn.Embedding(self.vocab_size,\n",
    "                                         self.embed_size,\n",
    "                                         padding_idx=0\n",
    "                                        )\n",
    "        \n",
    "        self.max_sentence_len = max_sentence_len\n",
    "        \n",
    "        self.GRU = nn.GRU(self.embed_size, \n",
    "                          self.hidden_size, \n",
    "                          self.num_layers, \n",
    "                          batch_first=True, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "        self.drop_out_function = nn.Dropout(self.dropout)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        hidden_ = torch.zeros(self.num_layers*self.num_directions, \n",
    "                             batch_size, self.hidden_size).to(device)\n",
    "        return hidden_\n",
    "\n",
    "    def forward(self, source_sentence, source_mask, source_lengths,\n",
    "                time_step):\n",
    "        \"\"\"Returns source lengths to feed into the decoder, since we do not want\n",
    "        the translation length to be above/below a certain treshold*source sentence length.\"\"\"\n",
    "        \n",
    "        source_sentence = source_sentence.view(-1,1)\n",
    "        # print (\"source size = \"+str(source_sentence.size()))\n",
    "        # (batch_size, 1)\n",
    "        \n",
    "        sort_original_source = sorted(range(len(source_lengths)), \n",
    "                             key=lambda sentence: -source_lengths[sentence])\n",
    "        unsort_to_original_source = sorted(range(len(source_lengths)), \n",
    "                             key=lambda sentence: sort_original_source[sentence])\n",
    "        \n",
    "        source_sentence = source_sentence[sort_original_source]\n",
    "        _source_mask = source_mask[sort_original_source]\n",
    "        source_lengths = source_lengths[sort_original_source]\n",
    "        batch_size, seq_len_source = source_sentence.size()\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "            \n",
    "        self.hidden_source = self.init_hidden(batch_size)\n",
    "        # (self.num_layers*self.num_directions, batch_size, self.hidden_size)\n",
    "        # (1, 32, 256)\n",
    "        # https://pytorch.org/docs/stable/nn.html\n",
    "        # print (\"self hidden size. = \"+str(self.hidden_source.size()))\n",
    "        \n",
    "        # If batch_first == True, then the input and output tensors are provided as \n",
    "        # (batch_size, seq_len, feature)\n",
    "        # https://pytorch.org/docs/stable/nn.html\n",
    "        # print (\"seq len source = \"+str(seq_len_source))\n",
    "        \n",
    "        source_sentence = source_sentence.unsqueeze(1)\n",
    "        \n",
    "        embeds_source = self.embed_source(source_sentence).view(batch_size, seq_len_source,\n",
    "                                                               self.embed_size)\n",
    "        \n",
    "        # print (\"embeds source size = \"+str(embeds_source.size()))\n",
    "        \n",
    "        embeds_source = source_mask[:,time_step,:].unsqueeze(1)*embeds_source + \\\n",
    "                        (1-_source_mask[:,time_step,:].unsqueeze(1))*embeds_source.clone().detach()\n",
    "        \n",
    "        # print (\"embeds source after mask size = \"+str(embeds_source.size()))\n",
    "        \n",
    "        \n",
    "#         embeds_source = torch.nn.utils.rnn.pack_padded_sequence(embeds_source, \n",
    "#                                                                 source_lengths, \n",
    "#                                                                 batch_first=True)\n",
    "        \n",
    "        gru_out_source, self.hidden_source = self.GRU(embeds_source, self.hidden_source)\n",
    "        \n",
    "        # print (\"gru out source size = \"+str(gru_out_source.size()))\n",
    "        \n",
    "        # print (\"hidden source size = \"+str(self.hidden_source.size()))\n",
    "        # print (\"gru out source size = \"+str(gru_out_source.size()))\n",
    "        \n",
    "        # hidden source size = torch.Size([1, 32, 256])\n",
    "        # gru out source size = torch.Size([32, 350, 256])\n",
    "        \n",
    "        # ref: pytorch documentation\n",
    "        # hidden source : h_n of shape \n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "        # print (\"hidden source size = \"+str(self.hidden_source.size()))\n",
    "        \n",
    "        # ref: pytorch documentation\n",
    "        # Like output, the layers can be separated using \n",
    "        # h_n.view(num_layers, num_directions, batch_size, hidden_size)\n",
    "        hidden_source = self.hidden_source.view(self.num_layers, self.num_directions, \n",
    "                                                batch_size, self.hidden_size)\n",
    "        \n",
    "        # print (\"hidden source size = \"+str(hidden_source.size()))\n",
    "        # hidden source size = torch.Size([1, 1, 32, 256])\n",
    "        \n",
    "        # the following should print (1, 1, 32, 256) for this config\n",
    "        # print (\"hidden source size after view = \"+str(hidden_source.size()))\n",
    "        \n",
    "        # get the mean along 0th axis (over layers)\n",
    "        hidden_source = torch.mean(hidden_source, dim=0) ## mean instead of sum for source representation as suggested in the class\n",
    "        # the following should print (1, 32, 256)\n",
    "        # print (\"hidden source size after mean = \"+str(hidden_source.size()))\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            hidden_source = torch.cat([hidden_source[:,i,:] for i in range(self.num_directions)], dim=1)\n",
    "            gru_out_source = gru_out_source\n",
    "        else:\n",
    "            hidden_source = hidden_source\n",
    "            gru_out_source = gru_out_source\n",
    "            \n",
    "        # view before unsort\n",
    "        hidden_source = hidden_source.view(batch_size, self.hidden_size)\n",
    "        \n",
    "        # the following should print (32, 256)\n",
    "        # print(\"hidden source size before unsort = \"+str(hidden_source.size()))\n",
    "        # UNSORT HIDDEN\n",
    "        hidden_source = hidden_source[unsort_to_original_source] ## back to original indices\n",
    "        \n",
    "#         gru_out_source, _ = torch.nn.utils.rnn.pad_packed_sequence(gru_out_source,\n",
    "#                                                                   batch_first=True)\n",
    "        \n",
    "        ### UNSORT GRU OUT\n",
    "        # get the mean for the GRU output (batch_size, output size, hidden_size)\n",
    "        gru_out_source = gru_out_source.view(batch_size, seq_len_source, self.hidden_size)\n",
    "        # gru_out_source = torch.mean(gru_out_source, dim=1).view(batch_size, 1, self.hidden_size)\n",
    "        gru_out_source = gru_out_source[unsort_to_original_source]\n",
    "        # print (\"gru_out_source size = \"+str(gru_out_source.size()))\n",
    "        \n",
    "        source_lengths = source_lengths[unsort_to_original_source]\n",
    "        \n",
    "        # here we return both hidden and out since we will pass both to\n",
    "        # the attention decoder\n",
    "        return hidden_source, gru_out_source, source_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=len(zhen_zh_train_token2id), \n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 max_sentence_len=50, \n",
    "                 num_gru_layers=1):\n",
    "\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = percent_dropout\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "        self.num_layers = num_gru_layers\n",
    "        self.embed_size = embedding_size\n",
    "        \n",
    "        self.embed_target = nn.Embedding(self.vocab_size,\n",
    "                                         self.hidden_size,\n",
    "                                         padding_idx=0\n",
    "                                        )\n",
    "        \n",
    "        self.GRU = nn.GRU(self.hidden_size, \n",
    "                          self.hidden_size,\n",
    "                          self.num_layers, \n",
    "                          batch_first=True, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "        # we concat embeds with hidden before attention, thus the input size\n",
    "        # of the linear attn layer is embed + hidden, and the output is hidden.\n",
    "        self.attn = nn.Linear(self.hidden_size*2, \n",
    "                              self.max_sentence_len)\n",
    "        \n",
    "        # we combine embeds with attention applied (self.attn out) before attn_combine\n",
    "        # so the input size of the linear attn_combine layer is embed_size + hidden_size \n",
    "        # \n",
    "        self.attn_combine = nn.Linear(self.hidden_size*2, \n",
    "                                      self.hidden_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        \n",
    "        self.log_softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self,\n",
    "                hidden, ## decoder_hidden = encoder_hidden at first time_step\n",
    "                input_, # input (batch_size, seq_len = 1)\n",
    "                encoder_outputs, # (encoder hidden and encoder out)\n",
    "                target_lengths,\n",
    "                target_mask,\n",
    "                time_step):\n",
    "        \n",
    "        # input (batch_size, seq_len = 1)\n",
    "        self.input = input_\n",
    "#         print (\"input size. =\"+str(self.input.size()))\n",
    "        \n",
    "        sort_original_target = sorted(range(len(target_lengths)), \n",
    "                             key=lambda sentence: -target_lengths[sentence])\n",
    "        unsort_to_original_target = sorted(range(len(target_lengths)), \n",
    "                             key=lambda sentence: sort_original_target[sentence])\n",
    "        \n",
    "        self.input = self.input[sort_original_target]\n",
    "        _target_mask = target_mask[sort_original_target]\n",
    "        target_lengths = target_lengths[sort_original_target]\n",
    "        \n",
    "        # seq_len_target is always 1 in the decoder since we are \n",
    "        # passing the tokens for only 1 time_step at a time\n",
    "        batch_size, seq_len_target = self.input.size()\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "            \n",
    "        self.hidden = hidden.view(batch_size, \n",
    "                                  self.num_layers*self.num_directions, \n",
    "                                  self.hidden_size)\n",
    "        \n",
    "        self.input = self.input.unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        embeds_target = self.dropout(self.embed_target(self.input.long()))\\\n",
    "                                                                .view(batch_size,\n",
    "                                                                      seq_len_target, -1)\n",
    "        \n",
    "        embeds_target = target_mask[:,time_step,:].unsqueeze(1)*embeds_target + \\\n",
    "                        (1-_target_mask[:,time_step,:].unsqueeze(1))*embeds_target.clone().detach()\n",
    "        \n",
    "        # print (\"embeds target size = \"+str(embeds_target.size()))\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embeds_target, self.hidden), 2)), dim=2)\n",
    "#         print (\"attn_weights size = \"+str(attn_weights.size()))\n",
    "        \n",
    "        # try for loop and bmm and see if these are the same \n",
    "        # print (\"enc out size = \"+str(encoder_outputs.size()))\n",
    "        attn_applied = torch.zeros(batch_size, self.max_sentence_len, self.hidden_size)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "#             print (\"attn_weights[i] = \"+str(attn_weights[i]))\n",
    "#             print (\"encoder_outputs[i] = \"+str(encoder_outputs[i]))\n",
    "            apply = torch.bmm(attn_weights[i].unsqueeze(0),\n",
    "                              encoder_outputs[i].unsqueeze(0))\n",
    "            \n",
    "            attn_applied[i] = apply\n",
    "        \n",
    "#         print (\"attn_applied size = \"+str(attn_applied.size()))\n",
    "#         print (\"embeds target size = \"+ str(embeds_target.size()))\n",
    "#         print (\"encoder outputs = \"+str(encoder_outputs))\n",
    "#         print (\"attn_applied[:,time_step,:] size = \"+str(attn_applied[:,time_step,:].view(batch_size,\n",
    "#                                                                                           1, self.hidden_size).size()))\n",
    "\n",
    "        output = torch.cat((embeds_target,\n",
    "                            attn_applied[:,time_step,:].view(batch_size,1, self.hidden_size)),2)\n",
    "        \n",
    "        output = self.attn_combine(output)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        \n",
    "        self.hidden = self.hidden.view(self.num_layers*self.num_directions,\n",
    "                                       batch_size,\n",
    "                                       self.hidden_size)\n",
    "\n",
    "        output, self.hidden = self.GRU(output, self.hidden)\n",
    "        \n",
    "        self.hidden = self.hidden.view(batch_size,\n",
    "                                       self.num_layers*self.num_directions,\n",
    "                                       self.hidden_size)\n",
    "        \n",
    "        output = output[unsort_to_original_target]\n",
    "        self.hidden = self.hidden[unsort_to_original_target]\n",
    "        \n",
    "#         print (\"output size = \"+str(output.size()))\n",
    "#         print (\"hidden size = \"+str(self.hidden.size()))\n",
    "        \n",
    "        output = self.out(output)\n",
    "#         print (\"out after linear size = \"+str(output.size()))\n",
    "        \n",
    "        output = F.log_softmax(output, dim=2)\n",
    "#         print (\"logsoft size = \"+str(output.size()))\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WORKS\n",
    "\n",
    "# chinese -> english\n",
    "enc = attnRNNencoder(vocab_size=len(zhen_zh_train_token2id), # for chinese\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=10)\n",
    "\n",
    "dec = AttnDecoderRNN(vocab_size=len(zhen_en_train_token2id), # for chinese-english's english\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=1)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "def train(encoder, decoder, loader=None, \n",
    "          criterion=torch.nn.NLLLoss(),\n",
    "          optimizer = torch.optim.Adam([*enc.parameters()] + [*dec.parameters()], lr=lr),\n",
    "          epoch=None, teacher_forcing=True):\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_idx, (source_sentence, source_mask, source_lengths, \n",
    "                    target_sentence, target_mask, target_lengths)\\\n",
    "    in enumerate(loader):\n",
    "        \n",
    "        source_sentence, source_mask = source_sentence.to(device), source_mask.to(device),  \n",
    "        target_sentence, target_mask = target_sentence.to(device), target_mask.to(device),\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # output softmax as generated by decoder \n",
    "        encoder_outputs = torch.zeros(BATCH_SIZE,\n",
    "                                      encoder.max_sentence_len, \n",
    "                                      encoder.hidden_size, \n",
    "                                      device=device)\n",
    "        \n",
    "        max_length = torch.max(torch.from_numpy(source_lengths))\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            #last hidden state of the encoder is the context\n",
    "            encoder_hidden, encoder_output, source_lengths = encoder(source_sentence[:,i],\n",
    "                                                                          source_mask,\n",
    "                                                                          source_lengths,\n",
    "                                                                          i) # i as time_step\n",
    "            # doing what we want, uncomment the prints below to check\n",
    "            # i-th time_step token of each sentence in batch is filled with the corresponding\n",
    "            # encoder output\n",
    "            encoder_outputs[:,i,:] = encoder_output.unsqueeze(1)[:,0,0]\n",
    "            \n",
    "            # print (\"encoder_outputs[:,i,:] size = \"+str(encoder_outputs[:,i,:].size()))\n",
    "            # print (\"encoder outputs size = \"+str(encoder_outputs.size()))\n",
    "            \n",
    "            # print (\"encoder outputs = \"+str(encoder_outputs))\n",
    "\n",
    "        # encoder hidden also used as the initial hidden state of the decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # decoder should start with SOS tokens \n",
    "        # ref: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "        input_ = SOS_token*torch.ones(BATCH_SIZE,1).view(-1,1)\n",
    "        \n",
    "        target_length = target_sentence.size(1)\n",
    "\n",
    "        if teacher_forcing:\n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            # WORKS\n",
    "            decoder_outputs = torch.zeros(BATCH_SIZE, torch.max(torch.from_numpy(target_lengths)), decoder.vocab_size)\n",
    "#             decoder_outputs = torch.zeros(BATCH_SIZE, torch.max(torch.from_numpy(target_lengths)),1)\n",
    "\n",
    "            for di in range(target_length):\n",
    "                # target tensor -> (batch_size, vocab_size) of t-th time step tokens \n",
    "                # from each sentence, converted to softmax (binary)\n",
    "#                 target_tensor = convert_to_softmax(target_sentence[:,di],32)\n",
    "                # print (\"target_tensor = \"+str(target_tensor))\n",
    "                # print (\"target tensor size = \"+str(target_tensor.size()))\n",
    "                \n",
    "                # take ith token from each sentence in the batch, and convert it to \n",
    "                # softmax\n",
    "                decoder_out, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_hidden, input_, encoder_outputs,\n",
    "                    target_lengths, target_mask, di) # di as time_step\n",
    "                \n",
    "                decoder_input = target_sentence[:,di].view(-1,1) # Teacher forcing\n",
    "                \n",
    "                # decoder out should be size (32, 1, vocab_size)\n",
    "                ### WORKS\n",
    "                decoder_outputs[:,di,:] = decoder_out.view(BATCH_SIZE, decoder.vocab_size)\n",
    "#                 decoder_outputs[:,di,:] = decoder_out.topk(1)[1].view(BATCH_SIZE, 1).view(-1,1)\n",
    "#                 print (\"deco out  = \"+str(decoder_outputs))\n",
    "               \n",
    "            loss_tensor = torch.zeros(BATCH_SIZE, 1)\n",
    "            \n",
    "            for i in range(BATCH_SIZE):\n",
    "#                 print (\"decoder i = \"+str(decoder_outputs[i]))\n",
    "                loss_tensor[i] = torch.sum(criterion(decoder_outputs[i], target_sentence[i]))/torch.from_numpy(target_lengths).float()[i]\n",
    "                \n",
    "            loss = torch.sum(loss_tensor)/BATCH_SIZE\n",
    "            loss.backward(retain_graph = True)\n",
    "#             loss += criterion(F.sigmoid(decoder_out), target_tokens)\n",
    "            \n",
    "\n",
    "            print (\"loss = \"+str('{0:.16f}'.format(loss)))\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1)\n",
    "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), 1)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "        else:\n",
    "            # Without teacher forcing: use its own predictions as the next input\n",
    "            # just like we did in the RNN encoder-decoder above\n",
    "\n",
    "            decoder_outputs = torch.zeros(BATCH_SIZE, torch.max(torch.from_numpy(target_lengths)), decoder.vocab_size)\n",
    "            \n",
    "            for di in range(target_length):\n",
    "                # target tensor -> (batch_size, vocab_size) of t-th time step tokens \n",
    "                # from each sentence, converted to softmax (binary)\n",
    "                target_tensor = convert_to_softmax(target_sentence[:,di],32)\n",
    "                # print (\"target_tensor = \"+str(target_tensor))\n",
    "                \n",
    "                decoder_out, decoder_hidden, decoder_attention = self.decoder(\n",
    "                    decoder_hidden, input_, encoder_outputs, target_lengths,\n",
    "                    target_mask, di)\n",
    "                \n",
    "                token_out = torch.max(decoder_out.view(BATCH_SIZE, decoder.vocab_size),1)[1]\n",
    "                input_ = token_out.view(-1,1)\n",
    "                \n",
    "                decoder_outputs[:,di,:] = decoder_out.view(BATCH_SIZE, decoder.vocab_size)\n",
    "               \n",
    "            loss_tensor = torch.zeros(BATCH_SIZE, 1)\n",
    "            \n",
    "            for i in range(BATCH_SIZE):\n",
    "#                 print (\"decoder outputs i = \"+str(decoder_outputs[i]))\n",
    "                loss_tensor[i] = torch.sum(criterion(decoder_outputs[i], target_sentence[i]))/torch.from_numpy(target_lengths).float()[i]\n",
    "                \n",
    "            loss = torch.sum(loss_tensor)/BATCH_SIZE\n",
    "            loss.backward(retain_graph = True)\n",
    "#             loss += criterion(F.sigmoid(decoder_out), target_tokens)\n",
    "            \n",
    "\n",
    "            print (\"loss = \"+str('{0:.16f}'.format(loss)))\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1)\n",
    "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), 1)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "#         print (\"decoder_outputs = \"+str(token_out))\n",
    "#         print (\"target_sent = \"+str(target_sentence))\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "loss = 0.9920510053634644\n",
      "loss = 0.8767580986022949\n",
      "loss = 0.8959162831306458\n",
      "loss = 0.9695891737937927\n",
      "epoch = 1\n",
      "loss = 0.9199833273887634\n",
      "loss = 0.8084396123886108\n",
      "loss = 0.7975307703018188\n",
      "loss = 0.8116895556449890\n",
      "epoch = 2\n",
      "loss = 0.7314515113830566\n",
      "loss = 0.6561849117279053\n",
      "loss = 0.6397017836570740\n",
      "loss = 0.6345977187156677\n",
      "epoch = 3\n",
      "loss = 0.5561968088150024\n",
      "loss = 0.5375205874443054\n",
      "loss = 0.5445695519447327\n",
      "loss = 0.5561699867248535\n",
      "epoch = 4\n",
      "loss = 0.4634081423282623\n",
      "loss = 0.4646818637847900\n",
      "loss = 0.4919323325157166\n",
      "loss = 0.5181626677513123\n",
      "epoch = 5\n",
      "loss = 0.4117937088012695\n",
      "loss = 0.4271088838577271\n",
      "loss = 0.4700047075748444\n",
      "loss = 0.5073997974395752\n",
      "epoch = 6\n",
      "loss = 0.3888230025768280\n",
      "loss = 0.4121096730232239\n",
      "loss = 0.4663365781307220\n",
      "loss = 0.5084107518196106\n",
      "epoch = 7\n",
      "loss = 0.3811817467212677\n",
      "loss = 0.4066292941570282\n",
      "loss = 0.4663747251033783\n",
      "loss = 0.5119415521621704\n",
      "epoch = 8\n",
      "loss = 0.3767030537128448\n",
      "loss = 0.4026760458946228\n",
      "loss = 0.4664025306701660\n",
      "loss = 0.5134052038192749\n",
      "epoch = 9\n",
      "loss = 0.3743362724781036\n",
      "loss = 0.3980008661746979\n",
      "loss = 0.4647325575351715\n",
      "loss = 0.5116896629333496\n",
      "epoch = 10\n",
      "loss = 0.3711381852626801\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-477-86cbf83decc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                  \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzhen_train_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                  \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                  epoch = epoch, teacher_forcing=True)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#     loss_train.append(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-476-6c9eb5bb9e74>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, loader, criterion, optimizer, epoch, teacher_forcing)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;31m#             loss += criterion(F.sigmoid(decoder_out), target_tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 150\n",
    "lr = 1e-3\n",
    "# batch_\n",
    "\n",
    "loss_train = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print (\"epoch = \"+str(epoch))\n",
    "\n",
    "    loss = train(enc, dec,\n",
    "                 loader = zhen_train_loader,\n",
    "                 optimizer = torch.optim.Adam([*enc.parameters()] + [*dec.parameters()], lr=lr, weight_decay=1e-6),\n",
    "                 epoch = epoch, teacher_forcing=True)\n",
    "    \n",
    "#     loss_train.append(loss)\n",
    "    \n",
    "#     print (loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoder Replacement with Convolutional or Self-attention-based Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Conv1d(300, 256, 1, stride=2)\n",
    "input = torch.randn(32, 1, 300)\n",
    "output = m(input.transpose(1,2)).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 256])"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://github.com/yanwii/seq2seq/blob/master/seq2seq.py\n",
    "\n",
    "# ENCODER\n",
    "\n",
    "class CNNencoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_size, # in channels\n",
    "                 hidden_size, \n",
    "                 kernel_size, \n",
    "                 padding = 1,\n",
    "                 stride = 2,\n",
    "                 percent_dropout = 0.3,\n",
    "                 vocab_size = len(zhen_zh_train.index2word),\n",
    "                 max_sentence_len=50):\n",
    "        \n",
    "        super(CNNencoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.vocab_size = vocab_size\n",
    "        self.stride = stride\n",
    "        self.dropout = nn.Dropout(percent_dropout)\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, \n",
    "                                      self.embedding_size)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(self.embedding_size, self.hidden_size, \n",
    "                               kernel_size=self.kernel_size, padding=self.padding,\n",
    "                               stride=self.stride)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(self.hidden_size, self.hidden_size, \n",
    "                               kernel_size=self.kernel_size, padding=self.padding,\n",
    "                               stride = self.stride)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool_1 = nn.MaxPool1d(3, 1)\n",
    "        self.maxpool_2 = nn.MaxPool1d(5, 2)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "\n",
    "    def forward(self, input_):\n",
    "        \n",
    "        # input size = 1'e uydurmaya calis\n",
    "        \n",
    "        batch_size, seq_len = input_.size()\n",
    "        \n",
    "        embed = self.dropout(self.embedding(input_))\n",
    "        # print (\"embed size = \"+str(embed.size()))\n",
    "        # 32, 350, 300 check\n",
    "        \n",
    "        hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden = self.relu(hidden)\n",
    "        hidden = self.maxpool_1(hidden.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "#         # second conv layer\n",
    "#         hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "#         hidden = self.relu(hidden)\n",
    "#         hidden = self.maxpool_2(hidden.transpose(1,2)).transpose(1,2)\n",
    "\n",
    "        # print (\"hidden size = \"+str(hidden.size()))\n",
    "        hidden = nn.functional.glu(hidden)\n",
    "        \n",
    "        # sum \n",
    "        hidden = torch.mean(hidden, 1).view(batch_size, 1, hidden.size(-1))\n",
    "        # sigmoid\n",
    "        hidden = self.sigmoid(hidden)\n",
    "        \n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNdecoder_CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=len(zhen_en_train_token2id), # for chinese-english's english\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=512,\n",
    "                 num_gru_layers=1,\n",
    "                 max_sentence_len=50):\n",
    "        \n",
    "        super(RNNdecoder_CNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embedding_size\n",
    "        self.dropout = percent_dropout\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_gru_layers\n",
    "        \n",
    "        self.GRU = nn.GRU(self.embed_size, \n",
    "                          self.hidden_size, \n",
    "                          self.num_layers, \n",
    "                          batch_first=True, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "        self.ReLU = nn.ReLU\n",
    "        \n",
    "        self.drop_out_function = nn.Dropout(self.dropout)\n",
    "        \n",
    "        self.embed_target = nn.Embedding(self.vocab_size,\n",
    "                                         self.embed_size, padding_idx=0)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # *2 because we are concating hidden with embedding plus context\n",
    "        self.linear_layer = nn.Linear(self.hidden_size*2, self.vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=0)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, \n",
    "                             batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "    def forward(self,\n",
    "                decoder_hidden, ## decoder_hidden = encoder_hidden at first time_step\n",
    "                input_, # input\n",
    "                target_lengths,\n",
    "                target_mask,\n",
    "                time_step):\n",
    "        \n",
    "        self.input = input_\n",
    "#         print (\"input size = \"+str(self.input.size()))\n",
    "        \n",
    "        sort_original_target = sorted(range(len(target_lengths)), \n",
    "                             key=lambda sentence: -target_lengths[sentence])\n",
    "        unsort_to_original_target = sorted(range(len(target_lengths)), \n",
    "                             key=lambda sentence: sort_original_target[sentence])\n",
    "        \n",
    "        self.input = self.input[sort_original_target]\n",
    "        _target_mask = target_mask[sort_original_target]\n",
    "        target_lengths = target_lengths[sort_original_target]\n",
    "        \n",
    "        # seq_len_target is always 1 in the decoder since we are \n",
    "        # passing the tokens for only 1 time_step at a time\n",
    "        batch_size, seq_len_target = self.input.size()\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        \n",
    "        # hidden => initial hidden will be the same as the context\n",
    "        # vector, which is the hidden_source tensor\n",
    "        # then as we update the hidden state at each time step, this will be \n",
    "        # updated as well\n",
    "        self.hidden = decoder_hidden.view(self.num_layers*self.num_directions,\n",
    "                                          batch_size, self.hidden_size)\n",
    "        \n",
    "        # the following should print (1, 32, 256) for this config\n",
    "        # print (\"self.hidden size = \"+str(self.hidden.size()))\n",
    "        \n",
    "        self.input = self.input.unsqueeze(1)\n",
    "        \n",
    "        embeds_target = self.drop_out_function(self.embed_target(self.input.long())).view(batch_size,\n",
    "                                                                                   seq_len_target,\n",
    "                                                                                   -1)\n",
    "    \n",
    "        embeds_target = target_mask[:,time_step,:].unsqueeze(1)*embeds_target + \\\n",
    "                        (1-_target_mask[:,time_step,:].unsqueeze(1))*embeds_target.clone().detach()\n",
    "\n",
    "\n",
    "        gru_out_target, self.hidden = self.GRU(embeds_target.data.view(batch_size, 1, self.embed_size),\n",
    "                                               self.hidden)\n",
    "        \n",
    "        # ref: pytorch documentation\n",
    "        # hidden source : h_n of shape \n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "        # the following should print (1, 32, 256) for this config\n",
    "        # print (\"hidden size after GRU = \"+str(self.hidden.size()))\n",
    "\n",
    "\n",
    "        hidden = self.hidden.view(self.num_layers, self.num_directions,\n",
    "                                  batch_size, self.hidden_size)\n",
    "        hidden = torch.sum(hidden, dim=0) # we don't divide here, just sum\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            # separate layers\n",
    "            gru_out_target = gru_out_target.contiguous().view(seq_len_target,\n",
    "                                                              batch_size,\n",
    "                                                              self.num_directions,\n",
    "                                                              self.hidden_size)\n",
    "        else:\n",
    "            gru_out_target = gru_out_target\n",
    "        \n",
    "#         print (\"gru out size = \"+str(gru_out_target.size()))\n",
    "        \n",
    "        # sum along sequence\n",
    "        gru_out_target = torch.sum(gru_out_target, dim=1) # we don't divide here, just sum\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            hidden = torch.cat([hidden[:,i,:] for i in range(self.num_directions)], \n",
    "                               dim=0)\n",
    "            gru_out_target = torch.cat([gru_out_target[:,i,:] for i in range(self.num_directions)], \n",
    "                                       dim=1)\n",
    "        else:\n",
    "            hidden = hidden.view(batch_size, \n",
    "                                 self.num_directions, self.hidden_size)\n",
    "            gru_out_target = gru_out_target.view(batch_size,\n",
    "                                                 self.num_directions, self.hidden_size)\n",
    "        \n",
    "        hidden = hidden[unsort_to_original_target] ## back to original indices\n",
    "        gru_out_target = gru_out_target[unsort_to_original_target] ## back to original indices\n",
    "\n",
    "        gru_out_target = self.sigmoid(gru_out_target)\n",
    "        # concating embedding + context = gru_out_target with hidden\n",
    "        out = torch.cat([gru_out_target,hidden], dim=2)\n",
    "        \n",
    "#         print (\"out size after concat = \"+str(out.size()))\n",
    "        \n",
    "        out = self.linear_layer(out)\n",
    "        \n",
    "        # softmax over vocabulary\n",
    "        pred = self.log_softmax(out)\n",
    "\n",
    "        return pred, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chinese -> english\n",
    "enc = CNNencoder(300, # embed size\n",
    "                 1024, # hidden size\n",
    "                 3, # kernel size\n",
    "                 padding = 1,\n",
    "                 stride = 2,\n",
    "                 percent_dropout = 0.3,\n",
    "                 vocab_size = len(zhen_zh_train.index2word),\n",
    "                 max_sentence_len=50)\n",
    "    \n",
    "dec = RNNdecoder_CNN()\n",
    "\n",
    "\n",
    "# model = Translate(enc, dec).to(device)\n",
    "\n",
    "loss_hist = []\n",
    "# train\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "def train(encoder, decoder, loader=zhen_train_loader,\n",
    "          optimizer = torch.optim.Adam([*enc.parameters()] + [*dec.parameters()], lr=1e-4),\n",
    "          epoch=None, teacher_forcing=False, criterion = torch.nn.NLLLoss()):\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for batch_idx, (source_sentence, source_mask, source_lengths, \n",
    "                    target_sentence, target_mask, target_lengths)\\\n",
    "                    in enumerate(loader):\n",
    "        \n",
    "        source_sentence, source_mask = source_sentence.to(device), source_mask.to(device) \n",
    "        target_sentence, target_mask = target_sentence.to(device), target_mask.to(device)\n",
    "        \n",
    "        encoder_hidden = encoder(source_sentence)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden.to(device)\n",
    "        \n",
    "        # decoder should start with SOS tokens \n",
    "        # ref: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "        input_ = SOS_token*torch.ones(BATCH_SIZE,1).view(-1,1).to(device)\n",
    "        \n",
    "        \n",
    "        if teacher_forcing:\n",
    "            \n",
    "            decoder_outputs = torch.zeros(BATCH_SIZE, torch.max(torch.from_numpy(target_lengths)), decoder.vocab_size)\n",
    "            \n",
    "            for t in range(0, target_sentence.size(1)):\n",
    "\n",
    "                decoder_out, decoder_hidden = decoder(decoder_hidden, # = gru_out_source - instead of encoded_source[0]\n",
    "                                                     input_, # instead of target sentence up to t \n",
    "                                                     target_lengths,  # target lengths\n",
    "                                                     target_mask,\n",
    "                                                     t)\n",
    "                \n",
    "                decoder_outputs[:,t,:] = decoder_out.view(BATCH_SIZE, decoder.vocab_size)\n",
    "            \n",
    "                input_ = target_sentence[:,t].view(-1,1)\n",
    "               \n",
    "            loss_tensor = torch.zeros(BATCH_SIZE, 1)\n",
    "            \n",
    "            for i in range(BATCH_SIZE):\n",
    "                loss_tensor[i] = torch.sum(criterion(decoder_outputs[i], target_sentence[i]))/torch.from_numpy(target_lengths).float()[i]\n",
    "                \n",
    "                \n",
    "            loss = torch.sum(loss_tensor)\n",
    "            loss.backward(retain_graph = True)\n",
    "            \n",
    "\n",
    "            print (\"loss = \"+str('{0:.16f}'.format(loss)))\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1)\n",
    "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), 1)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            decoder_outputs = torch.zeros(BATCH_SIZE, torch.max(torch.from_numpy(target_lengths)), decoder.vocab_size)\n",
    "            \n",
    "            for t in range(0, target_sentence.size(1)):\n",
    "\n",
    "                decoder_out, decoder_hidden = decoder(decoder_hidden, # = gru_out_source - instead of encoded_source[0]\n",
    "                                                     input_, # instead of target sentence up to t \n",
    "                                                     target_lengths,  # target lengths\n",
    "                                                     target_mask,\n",
    "                                                     t)\n",
    "\n",
    "                decoder_outputs[:,t,:] = decoder_out.view(BATCH_SIZE, decoder.vocab_size)\n",
    "                input_ = decoder_out.topk(1)[1].view(BATCH_SIZE, 1)\n",
    "\n",
    "            loss_tensor = torch.zeros(BATCH_SIZE, 1)\n",
    "    \n",
    "            for i in range(BATCH_SIZE):\n",
    "                loss_tensor[i] = torch.sum(criterion(decoder_outputs[i], target_sentence[i]))/torch.from_numpy(target_lengths).float()[i]\n",
    "                \n",
    "            loss = torch.sum(loss_tensor)/BATCH_SIZE\n",
    "            loss.backward(retain_graph = True)\n",
    "\n",
    "            print (\"loss = \"+str('{0:.16f}'.format(loss)))\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1)\n",
    "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), 1)\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "    torch.save(encoder.state_dict(), \"rnn_encoder_state_dict\")\n",
    "    torch.save(decoder.state_dict(), \"rnn_decoder_state_dict\")\n",
    "            \n",
    "    return loss\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "loss = 7.3747038841247559\n",
      "epoch = 1\n",
      "loss = 7.0964665412902832\n",
      "epoch = 2\n",
      "loss = 6.8683176040649414\n",
      "epoch = 3\n",
      "loss = 6.6570010185241699\n",
      "epoch = 4\n",
      "loss = 6.4639935493469238\n",
      "epoch = 5\n",
      "loss = 6.2949438095092773\n",
      "epoch = 6\n",
      "loss = 6.1415042877197266\n",
      "epoch = 7\n",
      "loss = 5.9666891098022461\n",
      "epoch = 8\n",
      "loss = 5.8206062316894531\n",
      "epoch = 9\n",
      "loss = 5.6812753677368164\n",
      "epoch = 10\n",
      "loss = 5.5318951606750488\n",
      "epoch = 11\n",
      "loss = 5.4130401611328125\n",
      "epoch = 12\n",
      "loss = 5.3188328742980957\n",
      "epoch = 13\n",
      "loss = 5.2347936630249023\n",
      "epoch = 14\n",
      "loss = 5.1057667732238770\n",
      "epoch = 15\n",
      "loss = 5.0297274589538574\n",
      "epoch = 16\n",
      "loss = 4.9333653450012207\n",
      "epoch = 17\n",
      "loss = 4.8222193717956543\n",
      "epoch = 18\n",
      "loss = 4.7188811302185059\n",
      "epoch = 19\n",
      "loss = 4.6476082801818848\n",
      "epoch = 20\n",
      "loss = 4.5690822601318359\n",
      "epoch = 21\n",
      "loss = 4.5040621757507324\n",
      "epoch = 22\n",
      "loss = 4.4066834449768066\n",
      "epoch = 23\n",
      "loss = 4.3466229438781738\n",
      "epoch = 24\n",
      "loss = 4.2600164413452148\n",
      "epoch = 25\n",
      "loss = 4.2192277908325195\n",
      "epoch = 26\n",
      "loss = 4.1255135536193848\n",
      "epoch = 27\n",
      "loss = 4.0800347328186035\n",
      "epoch = 28\n",
      "loss = 4.0090465545654297\n",
      "epoch = 29\n",
      "loss = 3.9643511772155762\n",
      "epoch = 30\n",
      "loss = 3.9320213794708252\n",
      "epoch = 31\n",
      "loss = 3.8924760818481445\n",
      "epoch = 32\n",
      "loss = 3.8348002433776855\n",
      "epoch = 33\n",
      "loss = 3.8136048316955566\n",
      "epoch = 34\n",
      "loss = 3.7727921009063721\n",
      "epoch = 35\n",
      "loss = 3.7577002048492432\n",
      "epoch = 36\n",
      "loss = 3.7247171401977539\n",
      "epoch = 37\n",
      "loss = 3.6980025768280029\n",
      "epoch = 38\n",
      "loss = 3.6794562339782715\n",
      "epoch = 39\n",
      "loss = 3.6645166873931885\n",
      "epoch = 40\n",
      "loss = 3.6281630992889404\n",
      "epoch = 41\n",
      "loss = 3.6094028949737549\n",
      "epoch = 42\n",
      "loss = 3.5824549198150635\n",
      "epoch = 43\n",
      "loss = 3.5711874961853027\n",
      "epoch = 44\n",
      "loss = 3.5471365451812744\n",
      "epoch = 45\n",
      "loss = 3.5529525279998779\n",
      "epoch = 46\n",
      "loss = 3.5230274200439453\n",
      "epoch = 47\n",
      "loss = 3.5258862972259521\n",
      "epoch = 48\n",
      "loss = 3.5064601898193359\n",
      "epoch = 49\n",
      "loss = 3.4962854385375977\n",
      "epoch = 50\n",
      "loss = 3.4772651195526123\n",
      "epoch = 51\n",
      "loss = 3.4822120666503906\n",
      "epoch = 52\n",
      "loss = 3.4666843414306641\n",
      "epoch = 53\n",
      "loss = 3.4803709983825684\n",
      "epoch = 54\n",
      "loss = 3.4634363651275635\n",
      "epoch = 55\n",
      "loss = 3.4322988986968994\n",
      "epoch = 56\n",
      "loss = 3.4277956485748291\n",
      "epoch = 57\n",
      "loss = 3.4274613857269287\n",
      "epoch = 58\n",
      "loss = 3.3980157375335693\n",
      "epoch = 59\n",
      "loss = 3.4019038677215576\n",
      "epoch = 60\n",
      "loss = 3.3939919471740723\n",
      "epoch = 61\n",
      "loss = 3.3755233287811279\n",
      "epoch = 62\n",
      "loss = 3.3769032955169678\n",
      "epoch = 63\n",
      "loss = 3.3752443790435791\n",
      "epoch = 64\n",
      "loss = 3.3568251132965088\n",
      "epoch = 65\n",
      "loss = 3.3628475666046143\n",
      "epoch = 66\n",
      "loss = 3.3663501739501953\n",
      "epoch = 67\n",
      "loss = 3.3557219505310059\n",
      "epoch = 68\n",
      "loss = 3.3387236595153809\n",
      "epoch = 69\n",
      "loss = 3.3453512191772461\n",
      "epoch = 70\n",
      "loss = 3.3542253971099854\n",
      "epoch = 71\n",
      "loss = 3.3360662460327148\n",
      "epoch = 72\n",
      "loss = 3.3356802463531494\n",
      "epoch = 73\n",
      "loss = 3.3164725303649902\n",
      "epoch = 74\n",
      "loss = 3.3311276435852051\n",
      "epoch = 75\n",
      "loss = 3.3207430839538574\n",
      "epoch = 76\n",
      "loss = 3.3085205554962158\n",
      "epoch = 77\n",
      "loss = 3.3165147304534912\n",
      "epoch = 78\n",
      "loss = 3.3155598640441895\n",
      "epoch = 79\n",
      "loss = 3.3130974769592285\n",
      "epoch = 80\n",
      "loss = 3.3111073970794678\n",
      "epoch = 81\n",
      "loss = 3.3042523860931396\n",
      "epoch = 82\n",
      "loss = 3.3071360588073730\n",
      "epoch = 83\n",
      "loss = 3.3004078865051270\n",
      "epoch = 84\n",
      "loss = 3.3098127841949463\n",
      "epoch = 85\n",
      "loss = 3.2973632812500000\n",
      "epoch = 86\n",
      "loss = 3.2986922264099121\n",
      "epoch = 87\n",
      "loss = 3.3241171836853027\n",
      "epoch = 88\n",
      "loss = 3.2815165519714355\n",
      "epoch = 89\n",
      "loss = 3.2852091789245605\n",
      "epoch = 90\n",
      "loss = 3.2810256481170654\n",
      "epoch = 91\n",
      "loss = 3.2786774635314941\n",
      "epoch = 92\n",
      "loss = 3.2976121902465820\n",
      "epoch = 93\n",
      "loss = 3.2882206439971924\n",
      "epoch = 94\n",
      "loss = 3.2862563133239746\n",
      "epoch = 95\n",
      "loss = 3.2744023799896240\n",
      "epoch = 96\n",
      "loss = 3.2932918071746826\n",
      "epoch = 97\n",
      "loss = 3.2750339508056641\n",
      "epoch = 98\n",
      "loss = 3.2852606773376465\n",
      "epoch = 99\n",
      "loss = 3.2584886550903320\n",
      "epoch = 100\n",
      "loss = 3.2758224010467529\n",
      "epoch = 101\n",
      "loss = 3.2497806549072266\n",
      "epoch = 102\n",
      "loss = 3.2668929100036621\n",
      "epoch = 103\n",
      "loss = 3.2523362636566162\n",
      "epoch = 104\n",
      "loss = 3.2584168910980225\n",
      "epoch = 105\n",
      "loss = 3.2509679794311523\n",
      "epoch = 106\n",
      "loss = 3.2527351379394531\n",
      "epoch = 107\n",
      "loss = 3.2578265666961670\n",
      "epoch = 108\n",
      "loss = 3.2638125419616699\n",
      "epoch = 109\n",
      "loss = 3.2480688095092773\n",
      "epoch = 110\n",
      "loss = 3.2522337436676025\n",
      "epoch = 111\n",
      "loss = 3.2589042186737061\n",
      "epoch = 112\n",
      "loss = 3.2427704334259033\n",
      "epoch = 113\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-342-86cbf83decc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                  \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzhen_train_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                  \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                  epoch = epoch, teacher_forcing=True)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#     loss_train.append(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-341-d4d79802aec6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, loader, optimizer, epoch, teacher_forcing, criterion)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;31m#             loss += criterion(F.sigmoid(decoder_out), target_tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 150\n",
    "lr = 1e-3\n",
    "# batch_\n",
    "\n",
    "loss_train = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print (\"epoch = \"+str(epoch))\n",
    "\n",
    "    loss = train(enc, dec,\n",
    "                 loader = zhen_train_loader,\n",
    "                 optimizer = torch.optim.Adam([*enc.parameters()] + [*dec.parameters()], \n",
    "                                              lr=lr, weight_decay=1e-6),\n",
    "                 epoch = epoch, teacher_forcing=True)\n",
    "    \n",
    "#     loss_train.append(loss)\n",
    "    \n",
    "#     print (loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Fully self-attention Translation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Multilingual Translation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
