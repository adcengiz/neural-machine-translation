{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Spring 2018 NLP Class Project: Neural Machine Translation</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pdb\n",
    "import os\n",
    "from underthesea import word_tokenize\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# running on cpu\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install spacy && python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Project Overview\n",
    "\n",
    "The goal of this project is to build a neural machine translation system and experience how recent advances have made their way. Each team will build the following sequence of neural translation systems for two language pairs, __Vietnamese (Vi)→English (En)__ and __Chinese (Zh)→En__ (prepared corpora is be provided):\n",
    "\n",
    "1. Recurrent neural network based encoder-decoder without attention\n",
    "2. Recurrent neural network based encoder-decoder with attention\n",
    "2. Replace the recurrent encoder with either convolutional or self-attention based encoder.\n",
    "4. [Optional] Build either or both fully self-attention translation system or/and multilingual translation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Upload & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of sentence\n",
    "SOS_token = 1\n",
    "# end of sentence\n",
    "EOS_token = 3\n",
    "\n",
    "## 2 = unk\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"<PAD>\",1: \"<SOS>\", 2:\"<UNK>\", 3: \"<EOS>\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    \"\"\"About \"NFC\" and \"NFD\": \n",
    "    \n",
    "    For each character, there are two normal forms: normal form C \n",
    "    and normal form D. Normal form D (NFD) is also known as canonical \n",
    "    decomposition, and translates each character into its decomposed form. \n",
    "    Normal form C (NFC) first applies a canonical decomposition, then composes \n",
    "    pre-combined characters again.\n",
    "    \n",
    "    About unicodedata.category: \n",
    "    \n",
    "    Returns the general category assigned to the Unicode character \n",
    "    unichr as string.\"\"\"\n",
    "    \n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Trim\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False,\n",
    "             dataset=\"train\"):\n",
    "    \n",
    "    \"\"\"Takes as input;\n",
    "    - lang1, lang2: either (vi, en) or (zh, en)\n",
    "    - dataset: one of (\"train\",\"dev\",\"test\")\"\"\"\n",
    "    print(\"Reading lines...\")\n",
    "    eos = [\".\",\"?\",\"!\",\"\\n\"]\n",
    "    # Read the pretokenized lang1 file and split into lines\n",
    "    lang1_lines = open(\"../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-%s-%s-processed/%s.tok.%s\" % (lang1, lang2, dataset, lang1), encoding=\"utf-8\").\\\n",
    "        read().strip().split(\"\\n\")\n",
    "    # Read the lang2 file and split into lines\n",
    "    lang2_lines = open(\"../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-%s-%s-processed/%s.tok.%s\" % (lang1, lang2, dataset, lang2), encoding=\"utf-8\").\\\n",
    "        read().strip().split(\"\\n\")\n",
    "    \n",
    "    # create sentence pairs (lists of length 2 that consist of string pairs)\n",
    "    # e.g. [\"And we &apos;re going to tell you some stories from the sea here in video .\",\n",
    "    #       \"我们 将 用 一些 影片 来讲 讲述 一些 深海 海里 的 故事  \"]\n",
    "    # check if there are the same number of sentences in each set\n",
    "    assert len(lang1_lines) == len(lang2_lines), \"Two languages must have the same number of sentences. \"+ str(len(lang1_lines)) + \" sentences were passed for \" + str(lang1) + \".\" + str(len(lang2_lines)) + \" sentences were passed for \" + str(lang2)+\".\"\n",
    "    # normalize if not Chinese, Chinese normalization is already handeled\n",
    "    if lang1 == \"zh\":\n",
    "        lang1_lines = [s + \"<EOS>\" for s in lang1_lines]\n",
    "    else:\n",
    "        lang1_lines = [normalizeString(s).replace(\".\",\"<EOS>\").\\\n",
    "                       replace(\"?\",\"<EOS>\").replace(\"!\",\"<EOS>\").replace(\"\\n\",\"<EOS>\") for s in lang1_lines]\n",
    "    lang2_lines = [normalizeString(s).replace(\".\",\"<EOS>\").\\\n",
    "                       replace(\"?\",\"<EOS>\").replace(\"!\",\"<EOS>\").replace(\"\\n\",\"<EOS>\") for s in lang2_lines]\n",
    "    # construct pairs\n",
    "    pair_ran = range(len(lang1_lines))\n",
    "    pairs = [[lang1_lines[i]] + [lang2_lines[i]] for i in pair_ran]\n",
    "    \n",
    "#     # Split every line into pairs and normalize\n",
    "#     pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 133317 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 16142\n",
      "en 47566\n",
      "['co nhieu cach neu o la tam thoi toi thieu hoa anh huong nhung no la mot van e <EOS>', 'There apos s ways if it apos s temporary to minimize the impact but it apos s a problem <EOS>']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False, dataset=\"train\"):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse, dataset=dataset)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# example\n",
    "input_lang, output_lang, pairs = prepareData('vi', 'en', False, dataset=\"train\")\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213376 sentence pairs\n",
      "Trimmed to 213376 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 89202\n",
      "en 59327\n",
      "['德鲁   德纳 维奇    会计 之 夜 的 即兴 表演   <EOS>', 'Drew Dernavich <EOS> quot Accounting night at the improv <EOS> quot ']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('zh', 'en', False, dataset=\"train\")\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vietnamese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Please find the original tokenizing code provided by Elman Mansimov in the following link:\n",
    "# # https://github.com/derincen/neural-machine-translation/tree/master/data/tokens_and_preprocessing_em/preprocess_translation\n",
    "\n",
    "# def tokenize_vi(f_names, f_out_names):\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             tok_lines.write(word_tokenize(sentence, format=\"text\") + '\\n')\n",
    "#         tok_lines.close()\n",
    "\n",
    "# def tokenize_en(f_names, f_out_names):\n",
    "#     tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             # replaced tokenizer(sentence) with str(tokenizer(sentence)) to avoid \n",
    "#             # type error while joining\n",
    "#             tok_lines.write(' '.join(str(tokenizer(sentence))) + '\\n')\n",
    "#         tok_lines.close()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     root = '../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-vi-en-processed/'\n",
    "#     tokenize_vi([os.path.join(root, 'train.vi'), os.path.join(root, 'dev.vi'), \n",
    "#                  os.path.join(root, 'test.vi')],\\\n",
    "#                [os.path.join(root, 'train.tok.vi'), os.path.join(root, 'dev.tok.vi'), \n",
    "#                 os.path.join(root, 'test.tok.vi')])\n",
    "\n",
    "#     tokenize_en([os.path.join(root, 'train.en'), os.path.join(root, 'dev.en'), \n",
    "#                  os.path.join(root, 'test.en')],\\\n",
    "#                 [os.path.join(root, 'train.tok.en'), os.path.join(root, 'dev.tok.en'), \n",
    "#                  os.path.join(root, 'test.tok.en')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 133317 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 16142\n",
      "en 47566\n",
      "Reading lines...\n",
      "Read 1268 sentence pairs\n",
      "Trimmed to 1268 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 1368\n",
      "en 3814\n",
      "Reading lines...\n",
      "Read 1553 sentence pairs\n",
      "Trimmed to 1553 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 1323\n",
      "en 3617\n"
     ]
    }
   ],
   "source": [
    "# Format: languagepair_language_dataset\n",
    "# Train \n",
    "vien_vi_train, vien_en_train, vi_en_train_pairs = prepareData('vi', 'en', False, dataset=\"train\")\n",
    "# Dev \n",
    "vien_vi_dev, vien_en_dev, vi_en_dev_pairs = prepareData('vi', 'en', False, dataset=\"dev\")\n",
    "# Test\n",
    "vien_vi_test, vien_en_test, vi_en_test_pairs = prepareData('vi', 'en', False, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Chinese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Please find the original tokenizing code provided by Elman Mansimov in the following link:\n",
    "# # https://github.com/derincen/neural-machine-translation/tree/master/data/tokens_and_preprocessing_em/preprocess_translation\n",
    "\n",
    "# def tokenize_zh(f_names, f_out_names):\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             tok_lines.write(' '.join(jieba.cut(sentence, cut_all=True)))\n",
    "#         tok_lines.close()\n",
    "\n",
    "# def tokenize_en(f_names, f_out_names):\n",
    "#     tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             # replaced tokenizer(sentence) with str(tokenizer(sentence)) to avoid \n",
    "#             # type error while joining\n",
    "#             tok_lines.write(' '.join(str(tokenizer(sentence))) + '\\n')\n",
    "#         tok_lines.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     root = '../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-zh-en-processed/'\n",
    "#     tokenize_zh([os.path.join(root, 'dev.zh'), os.path.join(root, 'test.zh'), os.path.join(root, 'train.zh')],\\\n",
    "#                 [os.path.join(root, 'dev.tok.zh'), os.path.join(root, 'test.tok.zh'), os.path.join(root, 'train.tok.zh')])\n",
    "\n",
    "# #     tokenize_en([os.path.join(root, 'dev.en'), os.path.join(root, 'test.en'), os.path.join(root, 'train.en')],\\\n",
    "# #                [os.path.join(root, 'dev.tok.en'), os.path.join(root, 'test.tok.en'), os.path.join(root, 'train.tok.en')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213376 sentence pairs\n",
      "Trimmed to 213376 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 89202\n",
      "en 59327\n",
      "Reading lines...\n",
      "Read 1261 sentence pairs\n",
      "Trimmed to 1261 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 6134\n",
      "en 3914\n",
      "Reading lines...\n",
      "Read 1397 sentence pairs\n",
      "Trimmed to 1397 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 5216\n",
      "en 3421\n"
     ]
    }
   ],
   "source": [
    "# Format: languagepair_language_dataset\n",
    "# Train \n",
    "zhen_zh_train, zhen_en_train, zh_en_train_pairs = prepareData('zh', 'en', False, dataset=\"train\")\n",
    "# Dev \n",
    "zhen_zh_dev, zhen_en_dev, zh_en_dev_pairs = prepareData('zh', 'en', False, dataset=\"dev\")\n",
    "# Test\n",
    "zhen_zh_test, zhen_en_test, zh_en_test_pairs = prepareData('zh', 'en', False, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我们 将 用 一些 影片 来讲 讲述 一些 深海 海里 的 故事  <EOS>',\n",
       " 'And we apos re going to tell you some stories from the sea here in video <EOS>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_en_train_pairs[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Check Source & Target Vocabs\n",
    "\n",
    "Since the source and target languages can have very different table lookup layers, it's good practice to have separate vocabularies for each. Thus, we build vocabularies for each language that we will be using. \n",
    "\n",
    "In the first class (Lang) of this section, we have already defined vocabularies for all languages. So, there is no need to redefine another function. We chech each vocabulary below.\n",
    "\n",
    "#### Chinese Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in Chinese training corpus is 89202\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in Chinese training corpus is \" + str(zhen_zh_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10481"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_zh_train.word2index[\"格\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'哈利'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_zh_train.index2word[10479]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vietnamese Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in Vietnamese training corpus is 16142\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in Vietnamese training corpus is \" + str(vien_vi_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6750"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_vi_train.word2index[\"Hamburger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Enlightened'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_vi_train.index2word[6752]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Vocabulary for Zh-En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in English training corpus for Zh-En is 59327\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in English training corpus for Zh-En is \" + str(zhen_en_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1449"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_en_train.word2index[\"translate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'directly'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_en_train.index2word[1451]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Vocabulary for Vi-En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in English training corpus for Vi-En is 47566\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in English training corpus for Vi-En is \" + str(vien_en_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "846"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_train.word2index[\"machine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'force'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_train.index2word[847]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Prepare Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_dev.word2index[\"<EOS>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<EOS>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_dev.index2word[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "SOS_IDX = 1\n",
    "UNK_IDX = 2\n",
    "# EOS_IDX = 3\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(paired_tokens, \n",
    "                        lang1_token2id_vocab,\n",
    "                        lang2_token2id_vocab):\n",
    "    \"\"\"Takes as input:\n",
    "    - paired_tokens: a list of sentence pairs that consist of source & target lang sentences.\n",
    "    - lang1_token2id_vocab: token2index vocabulary for the first language. \n",
    "                            Get by method Lang_dataset.word2index\n",
    "    - lang2_token2id_vocab: token2index vocabulary for the second language. \n",
    "                            Get by method Lang_dataset.word2index\n",
    "                            \n",
    "    Returns:\n",
    "    - indices_data_lang_1, indices_data_lang2: A list of lists where each sub-list holds corresponding indices for each\n",
    "                                               token in the sentence.\"\"\"\n",
    "    indices_data_lang_1, indices_data_lang_2 = [], []\n",
    "    vocabs = [lang1_token2id_vocab, lang2_token2id_vocab]\n",
    "    \n",
    "    # lang1\n",
    "    for t in range(len(paired_tokens)):\n",
    "        index_list = [vocabs[0][token] if token in vocabs[0]\\\n",
    "                                    else UNK_IDX for token in paired_tokens[t][0]] \n",
    "        indices_data_lang_1.append(index_list)\n",
    "    # lang2\n",
    "    for t in range(len(paired_tokens)):\n",
    "        index_list =  [vocabs[1][token] if token in vocabs[1] \\\n",
    "                                    else UNK_IDX for token in paired_tokens[t][1]] \n",
    "        indices_data_lang_2.append(index_list)\n",
    "        \n",
    "    return indices_data_lang_1, indices_data_lang_2\n",
    "\n",
    "# train indices\n",
    "zhen_zh_train_indices, zhen_en_train_indices = token2index_dataset(zh_en_train_pairs,\n",
    "                                                                   zhen_zh_train.word2index,\n",
    "                                                                   zhen_en_train.word2index)\n",
    "\n",
    "vien_vi_train_indices, vien_en_train_indices = token2index_dataset(vi_en_train_pairs,\n",
    "                                                                   vien_vi_train.word2index,\n",
    "                                                                   vien_en_train.word2index)\n",
    "\n",
    "# dev indices\n",
    "zhen_zh_dev_indices, zhen_en_dev_indices = token2index_dataset(zh_en_dev_pairs,\n",
    "                                                               zhen_zh_dev.word2index,\n",
    "                                                               zhen_en_dev.word2index)\n",
    "\n",
    "vien_vi_dev_indices, vien_en_dev_indices = token2index_dataset(vi_en_dev_pairs,\n",
    "                                                               vien_vi_dev.word2index,\n",
    "                                                               vien_en_dev.word2index)\n",
    "\n",
    "# test indices\n",
    "zhen_zh_test_indices, zhen_en_test_indices = token2index_dataset(zh_en_test_pairs,\n",
    "                                                                 zhen_zh_test.word2index,\n",
    "                                                                 zhen_en_test.word2index)\n",
    "\n",
    "vien_vi_test_indices, vien_en_test_indices = token2index_dataset(vi_en_test_pairs,\n",
    "                                                                 vien_vi_test.word2index,\n",
    "                                                                 vien_en_test.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese training set length = 213376\n",
      "Chinese-English (En) training set length = 213376\n",
      "\n",
      "Vietnamese training set length = 133317\n",
      "Vietnamese-English (En) training set length = 133317\n",
      "\n",
      "Chinese dev set length = 1261\n",
      "Chinese-English (En) dev set length = 1261\n",
      "\n",
      "Vietnamese dev set length = 1268\n",
      "Vietnamese-English (En) dev set length = 1268\n",
      "\n",
      "Chinese test set length = 1397\n",
      "Chinese-English (En) test set length = 1397\n",
      "\n",
      "Vietnamese test set length = 1553\n",
      "Vietnamese-English (En) test set length = 1553\n"
     ]
    }
   ],
   "source": [
    "# check length\n",
    "# train\n",
    "print (\"Chinese training set length = \"+str(len(zhen_zh_train_indices)))\n",
    "print (\"Chinese-English (En) training set length = \"+str(len(zhen_en_train_indices)))\n",
    "print (\"\\nVietnamese training set length = \"+str(len(vien_vi_train_indices)))\n",
    "print (\"Vietnamese-English (En) training set length = \"+str(len(vien_en_train_indices)))\n",
    "# dev\n",
    "print (\"\\nChinese dev set length = \"+str(len(zhen_zh_dev_indices)))\n",
    "print (\"Chinese-English (En) dev set length = \"+str(len(zhen_en_dev_indices)))\n",
    "print (\"\\nVietnamese dev set length = \"+str(len(vien_vi_dev_indices)))\n",
    "print (\"Vietnamese-English (En) dev set length = \"+str(len(vien_en_dev_indices)))\n",
    "# test\n",
    "print (\"\\nChinese test set length = \"+str(len(zhen_zh_test_indices)))\n",
    "print (\"Chinese-English (En) test set length = \"+str(len(zhen_en_test_indices)))\n",
    "print (\"\\nVietnamese test set length = \"+str(len(vien_vi_test_indices)))\n",
    "print (\"Vietnamese-English (En) test set length = \"+str(len(vien_en_test_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO \n",
    "\n",
    "MAX_SENTENCE_LENGTH = 15\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# zhen token2index vocabs\n",
    "zhen_zh_train_token2id = zhen_zh_train.word2index\n",
    "zhen_en_train_token2id = zhen_en_train.word2index\n",
    "\n",
    "# vien token2index vocabs\n",
    "vien_vi_train_token2id = vien_vi_train.word2index\n",
    "vien_en_train_token2id = vien_en_train.word2index\n",
    "\n",
    "class TranslationDataset():\n",
    "    \"\"\"\n",
    "    Class that represents a train/dev/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 data_source, # training indices data of the source language\n",
    "                 data_target, # training indices data of the target language\n",
    "                 token2id_source=None, # token2id dict of the source language\n",
    "                 token2id_target=None  # token2id dict of the target language\n",
    "                ):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.source_sentences, self.target_sentences =  data_source, data_target\n",
    "        \n",
    "        self.token2id_source = token2id_source\n",
    "        self.token2id_target = token2id_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def __getitem__(self, batch_index):\n",
    "\n",
    "#         source_word_idx, target_word_idx = [], []\n",
    "        source_mask, target_mask = [], []\n",
    "        \n",
    "        for index in self.source_sentences[batch_index][:MAX_SENTENCE_LENGTH]:\n",
    "            if index != UNK_IDX:\n",
    "                source_mask.append(0)\n",
    "            else:\n",
    "                source_mask.append(1)\n",
    "                \n",
    "        for index in self.target_sentences[batch_index][:MAX_SENTENCE_LENGTH]:\n",
    "            if index != UNK_IDX:\n",
    "                target_mask.append(0)\n",
    "            else:\n",
    "                target_mask.append(1)\n",
    "        \n",
    "        source_indices = self.source_sentences[batch_index][:MAX_SENTENCE_LENGTH]\n",
    "        target_indices = self.target_sentences[batch_index][:MAX_SENTENCE_LENGTH]\n",
    "        \n",
    "        source_list = [source_indices, source_mask, len(source_indices)]\n",
    "        target_list = [target_indices, target_mask, len(target_indices)]\n",
    "        \n",
    "        return source_list + target_list\n",
    "\n",
    "    \n",
    "def translation_collate(batch, max_sentence_length):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    source_data, target_data = [], []\n",
    "    source_mask, target_mask = [], []\n",
    "    source_lengths, target_lengths = [], []\n",
    "\n",
    "    for datum in batch:\n",
    "        source_lengths.append(datum[2])\n",
    "        target_lengths.append(datum[5])\n",
    "        \n",
    "        # PAD\n",
    "        source_data_padded = np.pad(np.array(datum[0]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        source_data.append(source_data_padded)\n",
    "        \n",
    "        source_mask_padded = np.pad(np.array(datum[1]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        source_mask.append(source_mask_padded)\n",
    "        \n",
    "        target_data_padded = np.pad(np.array(datum[3]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        target_data.append(target_data_padded)\n",
    "        \n",
    "        target_mask_padded = np.pad(np.array(datum[4]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])),\n",
    "                               mode=\"constant\", constant_values=0)\n",
    "        target_mask.append(target_mask_padded)\n",
    "        \n",
    "    ind_dec_order = np.argsort(source_lengths)[::-1]\n",
    "    source_data = np.array(source_data)[ind_dec_order]\n",
    "    target_data = np.array(target_data)[ind_dec_order]\n",
    "    source_mask = np.array(source_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    target_mask = np.array(target_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    source_lengths = np.array(source_lengths)[ind_dec_order]\n",
    "    target_lengths = np.array(target_lengths)[ind_dec_order]\n",
    "    \n",
    "    source_list = [torch.from_numpy(source_data), \n",
    "               torch.from_numpy(source_mask).float(), source_lengths]\n",
    "    target_list = [torch.from_numpy(target_data), \n",
    "               torch.from_numpy(target_mask).float(), target_lengths]\n",
    "        \n",
    "    return source_list + target_list\n",
    "\n",
    "\n",
    "zhen_train_dataset = TranslationDataset(zhen_zh_train_indices,\n",
    "                                       zhen_en_train_indices,\n",
    "                                       token2id_source=zhen_zh_train_token2id,\n",
    "                                       token2id_target=zhen_en_train_token2id)\n",
    "\n",
    "zhen_train_loader = torch.utils.data.DataLoader(dataset=zhen_train_dataset,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                               shuffle=False)\n",
    "\n",
    "zhen_dev_dataset = TranslationDataset(zhen_zh_dev_indices,\n",
    "                                       zhen_en_dev_indices,\n",
    "                                       token2id_source=zhen_zh_train_token2id,\n",
    "                                       token2id_target=zhen_en_train_token2id)\n",
    "\n",
    "zhen_dev_loader = torch.utils.data.DataLoader(dataset=zhen_dev_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                             shuffle=False)\n",
    "\n",
    "vien_train_dataset = TranslationDataset(vien_vi_train_indices,\n",
    "                                       vien_en_train_indices,\n",
    "                                       token2id_source=vien_vi_train_token2id,\n",
    "                                       token2id_target=vien_en_train_token2id)\n",
    "\n",
    "vien_train_loader = torch.utils.data.DataLoader(dataset=vien_train_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                             shuffle=False)\n",
    "\n",
    "vien_dev_dataset = TranslationDataset(vien_vi_dev_indices,\n",
    "                                       vien_en_dev_indices,\n",
    "                                       token2id_source=vien_vi_train_token2id,\n",
    "                                       token2id_target=vien_en_train_token2id)\n",
    "\n",
    "vien_dev_loader = torch.utils.data.DataLoader(dataset=vien_dev_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                             shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [*vien_dev_loader][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Evaluation Metric\n",
    "\n",
    "We use BLEU as the evaluation metric. Specifically, we focus on the corpus-level BLEU function. \n",
    "\n",
    "The code for BLEU is taken from https://github.com/mjpost/sacreBLEU/blob/master/sacrebleu.py#L1022-L1080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /Users/derin/anaconda/lib/python3.6/site-packages (1.2.12)\n",
      "Requirement already satisfied: typing in /Users/derin/anaconda/lib/python3.6/site-packages (from sacrebleu) (3.6.6)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Beam Search Algorithm\n",
    "\n",
    "In this section, we implement the Beam Search algorithm in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize k-many score lists\n",
    "# start only with the whole x\n",
    "# initialize k-many prev y's lists\n",
    "# choose top-k for y1 from the whole vocab\n",
    "# choose top-k for the second time step by expanding the first time step\n",
    "# compute scores by adding log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_size_k = 10\n",
    "\n",
    "# class BeamSearch:\n",
    "    \n",
    "#     \"\"\"RECURSE\"\"\"\n",
    "    \n",
    "#     def __init__(self,\n",
    "#                  beam_size=beam_size_k, ## insert num \n",
    "#                  softmax_out\n",
    "#                 ):\n",
    "#         \"\"\"\n",
    "#         Class that holds beam information, and search & score functions\n",
    "#         - beam_size = beam size\n",
    "#         - softmax_out = the softmax over the vocabulary at time step t, as computed by the RNN decoder,\n",
    "#                         given the source sequence X and the previously decoded y_<t tokens.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         self.beam_size = beam_size\n",
    "#         self.softmax_out = softmax_out\n",
    "        \n",
    "#         # initialize paths\n",
    "#         self.paths = np.empty((self.beam_size))\n",
    "        \n",
    "#         # initialize the dictionary that will hold the path scores \n",
    "#         # and update the scores at each time step\n",
    "#         self.path_score_dict = {}\n",
    "#         # we will later use each i < k as a key and populate this\n",
    "#         # dict with scores\n",
    "\n",
    "        \n",
    "#     def search():\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "#     def score(prev_ys = None):\n",
    "#         \"\"\"- prev_ys = previously decoded tokens (previously generated target language tokens)\n",
    "#         \"\"\"\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model\n",
    "\n",
    "1. Recurrent neural network based encoder-decoder without attention\n",
    "2. Recurrent neural network based encoder-decoder with attention\n",
    "2. Replace the recurrent encoder with either convolutional or self-attention based encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction loss = binary cross entropy between two (vocab_size x 1) vectors\n",
    "# used during training, since we can compare the real Y and and the generated Y\n",
    "# still at each time step of the decoder, we compare up to and including\n",
    "# the real t-th token and the generated t-th, then optimize\n",
    "\n",
    "def loss_function(y_hat, y):\n",
    "    \n",
    "    \"\"\"Takes as input;\n",
    "    - y: correct \"log-softmax\"(binary vector) that represents the correct t-th token in the target sentence,\n",
    "                 (vocab_size x 1) vector\n",
    "    - y_hat: predicted LogSoftmax for the predicted t-th token in the target sentence.\n",
    "             (vocab_size x 1) vector\n",
    "    Returns;\n",
    "    - NLL Loss in training time\"\"\"\n",
    "#     y_hat = torch.log(y_hat) # log softmax\n",
    "    loss = nn.functional.binary_cross_entropy(y_hat,y)\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "# generation/inference time - validation loss = BLEU\n",
    "\n",
    "def compute_BLEU(corpus_hat,corpus):\n",
    "    ## TODO\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_PATH_LENGTH = 400 # make changeable later !!!\n",
    "\n",
    "# class TargetOut:\n",
    "#     def __init__(self,\n",
    "#                  beam_size=5,\n",
    "#                  source_sentence_length=400,\n",
    "#                  time_step=0):\n",
    "#         \"\"\"\n",
    "#         - beam: the tensor that will be populated with beam_size-many paths in each timestep\n",
    "#         - beam_size: the width of the beam, top k tokens to include in the beam search,\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # initialized again for each timestep\n",
    "#         self.beam = torch.empty(beam_size)\n",
    "#         self.beam_size = beam_size\n",
    "#         self.beam_seq = beam_seq\n",
    "#         self.time_step = time_step\n",
    "        \n",
    "#         self.max_target_length = source_sentence_length*(1.5)\n",
    "#         # path is kept by hold_path\n",
    "#         self.path = torch.empty(beam_size, max_target_length)\n",
    "    \n",
    "#     def _add_and_score_paths(self, \n",
    "#              top_k_tokens):\n",
    "        \n",
    "#         \"\"\"top_k_tokens: torch.FloatTensor of indices according to logSoftmax \n",
    "#         (not embeddings - embedding matrix indices or vocab indices)\"\"\"\n",
    "        \n",
    "#         time_step = self.time_step\n",
    "#         self.path[:,time_step] = top_k_tokens\n",
    "        \n",
    "#         return self\n",
    "            \n",
    "#     def _score_paths(self,gru_out):\n",
    "        \n",
    "#         \"\"\"For each path, computes log(P(Y_i|Y_i-1,..,Y_i-n,X)) + log(P(Y_i-1|Y_i-2,..,Y_i-n,X)) + ...\n",
    "#         -gru_out is a softmax over the vocabulary for each timestep, so \n",
    "#         we need to take its log to obtain the scores\"\"\"\n",
    "#         if self.time_step = 0:\n",
    "            \n",
    "        \n",
    "    \n",
    "#     def _hold_path_score(self):\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([7., 5., 4.]), tensor([3, 4, 1]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([3,4,2,7,5,3,2]).topk(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "class BeamSearch(nn.Module):\n",
    "    \n",
    "    \"\"\"network that conducts beam search over the outputs of\n",
    "     any translator network. The translator networks that can \n",
    "     be passed are:\n",
    "     \n",
    "     - Translate (for RNN-enc-dec),\n",
    "     - AttnTranslate (for RNN-enc-dec with attention),\n",
    "     - CNNtranslate (for CNN-encoder based translation).\n",
    "     \n",
    "     The translation networks take care of the encoder-decoder\n",
    "     choices specific to each task. Please see in below sections.\"\"\"\n",
    "\n",
    "    def __init__(self, translator_network, beam_size):\n",
    "        super().__init__()\n",
    "        # translator network that returns the logsoftmax\n",
    "        # over vocabulary size:(vocab_size, 1)\n",
    "        self.translator_network = translator_network\n",
    "        self.beam_size = beam_size\n",
    "        \n",
    "    def init_search_tree(self, batch_size):\n",
    "        beam_size = self.beam_size\n",
    "        self.search_tree = torch.empty(batch_size, beam_size, 1)\n",
    "        return self\n",
    "    \n",
    "    def init_score_tree(self, batch_size):\n",
    "        beam_size = self.beam_size\n",
    "        search_tree = self.search_tree\n",
    "        self.score_tree = torch.zeros(search_tree.size())\n",
    "        return self\n",
    "    \n",
    "    def forward(source_sentence, source_mask, source_lengths,\n",
    "                target_sentence, target_mask, target_lengths):\n",
    "        \n",
    "        self.init_search_tree(BATCH_SIZE)\n",
    "        self.init_score_tree(BATCH_SIZE)\n",
    "        \n",
    "        # at each time step the decoder will give us the logsoftmax\n",
    "        # of one token (batch_size, vocab_size). \n",
    "        output = model(source_sentence, target_sentence,source_mask, \n",
    "                       target_mask, source_lengths,target_lengths)\n",
    "        \n",
    "        # for each sentence in the batch we get the top k predictions\n",
    "        # for each token and append it to the search and score trees. \n",
    "        for i in range(BATCH_SIZE):\n",
    "            beam = output[i].topk(beam_size) # (token scores, token indices)\n",
    "            # cat instead\n",
    "            self.search_tree[i] = self.search_tree.cat(beam[1]) # cat the indices to the search tree\n",
    "            self.score_tree[i,:] = beam[0] # append the scores to the score tree \n",
    "        \n",
    "        # we will sum the logs \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: RNN-based Encoder-Decoder without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "# same as 1st model's RNN encoder\n",
    "# the different part is the attention decoder in model 2\n",
    "\n",
    "class RNNencoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=len(zhen_zh_train_token2id), # for chinese\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=16,\n",
    "                 max_sentence_len=15):\n",
    "        \n",
    "        super(RNNencoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_gru_layers\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embedding_size\n",
    "        self.dropout = percent_dropout\n",
    "        self.embed_source = nn.Embedding(self.vocab_size,\n",
    "                                         self.embed_size,\n",
    "                                         padding_idx=0\n",
    "                                        )\n",
    "        \n",
    "        self.max_sentence_len = max_sentence_len\n",
    "        \n",
    "        self.GRU = nn.GRU(self.embed_size, \n",
    "                          self.hidden_size, \n",
    "                          self.num_layers, \n",
    "                          batch_first=True, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "        self.drop_out_function = nn.Dropout(self.dropout)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        hidden_ = torch.zeros(self.num_layers*self.num_directions, \n",
    "                             batch_size, self.hidden_size).to(device)\n",
    "        return hidden_\n",
    "\n",
    "    def forward(self, source_sentence, source_mask, source_lengths):\n",
    "        \"\"\"Returns source lengths to feed into the decoder, since we do not want\n",
    "        the translation length to be above/below a certain treshold*source sentence length.\"\"\"\n",
    "        \n",
    "        sort_original_source = sorted(range(len(source_lengths)), \n",
    "                             key=lambda sentence: -source_lengths[sentence])\n",
    "        unsort_to_original_source = sorted(range(len(source_lengths)), \n",
    "                             key=lambda sentence: sort_original_source[sentence])\n",
    "        \n",
    "        source_sentence = source_sentence[sort_original_source]\n",
    "        _source_mask = source_mask[sort_original_source]\n",
    "        source_lengths = source_lengths[sort_original_source]\n",
    "        batch_size, seq_len_source = source_sentence.size()\n",
    "        \n",
    "        # init hidden\n",
    "        if self.GRU.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        \n",
    "        self.hidden_source = self.init_hidden(batch_size)\n",
    "        # (self.num_layers*self.num_directions, batch_size, self.hidden_size)\n",
    "        # (1, 32, 256)\n",
    "        # https://pytorch.org/docs/stable/nn.html\n",
    "#         print (\"self hidden size. = \"+str(self.hidden_source.size()))\n",
    "        \n",
    "        # If batch_first == True, then the input and output tensors are provided as \n",
    "        # (batch_size, seq_len, feature)\n",
    "        # https://pytorch.org/docs/stable/nn.html\n",
    "#         print (\"seq len source = \"+str(seq_len_source))\n",
    "        embeds_source = self.embed_source(source_sentence).view(batch_size, seq_len_source,\n",
    "                                                               self.embed_size)\n",
    "        \n",
    "#         print (\"embeds source size = \"+str(embeds_source.size()))\n",
    "        \n",
    "        embeds_source = source_mask*embeds_source + (1-_source_mask)*embeds_source.clone().detach()\n",
    "        \n",
    "#         print (\"embeds source after mask size = \"+str(embeds_source.size()))\n",
    "        \n",
    "        embeds_source = torch.nn.utils.rnn.pack_padded_sequence(embeds_source, \n",
    "                                                                source_lengths, \n",
    "                                                                batch_first=True)\n",
    "        \n",
    "        gru_out_source, self.hidden_source = self.GRU(embeds_source, self.hidden_source)\n",
    "        \n",
    "#         print (\"hidden source size = \"+str(self.hidden_source.size()))\n",
    "        \n",
    "        \n",
    "        # ref: pytorch documentation\n",
    "        # hidden source : h_n of shape \n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "#         print (\"hidden source size = \"+str(self.hidden_source.size()))\n",
    "        \n",
    "        # ref: pytorch documentation\n",
    "        # Like output, the layers can be separated using \n",
    "        # h_n.view(num_layers, num_directions, batch_size, hidden_size)\n",
    "        hidden_source = self.hidden_source.view(self.num_layers, self.num_directions, \n",
    "                                                batch_size, self.hidden_size)\n",
    "        # the following should print (1, 1, 32, 256) for this config\n",
    "#         print (\"hidden source size after view = \"+str(hidden_source.size()))\n",
    "        \n",
    "        # get the mean along 0th axis (over layers)\n",
    "        hidden_source = torch.mean(hidden_source, dim=0) ## mean instead of sum for source representation as suggested in the class\n",
    "        # the following should print (1, 32, 256)\n",
    "#         print (\"hidden source size after mean = \"+str(hidden_source.size()))\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            hidden_source = torch.cat([hidden_source[:,i,:] for i in range(self.num_directions)], dim=1)\n",
    "            gru_out_source = gru_out_source\n",
    "        else:\n",
    "            hidden_source = hidden_source\n",
    "            gru_out_source = gru_out_source\n",
    "            \n",
    "        # view before unsort\n",
    "        hidden_source = hidden_source.view(batch_size, self.hidden_size)\n",
    "        \n",
    "        # the following should print (32, 256)\n",
    "        # print(\"hidden source size before unsort = \"+str(hidden_source.size()))\n",
    "        # UNSORT HIDDEN\n",
    "        hidden_source = hidden_source[unsort_to_original_source] ## back to original indices\n",
    "        \n",
    "        gru_out_source, _ = torch.nn.utils.rnn.pad_packed_sequence(gru_out_source,\n",
    "                                                                  batch_first=True)\n",
    "        \n",
    "#         ### UNSORT GRU OUT\n",
    "#         # get the mean for the GRU output (batch_size, output size, hidden_size)\n",
    "#         gru_out_source = torch.mean(gru_out_source, dim=1).view(batch_size, 1, self.hidden_size)\n",
    "#         gru_out_source = gru_out_source[unsort_to_original_source]\n",
    "# #         print (\"gru_out_source size = \"+str(gru_out_source.size()))\n",
    "        \n",
    "        source_lengths = source_lengths[unsort_to_original_source]\n",
    "        \n",
    "        # here we return both hidden and out since we will pass both to\n",
    "        # the attention decoder\n",
    "        return hidden_source, source_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNdecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=len(zhen_en_train_token2id), # for chinese-english's english\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=1,\n",
    "                 max_sentence_len=15):\n",
    "        \n",
    "        super(RNNdecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embedding_size\n",
    "        self.dropout = percent_dropout\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_gru_layers\n",
    "        \n",
    "        self.GRU = nn.GRU(self.embed_size, \n",
    "                          self.hidden_size, \n",
    "                          self.num_layers, \n",
    "                          batch_first=True, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "        self.GRUcell = nn.GRUCell(self.embed_size, \n",
    "                          self.hidden_size)\n",
    "        \n",
    "        self.ReLU = nn.ReLU\n",
    "        \n",
    "        self.drop_out_function = nn.Dropout(self.dropout)\n",
    "        \n",
    "        self.embed_target = nn.Embedding(self.vocab_size,\n",
    "                                         self.embed_size, padding_idx=0)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # *2 because we are concating hidden with embedding plus context\n",
    "        self.linear_layer = nn.Linear(self.hidden_size*2, self.vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=0)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, \n",
    "                             batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "    def forward(self,\n",
    "                decoder_hidden, ## decoder_hidden = encoder_hidden at first time_step\n",
    "                input_, # input\n",
    "                target_lengths,\n",
    "                target_mask,\n",
    "                time_step):\n",
    "        \n",
    "        # input (batch_size, seq_len_target = 1)\n",
    "        # hidden (self.num_layers*self.num_directions, batch_size, self.hidden_size)\n",
    "        \n",
    "        self.input = input_\n",
    "#         print (\"self.input size = \"+str(self.input.size()))\n",
    "        \n",
    "        sort_original_target = sorted(range(len(target_lengths)), \n",
    "                             key=lambda sentence: -target_lengths[sentence])\n",
    "        unsort_to_original_target = sorted(range(len(target_lengths)), \n",
    "                             key=lambda sentence: sort_original_target[sentence])\n",
    "        \n",
    "        self.input = self.input[sort_original_target]\n",
    "        _target_mask = target_mask[sort_original_target]\n",
    "        target_lengths = target_lengths[sort_original_target]\n",
    "        \n",
    "        # seq_len_target is always 1 in the decoder since we are \n",
    "        # passing the tokens for only 1 time_step at a time\n",
    "        batch_size, seq_len_target = self.input.size()\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        \n",
    "        # hidden => initial hidden will be the same as the context\n",
    "        # vector, which is the hidden_source tensor\n",
    "        # then as we update the hidden state at each time step, this will be \n",
    "        # updated as well\n",
    "        self.hidden = decoder_hidden.view(self.num_layers*self.num_directions,\n",
    "                                          batch_size, self.hidden_size)\n",
    "        \n",
    "        # the following should print (1, 32, 256) for this config\n",
    "#         print (\"self.hidden size = \"+str(self.hidden.size()))\n",
    "        \n",
    "        self.input = self.input.unsqueeze(1)\n",
    "        \n",
    "        embeds_target = self.drop_out_function(self.embed_target(self.input.long())).view(batch_size,\n",
    "                                                                                   seq_len_target,\n",
    "                                                                                   -1)\n",
    "    \n",
    "#         embeds_target = target_mask*embeds_target + (1-_target_mask)*embeds_target.clone().detach()\n",
    "        embeds_target = target_mask[:,time_step,:].unsqueeze(1)*embeds_target + \\\n",
    "                        (1-_target_mask[:,time_step,:].unsqueeze(1))*embeds_target.clone().detach()\n",
    "\n",
    "#         print (\"embeds_target size = \"+str(embeds_target.size()))    \n",
    "        \n",
    "#         embeds_target = torch.nn.utils.rnn.pack_padded_sequence(embeds_target,\n",
    "#                                                         target_lengths,\n",
    "#                                                         batch_first=True)\n",
    "        \n",
    "#         print (\"type embeds target = \"+str(type(embeds_target)))\n",
    "\n",
    "        gru_out_target, self.hidden = self.GRU(embeds_target.data.view(batch_size, 1, self.embed_size),\n",
    "                                               self.hidden)\n",
    "        \n",
    "        # ref: pytorch documentation\n",
    "        # hidden source : h_n of shape \n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "        # the following should print (1, 32, 256) for this config\n",
    "#         print (\"hidden size after GRU = \"+str(self.hidden.size()))\n",
    "        \n",
    "        # undo packing \n",
    "#         gru_out_target, _ = torch.nn.utils.rnn.pad_packed_sequence(gru_out_target,\n",
    "#                                                                    batch_first=True)\n",
    "        \n",
    "#         print (\"out size after GRU = \"+str(gru_out_target.size()))\n",
    "\n",
    "\n",
    "        hidden = self.hidden.view(self.num_layers, self.num_directions,\n",
    "                                  batch_size, self.hidden_size)\n",
    "        hidden = torch.sum(hidden, dim=0) # we don't divide here, just sum\n",
    "    \n",
    "#         print (\"hidden size = \"+str(hidden.size()))\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            # separate layers\n",
    "            gru_out_target = gru_out_target.contiguous().view(seq_len_target,\n",
    "                                                              batch_size,\n",
    "                                                              self.num_directions,\n",
    "                                                              self.hidden_size)\n",
    "        else:\n",
    "            gru_out_target = gru_out_target\n",
    "        \n",
    "#         print (\"gru out size = \"+str(gru_out_target.size()))\n",
    "        \n",
    "        # sum along sequence\n",
    "        gru_out_target = torch.sum(gru_out_target, dim=1) # we don't divide here, just sum\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            hidden = torch.cat([hidden[:,i,:] for i in range(self.num_directions)], \n",
    "                               dim=0)\n",
    "            gru_out_target = torch.cat([gru_out_target[:,i,:] for i in range(self.num_directions)], \n",
    "                                       dim=1)\n",
    "        else:\n",
    "            hidden = hidden.view(batch_size, \n",
    "                                 self.num_directions, self.hidden_size)\n",
    "            gru_out_target = gru_out_target.view(batch_size,\n",
    "                                                 self.num_directions, self.hidden_size)\n",
    "        \n",
    "        hidden = hidden[unsort_to_original_target] ## back to original indices\n",
    "        gru_out_target = gru_out_target[unsort_to_original_target] ## back to original indices\n",
    "\n",
    "        gru_out_target = self.sigmoid(gru_out_target)\n",
    "        # concating embedding + context = gru_out_target with hidden\n",
    "        out = torch.cat([gru_out_target,hidden], dim=2)\n",
    "        \n",
    "#         print (\"out size after concat = \"+str(out.size()))\n",
    "        \n",
    "        out = self.linear_layer(out)\n",
    "        \n",
    "        # softmax over vocabulary\n",
    "        pred = self.log_softmax(out)\n",
    "\n",
    "        return pred, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(32,1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_softmax(tensor_of_indices,\n",
    "                       batch_size,\n",
    "                       vocab_size = len(zhen_en_train_token2id)):\n",
    "    \"\"\"\n",
    "    - takes as input a time_step vector of the batch (t-th token of each sentence in the batch)\n",
    "      size: (batch_size, 1)\n",
    "    - converts it to softmax of (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    index_tensor_ = tensor_of_indices.view(-1,1).long()\n",
    "        \n",
    "    one_hot = torch.FloatTensor(batch_size, vocab_size).zero_()\n",
    "    one_hot.scatter_(1, index_tensor_, 1)\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 59325])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_softmax(torch.FloatTensor([2,3,4]), 3).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_softmax(torch.ones(32), 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.],\n",
       "         [6.]]), tensor([[3],\n",
       "         [3]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([[2,3,4,4],[4,5,6,6]]).topk(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.empty(32,1), torch.ones(32,1)),1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chinese -> english\n",
    "enc = RNNencoder(vocab_size=len(zhen_zh_train_token2id), # for chinese\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=16)\n",
    "\n",
    "dec = RNNdecoder(vocab_size=len(zhen_en_train_token2id), # for chinese-english's english\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=1)\n",
    "\n",
    "# model = Translate(enc, dec).to(device)\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "def train(encoder, decoder, loader=zhen_train_loader,\n",
    "          optimizer = torch.optim.Adam([*enc.parameters()] + [*dec.parameters()], lr=1e-4),\n",
    "#           encoder_optimizer = torch.optim.Adam(enc.parameters(), lr=1e-4),\n",
    "#           decoder_optimizer = torch.optim.Adam(dec.parameters(), lr=1e-4),\n",
    "          epoch=None):\n",
    "    \n",
    "#     encoder_optimizer.zero_grad()\n",
    "#     decoder_optimizer.zero_grad()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for batch_idx, (source_sentence, source_mask, source_lengths, \n",
    "                    target_sentence, target_mask, target_lengths)\\\n",
    "                    in enumerate(loader):\n",
    "        \n",
    "        source_sentence, source_mask = source_sentence.to(device), source_mask.to(device) \n",
    "        target_sentence, target_mask = target_sentence.to(device), target_mask.to(device)\n",
    "        \n",
    "        encoder_hidden, source_lengths = encoder(source_sentence,\n",
    "                                               source_mask,\n",
    "                                               source_lengths)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        # decoder should start with SOS tokens \n",
    "        # ref: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "        input_ = SOS_token*torch.ones(BATCH_SIZE,1).view(-1,1).to(device)\n",
    "        \n",
    "        for t in range(0, target_sentence.size(1)):\n",
    "            \n",
    "            decoder_out, decoder_hidden = decoder(decoder_hidden, # = gru_out_source - instead of encoded_source[0]\n",
    "                                                 input_, # instead of target sentence up to t \n",
    "                                                 target_lengths,  # target lengths\n",
    "                                                 target_mask,\n",
    "                                                 t)\n",
    "            \n",
    "#             print (\"decoder out size = \"+str(decoder_out.size()))\n",
    "            target_tokens = convert_to_softmax(target_sentence[:,t], BATCH_SIZE)\n",
    "            \n",
    "            loss += F.binary_cross_entropy(F.sigmoid(decoder_out), target_tokens)\n",
    "        print (\"loss = \"+str(loss))\n",
    "        loss.backward(retain_graph = True)\n",
    "        \n",
    "        optimizer.step()\n",
    "            \n",
    "#             epoch_loss.backward(retain_graph = True) # if necessary call retain_graph = True\n",
    "            \n",
    "            \n",
    "#             encoder_optimizer.step()\n",
    "#             decoder_optimizer.step()\n",
    "            \n",
    "    return epoch_loss/BATCH_SIZE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/derin/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([32, 59325])) that is different to the input size (torch.Size([32, 1, 59325])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = tensor(0.4624, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(0.9248, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(1.3872, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(1.8496, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(2.3120, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(2.7744, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(3.2368, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(3.6992, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(4.1616, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(4.6240, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(5.0864, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(5.5488, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(6.0112, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(6.4736, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(6.9360, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(7.3984, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(7.8608, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(8.3231, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(8.7855, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(9.2479, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(9.7103, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(10.1727, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(10.6350, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(11.0974, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(11.5598, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(12.0221, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(12.4845, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(12.9469, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(13.4093, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(13.8716, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(14.3340, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(14.7963, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(15.2587, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(15.7210, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(16.1834, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(16.6457, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(17.1081, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(17.5704, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(18.0328, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(18.4951, grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "lr = 1e-4\n",
    "# batch_\n",
    "\n",
    "loss_train = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print (\"epoch = \"+str(epoch))\n",
    "\n",
    "    loss = train(enc, dec,\n",
    "                 loader = zhen_train_loader,\n",
    "                 optimizer = torch.optim.Adam([*enc.parameters()] + [*dec.parameters()], lr=1e-4),\n",
    "                 epoch = epoch)\n",
    "    \n",
    "    loss_train.append(loss)\n",
    "    \n",
    "    print (loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 RNN-based Encoder-Decoder with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 RNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "# same as 1st model's RNN encoder except that works on one token at a time\n",
    "# the different part is the attention decoder in model 2\n",
    "\n",
    "class attnRNNencoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=len(zhen_zh_train_token2id), # for chinese\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=4,\n",
    "                 max_sentence_len=50):\n",
    "        \n",
    "        super(attnRNNencoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_gru_layers\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embedding_size\n",
    "        self.dropout = percent_dropout\n",
    "        self.embed_source = nn.Embedding(self.vocab_size,\n",
    "                                         self.embed_size,\n",
    "                                         padding_idx=0\n",
    "                                        )\n",
    "        \n",
    "        self.max_sentence_len = max_sentence_len\n",
    "        \n",
    "        self.GRU = nn.GRU(self.embed_size, \n",
    "                          self.hidden_size, \n",
    "                          self.num_layers, \n",
    "                          batch_first=True, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "        self.drop_out_function = nn.Dropout(self.dropout)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        hidden_ = torch.zeros(self.num_layers*self.num_directions, \n",
    "                             batch_size, self.hidden_size).to(device)\n",
    "        return hidden_\n",
    "\n",
    "    def forward(self, source_sentence, source_mask, source_lengths,\n",
    "                time_step):\n",
    "        \"\"\"Returns source lengths to feed into the decoder, since we do not want\n",
    "        the translation length to be above/below a certain treshold*source sentence length.\"\"\"\n",
    "        \n",
    "        source_sentence = source_sentence.view(-1,1)\n",
    "        # print (\"source size = \"+str(source_sentence.size()))\n",
    "        # (batch_size, 1)\n",
    "        \n",
    "        sort_original_source = sorted(range(len(source_lengths)), \n",
    "                             key=lambda sentence: -source_lengths[sentence])\n",
    "        unsort_to_original_source = sorted(range(len(source_lengths)), \n",
    "                             key=lambda sentence: sort_original_source[sentence])\n",
    "        \n",
    "        source_sentence = source_sentence[sort_original_source]\n",
    "        _source_mask = source_mask[sort_original_source]\n",
    "        source_lengths = source_lengths[sort_original_source]\n",
    "        batch_size, seq_len_source = source_sentence.size()\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "            \n",
    "        self.hidden_source = self.init_hidden(batch_size)\n",
    "        # (self.num_layers*self.num_directions, batch_size, self.hidden_size)\n",
    "        # (1, 32, 256)\n",
    "        # https://pytorch.org/docs/stable/nn.html\n",
    "        # print (\"self hidden size. = \"+str(self.hidden_source.size()))\n",
    "        \n",
    "        # If batch_first == True, then the input and output tensors are provided as \n",
    "        # (batch_size, seq_len, feature)\n",
    "        # https://pytorch.org/docs/stable/nn.html\n",
    "        # print (\"seq len source = \"+str(seq_len_source))\n",
    "        \n",
    "        source_sentence = source_sentence.unsqueeze(1)\n",
    "        \n",
    "        embeds_source = self.embed_source(source_sentence).view(batch_size, seq_len_source,\n",
    "                                                               self.embed_size)\n",
    "        \n",
    "        # print (\"embeds source size = \"+str(embeds_source.size()))\n",
    "        \n",
    "        embeds_source = source_mask[:,time_step,:].unsqueeze(1)*embeds_source + \\\n",
    "                        (1-_source_mask[:,time_step,:].unsqueeze(1))*embeds_source.clone().detach()\n",
    "        \n",
    "        # print (\"embeds source after mask size = \"+str(embeds_source.size()))\n",
    "        \n",
    "        \n",
    "#         embeds_source = torch.nn.utils.rnn.pack_padded_sequence(embeds_source, \n",
    "#                                                                 source_lengths, \n",
    "#                                                                 batch_first=True)\n",
    "        \n",
    "        gru_out_source, self.hidden_source = self.GRU(embeds_source, self.hidden_source)\n",
    "        \n",
    "        # print (\"gru out source size = \"+str(gru_out_source.size()))\n",
    "        \n",
    "        # print (\"hidden source size = \"+str(self.hidden_source.size()))\n",
    "        # print (\"gru out source size = \"+str(gru_out_source.size()))\n",
    "        \n",
    "        # hidden source size = torch.Size([1, 32, 256])\n",
    "        # gru out source size = torch.Size([32, 350, 256])\n",
    "        \n",
    "        # ref: pytorch documentation\n",
    "        # hidden source : h_n of shape \n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "        # print (\"hidden source size = \"+str(self.hidden_source.size()))\n",
    "        \n",
    "        # ref: pytorch documentation\n",
    "        # Like output, the layers can be separated using \n",
    "        # h_n.view(num_layers, num_directions, batch_size, hidden_size)\n",
    "        hidden_source = self.hidden_source.view(self.num_layers, self.num_directions, \n",
    "                                                batch_size, self.hidden_size)\n",
    "        \n",
    "        # print (\"hidden source size = \"+str(hidden_source.size()))\n",
    "        # hidden source size = torch.Size([1, 1, 32, 256])\n",
    "        \n",
    "        # the following should print (1, 1, 32, 256) for this config\n",
    "        # print (\"hidden source size after view = \"+str(hidden_source.size()))\n",
    "        \n",
    "        # get the mean along 0th axis (over layers)\n",
    "        hidden_source = torch.mean(hidden_source, dim=0) ## mean instead of sum for source representation as suggested in the class\n",
    "        # the following should print (1, 32, 256)\n",
    "        # print (\"hidden source size after mean = \"+str(hidden_source.size()))\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            hidden_source = torch.cat([hidden_source[:,i,:] for i in range(self.num_directions)], dim=1)\n",
    "            gru_out_source = gru_out_source\n",
    "        else:\n",
    "            hidden_source = hidden_source\n",
    "            gru_out_source = gru_out_source\n",
    "            \n",
    "        # view before unsort\n",
    "        hidden_source = hidden_source.view(batch_size, self.hidden_size)\n",
    "        \n",
    "        # the following should print (32, 256)\n",
    "        # print(\"hidden source size before unsort = \"+str(hidden_source.size()))\n",
    "        # UNSORT HIDDEN\n",
    "        hidden_source = hidden_source[unsort_to_original_source] ## back to original indices\n",
    "        \n",
    "#         gru_out_source, _ = torch.nn.utils.rnn.pad_packed_sequence(gru_out_source,\n",
    "#                                                                   batch_first=True)\n",
    "        \n",
    "        ### UNSORT GRU OUT\n",
    "        # get the mean for the GRU output (batch_size, output size, hidden_size)\n",
    "        gru_out_source = gru_out_source.view(batch_size, seq_len_source, self.hidden_size)\n",
    "        # gru_out_source = torch.mean(gru_out_source, dim=1).view(batch_size, 1, self.hidden_size)\n",
    "        gru_out_source = gru_out_source[unsort_to_original_source]\n",
    "        # print (\"gru_out_source size = \"+str(gru_out_source.size()))\n",
    "        \n",
    "        source_lengths = source_lengths[unsort_to_original_source]\n",
    "        \n",
    "        # here we return both hidden and out since we will pass both to\n",
    "        # the attention decoder\n",
    "        return hidden_source, gru_out_source, source_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=len(zhen_zh_train_token2id), \n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 max_sentence_len=50, \n",
    "                 num_gru_layers=1):\n",
    "\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = percent_dropout\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "        self.num_layers = num_gru_layers\n",
    "        self.embed_size = embedding_size\n",
    "        \n",
    "        self.embed_target = nn.Embedding(self.vocab_size,\n",
    "                                         self.hidden_size,\n",
    "                                         padding_idx=0\n",
    "                                        )\n",
    "        \n",
    "        self.GRU = nn.GRU(self.hidden_size, \n",
    "                          self.hidden_size,\n",
    "                          self.num_layers, \n",
    "                          batch_first=True, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "        # we concat embeds with hidden before attention, thus the input size\n",
    "        # of the linear attn layer is embed + hidden, and the output is hidden.\n",
    "        self.attn = nn.Linear(self.hidden_size*2, \n",
    "                              self.max_sentence_len)\n",
    "        \n",
    "        # we combine embeds with attention applied (self.attn out) before attn_combine\n",
    "        # so the input size of the linear attn_combine layer is embed_size + hidden_size \n",
    "        # \n",
    "        self.attn_combine = nn.Linear(self.hidden_size*2, \n",
    "                                      self.hidden_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        \n",
    "        self.log_softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self,\n",
    "                hidden, ## decoder_hidden = encoder_hidden at first time_step\n",
    "                input_, # input (batch_size, seq_len = 1)\n",
    "                encoder_outputs, # (encoder hidden and encoder out)\n",
    "                target_lengths,\n",
    "                target_mask,\n",
    "                time_step):\n",
    "        \n",
    "        # input (batch_size, seq_len = 1)\n",
    "        self.input = input_\n",
    "        print (\"input size. =\"+str(self.input.size()))\n",
    "        \n",
    "        sort_original_target = sorted(range(len(target_lengths)), \n",
    "                             key=lambda sentence: -target_lengths[sentence])\n",
    "        unsort_to_original_target = sorted(range(len(target_lengths)), \n",
    "                             key=lambda sentence: sort_original_target[sentence])\n",
    "        \n",
    "        self.input = self.input[sort_original_target]\n",
    "        _target_mask = target_mask[sort_original_target]\n",
    "        target_lengths = target_lengths[sort_original_target]\n",
    "        \n",
    "        # seq_len_target is always 1 in the decoder since we are \n",
    "        # passing the tokens for only 1 time_step at a time\n",
    "        batch_size, seq_len_target = self.input.size()\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "            \n",
    "        self.hidden = hidden.view(batch_size, \n",
    "                                  self.num_layers*self.num_directions, \n",
    "                                  self.hidden_size)\n",
    "        \n",
    "        self.input = self.input.unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        embeds_target = self.dropout(self.embed_target(self.input.long()))\\\n",
    "                                                                .view(batch_size,\n",
    "                                                                      seq_len_target, -1)\n",
    "        \n",
    "        embeds_target = target_mask[:,time_step,:].unsqueeze(1)*embeds_target + \\\n",
    "                        (1-_target_mask[:,time_step,:].unsqueeze(1))*embeds_target.clone().detach()\n",
    "        \n",
    "        # print (\"embeds target size = \"+str(embeds_target.size()))\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embeds_target, self.hidden), 2)), dim=2)\n",
    "#         print (\"attn_weights size = \"+str(attn_weights.size()))\n",
    "        \n",
    "        # try for loop and bmm and see if these are the same \n",
    "        # print (\"enc out size = \"+str(encoder_outputs.size()))\n",
    "        attn_applied = torch.zeros(batch_size, self.max_sentence_len, self.hidden_size)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "#             print (\"attn_weights[i] = \"+str(attn_weights[i]))\n",
    "#             print (\"encoder_outputs[i] = \"+str(encoder_outputs[i]))\n",
    "            apply = torch.bmm(attn_weights[i].unsqueeze(0),\n",
    "                              encoder_outputs[i].unsqueeze(0))\n",
    "            \n",
    "            attn_applied[i] = apply\n",
    "        \n",
    "        print (\"attn_applied size = \"+str(attn_applied.size()))\n",
    "        print (\"embeds target size = \"+ str(embeds_target.size()))\n",
    "#         print (\"encoder outputs = \"+str(encoder_outputs))\n",
    "        print (\"attn_applied[:,time_step,:] size = \"+str(attn_applied[:,time_step,:].view(batch_size,\n",
    "                                                                                          1, self.hidden_size).size()))\n",
    "\n",
    "        output = torch.cat((embeds_target,\n",
    "                            attn_applied[:,time_step,:].view(batch_size,1,\n",
    "                                                             self.hidden_size)),2)\n",
    "        \n",
    "        output = self.attn_combine(output)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        \n",
    "        self.hidden = self.hidden.view(self.num_layers*self.num_directions,\n",
    "                                       batch_size,\n",
    "                                       self.hidden_size)\n",
    "\n",
    "        output, self.hidden = self.GRU(output, self.hidden)\n",
    "        \n",
    "        self.hidden = self.hidden.view(batch_size,\n",
    "                                       self.num_layers*self.num_directions,\n",
    "                                       self.hidden_size)\n",
    "        \n",
    "        output = output[unsort_to_original_target]\n",
    "        self.hidden = self.hidden[unsort_to_original_target]\n",
    "        \n",
    "        print (\"output size = \"+str(output.size()))\n",
    "        print (\"hidden size = \"+str(self.hidden.size()))\n",
    "        \n",
    "        output = self.out(output)\n",
    "        print (\"out after linear size = \"+str(output.size()))\n",
    "        \n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        print (\"logsoft size = \"+str(output.size()))\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "class AttnTranslate(nn.Module):\n",
    "    def __init__(self, encoder, decoder, use_teacher_forcing=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.use_teacher_forcing = use_teacher_forcing\n",
    "        self.max_length = self.encoder.max_sentence_len\n",
    "        \n",
    "    def forward(self, source_sentence, target_sentence, \n",
    "                source_mask, target_mask, source_lengths,\n",
    "                target_lengths):\n",
    "        \n",
    "        # following should print (batch_size, max_sentence_len) = (32, 350)\n",
    "        # print (\"target_sentence size = \"+str(target_sentence.size()))\n",
    "        \n",
    "        # to hold previously decoded ys\n",
    "        y_outputs = torch.zeros(batch_size, \n",
    "                                target_sentence.size(1), \n",
    "                                len(zhen_en_train_token2id)).to(device)\n",
    "        \n",
    "        encoder_outputs = torch.zeros(BATCH_SIZE,\n",
    "                                      self.max_length, \n",
    "                                      self.encoder.hidden_size, \n",
    "                                      device=device)\n",
    "        \n",
    "        for i in range(self.max_length):\n",
    "            #last hidden state of the encoder is the context\n",
    "            encoder_hidden, encoder_output, source_lengths = self.encoder(source_sentence[:,i],\n",
    "                                                                          source_mask,\n",
    "                                                                          source_lengths,\n",
    "                                                                          i) # i as time_step\n",
    "            # doing what we want, uncomment the prints below to check\n",
    "            # i-th time_step token of each sentence in batch is filled with the corresponding\n",
    "            # encoder output\n",
    "            encoder_outputs[:,i,:] = encoder_output.unsqueeze(1)[:,0,0]\n",
    "            \n",
    "            # print (\"encoder_outputs[:,i,:] size = \"+str(encoder_outputs[:,i,:].size()))\n",
    "            # print (\"encoder outputs size = \"+str(encoder_outputs.size()))\n",
    "            \n",
    "            # print (\"encoder outputs = \"+str(encoder_outputs))\n",
    "        \n",
    "#         print (\"encoder outputs size = \"+str(encoder_outputs.size()))\n",
    "#         print (\"enc outs = \"+str(encoder_outputs))\n",
    "\n",
    "        # encoder hidden also used as the initial hidden state of the decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # decoder should start with SOS tokens \n",
    "        # ref: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "        input_ = SOS_token*torch.ones(BATCH_SIZE,1).view(-1,1)\n",
    "        \n",
    "        # TODO\n",
    "        # Obtain target tensor using convert_to_softmax (debug function first)  \n",
    "        # target tensor -> batch_size, max_sent_len, vocab_size = 32, 350, vocab_size\n",
    "#         target_tensor = torch.zeros()\n",
    "        # append it to y_outputs\n",
    "\n",
    "        target_length = target_sentence.size(1)\n",
    "\n",
    "        if self.use_teacher_forcing:\n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            for di in range(target_length):\n",
    "                # target tensor -> (batch_size, vocab_size) of t-th time step tokens \n",
    "                # from each sentence, converted to softmax (binary)\n",
    "                target_tensor = convert_to_softmax(target_sentence[:,di],32)\n",
    "#                 print (\"target_tensor = \"+str(target_tensor))\n",
    "                print (\"target tensor size = \"+str(target_tensor.size()))\n",
    "                \n",
    "                # take ith token from each sentence in the batch, and convert it to \n",
    "                # softmax\n",
    "                decoder_out, decoder_hidden, decoder_attention = self.decoder(\n",
    "                    decoder_hidden, input_, encoder_outputs,\n",
    "                    target_lengths, target_mask, di) # di as time_step\n",
    "                \n",
    "                # decoder out should be size (32, 1, vocab_size)\n",
    "                \n",
    "                loss += loss_function(decoder_out, target_tensor[:,di,:]) # slicing (whole batch, \n",
    "                                                                          #          token_index, vocab_size)\n",
    "                decoder_input = target_sentence[:,di] # Teacher forcing\n",
    "            \n",
    "        else:\n",
    "            # Without teacher forcing: use its own predictions as the next input\n",
    "            # just like we did in the RNN encoder-decoder above\n",
    "            for di in range(target_length):\n",
    "                # target tensor -> (batch_size, vocab_size) of t-th time step tokens \n",
    "                # from each sentence, converted to softmax (binary)\n",
    "                target_tensor = convert_to_softmax(target_sentence[:,di],32)\n",
    "#                 print (\"target_tensor = \"+str(target_tensor))\n",
    "                print (\"target tensor size = \"+str(target_tensor.size()))\n",
    "                \n",
    "                decoder_out, decoder_hidden, decoder_attention = self.decoder(\n",
    "                    decoder_hidden, input_, encoder_outputs, target_lengths,\n",
    "                    target_mask, di)\n",
    "                \n",
    "                token_out = torch.max(decoder_out.view(BATCH_SIZE,self.decoder.vocab_size),1)[1]\n",
    "                input_ = token_out.view(-1,1)\n",
    "                print (\"decoder input size = \"+str(input_.size()))\n",
    "\n",
    "                loss = nn.functional.binary_cross_entropy(decoder_out, target_tensor[di])\n",
    "            \n",
    "        for t in range(0, target_sentence.size(1)):\n",
    "            \n",
    "            decoder_out, decoder_hidden = self.decoder(decoder_hidden, # = gru_out_source - instead of encoded_source[0]\n",
    "                                                 input_, # instead of target sentence up to t \n",
    "                                                 target_lengths,  # target lengths\n",
    "                                                 target_mask,\n",
    "                                                 t)\n",
    "            \n",
    "#             print (\"decoder out size = \"+str(decoder_out.size()))\n",
    "            for s in range(batch_size):\n",
    "                y_outputs[s,t] = decoder_out[s,0]\n",
    "            \n",
    "        return y_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "def train(model, loader=zhen_train_loader,criterion=loss_function,\n",
    "          encoder_optimizer=None, decoder_optimizer = None, \n",
    "          epoch=None):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_idx, (source_sentence, source_mask, source_lengths, \n",
    "                    target_sentence, target_mask, target_lengths)\\\n",
    "    in enumerate(loader):\n",
    "        \n",
    "        source_sentence, source_mask = source_sentence.to(device), source_mask.to(device),  \n",
    "        target_sentence, target_mask = target_sentence.to(device), target_mask.to(device),\n",
    "        \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        # output softmax as generated by decoder \n",
    "        output = model(source_sentence, target_sentence, \n",
    "                source_mask, target_mask, source_lengths,\n",
    "                target_lengths)\n",
    "        \n",
    "        print (\"output size = \"+str(output.size()))\n",
    "        \n",
    "        batch_target = torch.zeros(batch_size,model.decoder.embed_size)\n",
    "        for i in range(target_sentence.size(0)):\n",
    "            one_hot_ = convert_to_softmax(target_sentence[i])\n",
    "            batch_target[i] = one_hot_\n",
    "            \n",
    "        print (\"batch_target[ix] sum = \"+str(torch.sum(batch_target[0],1)))\n",
    "\n",
    "        loss = criterion(out, target)\n",
    "        print (\"loss = \"+str(loss))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        print (\"backprop done.\")\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        print (\"steps done\")\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss/BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chinese -> english\n",
    "enc = attnRNNencoder(vocab_size=len(zhen_zh_train_token2id), # for chinese\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=1)\n",
    "\n",
    "dec = AttnDecoderRNN(vocab_size=len(zhen_en_train_token2id), # for chinese-english's english\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=1)\n",
    "\n",
    "model = AttnTranslate(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "target tensor size = torch.Size([32, 59325])\n",
      "input size. =torch.Size([32, 1])\n",
      "attn_applied size = torch.Size([32, 50, 256])\n",
      "embeds target size = torch.Size([32, 1, 256])\n",
      "attn_applied[:,time_step,:] size = torch.Size([32, 1, 256])\n",
      "output size = torch.Size([32, 1, 256])\n",
      "hidden size = torch.Size([32, 1, 256])\n",
      "out after linear size = torch.Size([32, 1, 59325])\n",
      "logsoft size = torch.Size([32, 1, 59325])\n",
      "decoder input size = torch.Size([32, 1])\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-17b37e63365e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                        decoder_optimizer = torch.optim.Adam(model.decoder.parameters(),\n\u001b[1;32m     15\u001b[0m                                                     lr = lr),\n\u001b[0;32m---> 16\u001b[0;31m                       epoch = epoch)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-f92c70c4aec7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, criterion, encoder_optimizer, decoder_optimizer, epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m         output = model(source_sentence, target_sentence, \n\u001b[1;32m     22\u001b[0m                 \u001b[0msource_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 target_lengths)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"output size = \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-d09df0746197>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source_sentence, target_sentence, source_mask, target_mask, source_lengths, target_lengths)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"decoder input size = \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "lr = 1e-4\n",
    "# batch_\n",
    "\n",
    "loss_train = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print (\"epoch = \"+str(epoch))\n",
    "\n",
    "    loss = train(model,\n",
    "                       loader = zhen_train_loader,\n",
    "                       encoder_optimizer = torch.optim.Adam(model.encoder.parameters(), \n",
    "                                                   lr=lr),\n",
    "                       decoder_optimizer = torch.optim.Adam(model.decoder.parameters(),\n",
    "                                                    lr = lr),\n",
    "                      epoch = epoch)\n",
    "    \n",
    "    loss_train.append(loss)\n",
    "    \n",
    "    print (loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoder Replacement with Convolutional or Self-attention-based Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Conv1d(300, 256, 1, stride=2)\n",
    "input = torch.randn(32, 1, 300)\n",
    "output = m(input.transpose(1,2)).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 256])"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://github.com/yanwii/seq2seq/blob/master/seq2seq.py\n",
    "\n",
    "# ENCODER\n",
    "\n",
    "class CNNencoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_size, # in channels\n",
    "                 hidden_size, \n",
    "                 kernel_size, \n",
    "                 padding = 1,\n",
    "                 stride = 2,\n",
    "                 percent_dropout = 0.3,\n",
    "                 vocab_size = len(zhen_zh_train.index2word),\n",
    "                 max_sentence_len=50):\n",
    "        \n",
    "        super(CNNencoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.vocab_size = vocab_size\n",
    "        self.stride = stride\n",
    "        self.dropout = nn.Dropout(percent_dropout)\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, \n",
    "                                      self.embedding_size)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(self.embedding_size, self.hidden_size, \n",
    "                               kernel_size=self.kernel_size, padding=self.padding,\n",
    "                               stride=self.stride)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(self.hidden_size, self.hidden_size, \n",
    "                               kernel_size=self.kernel_size, padding=self.padding,\n",
    "                               stride = self.stride)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool_1 = nn.MaxPool1d(3, 1)\n",
    "        self.maxpool_2 = nn.MaxPool1d(5, 2)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "\n",
    "    def forward(self, input_):\n",
    "        \n",
    "        # input size = 1'e uydurmaya calis\n",
    "        \n",
    "        batch_size, seq_len = input_.size()\n",
    "        \n",
    "        embed = self.dropout(self.embedding(input_))\n",
    "        # print (\"embed size = \"+str(embed.size()))\n",
    "        # 32, 350, 300 check\n",
    "        \n",
    "        hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden = self.relu(hidden)\n",
    "        hidden = self.maxpool_1(hidden.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "#         # second conv layer\n",
    "#         hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "#         hidden = self.relu(hidden)\n",
    "#         hidden = self.maxpool_2(hidden.transpose(1,2)).transpose(1,2)\n",
    "\n",
    "        # print (\"hidden size = \"+str(hidden.size()))\n",
    "        hidden = nn.functional.glu(hidden)\n",
    "        \n",
    "        # sum \n",
    "        hidden = torch.mean(hidden, 1).view(batch_size, 1, hidden.size(-1))\n",
    "        # sigmoid\n",
    "        hidden = self.sigmoid(hidden)\n",
    "        \n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNdecoder_CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=len(zhen_en_train_token2id), # for chinese-english's english\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=512,\n",
    "                 num_gru_layers=1,\n",
    "                 max_sentence_len=15):\n",
    "        \n",
    "        super(RNNdecoder_CNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embedding_size\n",
    "        self.dropout = percent_dropout\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_gru_layers\n",
    "        \n",
    "        self.GRU = nn.GRU(self.embed_size, \n",
    "                          self.hidden_size, \n",
    "                          self.num_layers, \n",
    "                          batch_first=True, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "        self.ReLU = nn.ReLU\n",
    "        \n",
    "        self.drop_out_function = nn.Dropout(self.dropout)\n",
    "        \n",
    "        self.embed_target = nn.Embedding(self.vocab_size,\n",
    "                                         self.embed_size, padding_idx=0)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # *2 because we are concating hidden with embedding plus context\n",
    "        self.linear_layer = nn.Linear(self.hidden_size*2, self.vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=0)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, \n",
    "                             batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "    def forward(self,\n",
    "                decoder_hidden, ## decoder_hidden = encoder_hidden at first time_step\n",
    "                input_, # input\n",
    "                target_lengths,\n",
    "                target_mask,\n",
    "                time_step):\n",
    "        \n",
    "        self.input = input_\n",
    "#         print (\"input size = \"+str(self.input.size()))\n",
    "        \n",
    "        sort_original_target = sorted(range(len(target_lengths)), \n",
    "                             key=lambda sentence: -target_lengths[sentence])\n",
    "        unsort_to_original_target = sorted(range(len(target_lengths)), \n",
    "                             key=lambda sentence: sort_original_target[sentence])\n",
    "        \n",
    "        self.input = self.input[sort_original_target]\n",
    "        _target_mask = target_mask[sort_original_target]\n",
    "        target_lengths = target_lengths[sort_original_target]\n",
    "        \n",
    "        # seq_len_target is always 1 in the decoder since we are \n",
    "        # passing the tokens for only 1 time_step at a time\n",
    "        batch_size, seq_len_target = self.input.size()\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        \n",
    "        # hidden => initial hidden will be the same as the context\n",
    "        # vector, which is the hidden_source tensor\n",
    "        # then as we update the hidden state at each time step, this will be \n",
    "        # updated as well\n",
    "        self.hidden = decoder_hidden.view(self.num_layers*self.num_directions,\n",
    "                                          batch_size, self.hidden_size)\n",
    "        \n",
    "        # the following should print (1, 32, 256) for this config\n",
    "        # print (\"self.hidden size = \"+str(self.hidden.size()))\n",
    "        \n",
    "        self.input = self.input.unsqueeze(1)\n",
    "        \n",
    "        embeds_target = self.drop_out_function(self.embed_target(self.input.long())).view(batch_size,\n",
    "                                                                                   seq_len_target,\n",
    "                                                                                   -1)\n",
    "    \n",
    "        embeds_target = target_mask[:,time_step,:].unsqueeze(1)*embeds_target + \\\n",
    "                        (1-_target_mask[:,time_step,:].unsqueeze(1))*embeds_target.clone().detach()\n",
    "\n",
    "\n",
    "        gru_out_target, self.hidden = self.GRU(embeds_target.data.view(batch_size, 1, self.embed_size),\n",
    "                                               self.hidden)\n",
    "        \n",
    "        # ref: pytorch documentation\n",
    "        # hidden source : h_n of shape \n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "        # the following should print (1, 32, 256) for this config\n",
    "        # print (\"hidden size after GRU = \"+str(self.hidden.size()))\n",
    "\n",
    "\n",
    "        hidden = self.hidden.view(self.num_layers, self.num_directions,\n",
    "                                  batch_size, self.hidden_size)\n",
    "        hidden = torch.sum(hidden, dim=0) # we don't divide here, just sum\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            # separate layers\n",
    "            gru_out_target = gru_out_target.contiguous().view(seq_len_target,\n",
    "                                                              batch_size,\n",
    "                                                              self.num_directions,\n",
    "                                                              self.hidden_size)\n",
    "        else:\n",
    "            gru_out_target = gru_out_target\n",
    "        \n",
    "#         print (\"gru out size = \"+str(gru_out_target.size()))\n",
    "        \n",
    "        # sum along sequence\n",
    "        gru_out_target = torch.sum(gru_out_target, dim=1) # we don't divide here, just sum\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            hidden = torch.cat([hidden[:,i,:] for i in range(self.num_directions)], \n",
    "                               dim=0)\n",
    "            gru_out_target = torch.cat([gru_out_target[:,i,:] for i in range(self.num_directions)], \n",
    "                                       dim=1)\n",
    "        else:\n",
    "            hidden = hidden.view(batch_size, \n",
    "                                 self.num_directions, self.hidden_size)\n",
    "            gru_out_target = gru_out_target.view(batch_size,\n",
    "                                                 self.num_directions, self.hidden_size)\n",
    "        \n",
    "        hidden = hidden[unsort_to_original_target] ## back to original indices\n",
    "        gru_out_target = gru_out_target[unsort_to_original_target] ## back to original indices\n",
    "\n",
    "        gru_out_target = self.sigmoid(gru_out_target)\n",
    "        # concating embedding + context = gru_out_target with hidden\n",
    "        out = torch.cat([gru_out_target,hidden], dim=2)\n",
    "        \n",
    "#         print (\"out size after concat = \"+str(out.size()))\n",
    "        \n",
    "        out = self.linear_layer(out)\n",
    "        \n",
    "        # softmax over vocabulary\n",
    "        pred = self.log_softmax(out)\n",
    "\n",
    "        return pred, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chinese -> english\n",
    "enc = CNNencoder(300, # embed size\n",
    "                 1024, # hidden size\n",
    "                 3, # kernel size\n",
    "                 padding = 1,\n",
    "                 stride = 2,\n",
    "                 percent_dropout = 0.3,\n",
    "                 vocab_size = len(zhen_zh_train.index2word),\n",
    "                 max_sentence_len=15)\n",
    "    \n",
    "dec = RNNdecoder_CNN()\n",
    "\n",
    "# train\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "def train(encoder, decoder, loader=zhen_train_loader,\n",
    "          optimizer = torch.optim.Adam([*enc.parameters()] + [*dec.parameters()], lr=1e-4),\n",
    "#           encoder_optimizer = torch.optim.Adam(enc.parameters(), lr=1e-4),\n",
    "#           decoder_optimizer = torch.optim.Adam(dec.parameters(), lr=1e-4),\n",
    "          epoch=None):\n",
    "    \n",
    "#     encoder_optimizer.zero_grad()\n",
    "#     decoder_optimizer.zero_grad()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for batch_idx, (source_sentence, source_mask, source_lengths, \n",
    "                    target_sentence, target_mask, target_lengths)\\\n",
    "                    in enumerate(loader):\n",
    "        \n",
    "        source_sentence, source_mask = source_sentence.to(device), source_mask.to(device) \n",
    "        target_sentence, target_mask = target_sentence.to(device), target_mask.to(device)\n",
    "        \n",
    "        encoder_hidden = encoder(source_sentence)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        # decoder should start with SOS tokens \n",
    "        # ref: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "        input_ = SOS_token*torch.ones(BATCH_SIZE,1).view(-1,1).to(device)\n",
    "        \n",
    "        for t in range(0, target_sentence.size(1)):\n",
    "            \n",
    "            decoder_out, decoder_hidden = decoder(decoder_hidden, # = gru_out_source - instead of encoded_source[0]\n",
    "                                                 input_, # instead of target sentence up to t \n",
    "                                                 target_lengths,  # target lengths\n",
    "                                                 target_mask,\n",
    "                                                 t)\n",
    "            \n",
    "#             print (\"decoder out size = \"+str(decoder_out.size()))\n",
    "            target_tokens = convert_to_softmax(target_sentence[:,t], BATCH_SIZE)\n",
    "            \n",
    "            loss += F.binary_cross_entropy(F.sigmoid(decoder_out), target_tokens)\n",
    "            \n",
    "        print (\"loss = \"+str(loss))\n",
    "        loss.backward(retain_graph = True)\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), 50)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), 50)\n",
    "        optimizer.step()\n",
    "        \n",
    "    torch.save(encoder.state_dict(), \"encoder_state_dict\")\n",
    "    torch.save(decoder.state_dict(), \"decoder_state_dict\")\n",
    "            \n",
    "#             epoch_loss.backward(retain_graph = True) # if necessary call retain_graph = True\n",
    "            \n",
    "            \n",
    "#             encoder_optimizer.step()\n",
    "#             decoder_optimizer.step()\n",
    "            \n",
    "    return epoch_loss/BATCH_SIZE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/derin/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([32, 59325])) that is different to the input size (torch.Size([32, 1, 59325])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = tensor(0.4626, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(0.9252, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(1.3878, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(1.8504, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(2.3130, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(2.7756, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(3.2382, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(3.7007, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(4.1632, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(4.6258, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(5.0883, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(5.5508, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(6.0133, grad_fn=<ThAddBackward>)\n",
      "loss = tensor(6.4758, grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "lr = 1e-4\n",
    "# batch_\n",
    "\n",
    "loss_train = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print (\"epoch = \"+str(epoch))\n",
    "\n",
    "    loss = train(enc, dec,\n",
    "                 loader = zhen_train_loader,\n",
    "                 optimizer = torch.optim.Adam([*enc.parameters()] + [*dec.parameters()], lr=1e-4),\n",
    "                 epoch = epoch)\n",
    "    \n",
    "    loss_train.append(loss)\n",
    "    \n",
    "    print (loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Fully self-attention Translation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Multilingual Translation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
