{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Spring 2018 NLP Class Project: Neural Machine Translation</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pdb\n",
    "import os\n",
    "from underthesea import word_tokenize\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install spacy && python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Project Overview\n",
    "\n",
    "The goal of this project is to build a neural machine translation system and experience how recent advances have made their way. Each team will build the following sequence of neural translation systems for two language pairs, __Vietnamese (Vi)→English (En)__ and __Chinese (Zh)→En__ (prepared corpora is be provided):\n",
    "\n",
    "1. Recurrent neural network based encoder-decoder without attention\n",
    "2. Recurrent neural network based encoder-decoder with attention\n",
    "2. Replace the recurrent encoder with either convolutional or self-attention based encoder.\n",
    "4. [Optional] Build either or both fully self-attention translation system or/and multilingual translation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Upload & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of sentence\n",
    "SOS_token = 1\n",
    "# end of sentence\n",
    "EOS_token = 3\n",
    "\n",
    "## 2 = unk\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"<PAD>\",1: \"<SOS>\", 2:\"<UNK>\", 3: \"<EOS>\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    \"\"\"About \"NFC\" and \"NFD\": \n",
    "    \n",
    "    For each character, there are two normal forms: normal form C \n",
    "    and normal form D. Normal form D (NFD) is also known as canonical \n",
    "    decomposition, and translates each character into its decomposed form. \n",
    "    Normal form C (NFC) first applies a canonical decomposition, then composes \n",
    "    pre-combined characters again.\n",
    "    \n",
    "    About unicodedata.category: \n",
    "    \n",
    "    Returns the general category assigned to the Unicode character \n",
    "    unichr as string.\"\"\"\n",
    "    \n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Trim\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False,\n",
    "             dataset=\"train\"):\n",
    "    \n",
    "    \"\"\"Takes as input;\n",
    "    - lang1, lang2: either (vi, en) or (zh, en)\n",
    "    - dataset: one of (\"train\",\"dev\",\"test\")\"\"\"\n",
    "    print(\"Reading lines...\")\n",
    "    eos = [\".\",\"?\",\"!\",\"\\n\"]\n",
    "    # Read the pretokenized lang1 file and split into lines\n",
    "    lang1_lines = open(\"../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-%s-%s-processed/%s.tok.%s\" % (lang1, lang2, dataset, lang1), encoding=\"utf-8\").\\\n",
    "        read().strip().split(\"\\n\")\n",
    "    # Read the lang2 file and split into lines\n",
    "    lang2_lines = open(\"../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-%s-%s-processed/%s.tok.%s\" % (lang1, lang2, dataset, lang2), encoding=\"utf-8\").\\\n",
    "        read().strip().split(\"\\n\")\n",
    "    \n",
    "    # create sentence pairs (lists of length 2 that consist of string pairs)\n",
    "    # e.g. [\"And we &apos;re going to tell you some stories from the sea here in video .\",\n",
    "    #       \"我们 将 用 一些 影片 来讲 讲述 一些 深海 海里 的 故事  \"]\n",
    "    # check if there are the same number of sentences in each set\n",
    "    assert len(lang1_lines) == len(lang2_lines), \"Two languages must have the same number of sentences. \"+ str(len(lang1_lines)) + \" sentences were passed for \" + str(lang1) + \".\" + str(len(lang2_lines)) + \" sentences were passed for \" + str(lang2)+\".\"\n",
    "    # normalize if not Chinese, Chinese normalization is already handeled\n",
    "    if lang1 == \"zh\":\n",
    "        lang1_lines = lang1_lines\n",
    "    else:\n",
    "        lang1_lines = [normalizeString(s) for s in lang1_lines]\n",
    "    lang2_lines = [normalizeString(s) for s in lang2_lines]\n",
    "    # construct pairs\n",
    "    pair_ran = range(len(lang1_lines))\n",
    "    pairs = [[lang1_lines[i]] + [lang2_lines[i]] for i in pair_ran]\n",
    "    \n",
    "#     # Split every line into pairs and normalize\n",
    "#     pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 133317 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 16144\n",
      "en 47568\n",
      "['Tac pham nghe thuat tiep theo la Con chim moi .', 'The next work is Decoy .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False, dataset=\"train\"):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse, dataset=dataset)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# example\n",
    "input_lang, output_lang, pairs = prepareData('vi', 'en', False, dataset=\"train\")\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213376 sentence pairs\n",
      "Trimmed to 213376 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 88917\n",
      "en 59329\n",
      "['如果 比较 二者 的 特征 会 发现   享乐 视 时间   对象   地点 而 定  ', 'But if you look at the characteristics of those two pleasure is contingent upon time upon its object upon the place .']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('zh', 'en', False, dataset=\"train\")\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vietnamese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Please find the original tokenizing code provided by Elman Mansimov in the following link:\n",
    "# # https://github.com/derincen/neural-machine-translation/tree/master/data/tokens_and_preprocessing_em/preprocess_translation\n",
    "\n",
    "# def tokenize_vi(f_names, f_out_names):\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             tok_lines.write(word_tokenize(sentence, format=\"text\") + '\\n')\n",
    "#         tok_lines.close()\n",
    "\n",
    "# def tokenize_en(f_names, f_out_names):\n",
    "#     tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             # replaced tokenizer(sentence) with str(tokenizer(sentence)) to avoid \n",
    "#             # type error while joining\n",
    "#             tok_lines.write(' '.join(str(tokenizer(sentence))) + '\\n')\n",
    "#         tok_lines.close()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     root = '../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-vi-en-processed/'\n",
    "#     tokenize_vi([os.path.join(root, 'train.vi'), os.path.join(root, 'dev.vi'), \n",
    "#                  os.path.join(root, 'test.vi')],\\\n",
    "#                [os.path.join(root, 'train.tok.vi'), os.path.join(root, 'dev.tok.vi'), \n",
    "#                 os.path.join(root, 'test.tok.vi')])\n",
    "\n",
    "#     tokenize_en([os.path.join(root, 'train.en'), os.path.join(root, 'dev.en'), \n",
    "#                  os.path.join(root, 'test.en')],\\\n",
    "#                 [os.path.join(root, 'train.tok.en'), os.path.join(root, 'dev.tok.en'), \n",
    "#                  os.path.join(root, 'test.tok.en')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 133317 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 16144\n",
      "en 47568\n",
      "Reading lines...\n",
      "Read 1268 sentence pairs\n",
      "Trimmed to 1268 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 1370\n",
      "en 3816\n",
      "Reading lines...\n",
      "Read 1553 sentence pairs\n",
      "Trimmed to 1553 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 1325\n",
      "en 3619\n"
     ]
    }
   ],
   "source": [
    "# Format: languagepair_language_dataset\n",
    "# Train \n",
    "vien_vi_train, vien_en_train, vi_en_train_pairs = prepareData('vi', 'en', False, dataset=\"train\")\n",
    "# Dev \n",
    "vien_vi_dev, vien_en_dev, vi_en_dev_pairs = prepareData('vi', 'en', False, dataset=\"dev\")\n",
    "# Test\n",
    "vien_vi_test, vien_en_test, vi_en_test_pairs = prepareData('vi', 'en', False, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Chinese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Please find the original tokenizing code provided by Elman Mansimov in the following link:\n",
    "# # https://github.com/derincen/neural-machine-translation/tree/master/data/tokens_and_preprocessing_em/preprocess_translation\n",
    "\n",
    "# def tokenize_zh(f_names, f_out_names):\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             tok_lines.write(' '.join(jieba.cut(sentence, cut_all=True)))\n",
    "#         tok_lines.close()\n",
    "\n",
    "# def tokenize_en(f_names, f_out_names):\n",
    "#     tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             # replaced tokenizer(sentence) with str(tokenizer(sentence)) to avoid \n",
    "#             # type error while joining\n",
    "#             tok_lines.write(' '.join(str(tokenizer(sentence))) + '\\n')\n",
    "#         tok_lines.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     root = '../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-zh-en-processed/'\n",
    "#     tokenize_zh([os.path.join(root, 'dev.zh'), os.path.join(root, 'test.zh'), os.path.join(root, 'train.zh')],\\\n",
    "#                 [os.path.join(root, 'dev.tok.zh'), os.path.join(root, 'test.tok.zh'), os.path.join(root, 'train.tok.zh')])\n",
    "\n",
    "# #     tokenize_en([os.path.join(root, 'dev.en'), os.path.join(root, 'test.en'), os.path.join(root, 'train.en')],\\\n",
    "# #                [os.path.join(root, 'dev.tok.en'), os.path.join(root, 'test.tok.en'), os.path.join(root, 'train.tok.en')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213376 sentence pairs\n",
      "Trimmed to 213376 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 88917\n",
      "en 59329\n",
      "Reading lines...\n",
      "Read 1261 sentence pairs\n",
      "Trimmed to 1261 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 6132\n",
      "en 3916\n",
      "Reading lines...\n",
      "Read 1397 sentence pairs\n",
      "Trimmed to 1397 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 5214\n",
      "en 3423\n"
     ]
    }
   ],
   "source": [
    "# Format: languagepair_language_dataset\n",
    "# Train \n",
    "zhen_zh_train, zhen_en_train, zh_en_train_pairs = prepareData('zh', 'en', False, dataset=\"train\")\n",
    "# Dev \n",
    "zhen_zh_dev, zhen_en_dev, zh_en_dev_pairs = prepareData('zh', 'en', False, dataset=\"dev\")\n",
    "# Test\n",
    "zhen_zh_test, zhen_en_test, zh_en_test_pairs = prepareData('zh', 'en', False, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我们 将 用 一些 影片 来讲 讲述 一些 深海 海里 的 故事  ',\n",
       " 'And we apos re going to tell you some stories from the sea here in video .']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_en_train_pairs[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Check Source & Target Vocabs\n",
    "\n",
    "Since the source and target languages can have very different table lookup layers, it's good practice to have separate vocabularies for each. Thus, we build vocabularies for each language that we will be using. \n",
    "\n",
    "In the first class (Lang) of this section, we have already defined vocabularies for all languages. So, there is no need to redefine another function. We chech each vocabulary below.\n",
    "\n",
    "#### Chinese Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in Chinese training corpus is 88917\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in Chinese training corpus is \" + str(zhen_zh_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10479"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_zh_train.word2index[\"格\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'格'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_zh_train.index2word[10479]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vietnamese Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in Vietnamese training corpus is 16144\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in Vietnamese training corpus is \" + str(vien_vi_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6752"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_vi_train.word2index[\"Hamburger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hamburger'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_vi_train.index2word[6752]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Vocabulary for Zh-En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in English training corpus for Zh-En is 59329\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in English training corpus for Zh-En is \" + str(zhen_en_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1451"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_en_train.word2index[\"translate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'translate'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_en_train.index2word[1451]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Vocabulary for Vi-En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in English training corpus for Vi-En is 47568\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in English training corpus for Vi-En is \" + str(vien_en_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "847"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_train.word2index[\"machine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_train.index2word[847]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Prepare Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "SOS_IDX = 1\n",
    "UNK_IDX = 2\n",
    "EOS_IDX = 3\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(paired_tokens, \n",
    "                        lang1_token2id_vocab,\n",
    "                        lang2_token2id_vocab):\n",
    "    \"\"\"Takes as input:\n",
    "    - paired_tokens: a list of sentence pairs that consist of source & target lang sentences.\n",
    "    - lang1_token2id_vocab: token2index vocabulary for the first language. \n",
    "                            Get by method Lang_dataset.word2index\n",
    "    - lang2_token2id_vocab: token2index vocabulary for the second language. \n",
    "                            Get by method Lang_dataset.word2index\n",
    "                            \n",
    "    Returns:\n",
    "    - indices_data_lang_1, indices_data_lang2: A list of lists where each sub-list holds corresponding indices for each\n",
    "                                               token in the sentence.\"\"\"\n",
    "    indices_data_lang_1, indices_data_lang_2 = [], []\n",
    "    vocabs = [lang1_token2id_vocab, lang2_token2id_vocab]\n",
    "    \n",
    "    # lang1\n",
    "    for t in range(len(paired_tokens)):\n",
    "        index_list = [SOS_IDX] + [vocabs[0][token] if token in vocabs[0]\\\n",
    "                                    else UNK_IDX for token in paired_tokens[t][0]] + [EOS_IDX]\n",
    "        indices_data_lang_1.append(index_list)\n",
    "    # lang2\n",
    "    for t in range(len(paired_tokens)):\n",
    "        index_list = [SOS_IDX] + [vocabs[1][token] if token in vocabs[1] \\\n",
    "                                    else UNK_IDX for token in paired_tokens[t][1]] + [EOS_IDX]\n",
    "        indices_data_lang_2.append(index_list)\n",
    "        \n",
    "    return indices_data_lang_1, indices_data_lang_2\n",
    "\n",
    "# train indices\n",
    "zhen_zh_train_indices, zhen_en_train_indices = token2index_dataset(zh_en_train_pairs,\n",
    "                                                                   zhen_zh_train.word2index,\n",
    "                                                                   zhen_en_train.word2index)\n",
    "\n",
    "vien_vi_train_indices, vien_en_train_indices = token2index_dataset(vi_en_train_pairs,\n",
    "                                                                   vien_vi_train.word2index,\n",
    "                                                                   vien_en_train.word2index)\n",
    "\n",
    "# dev indices\n",
    "zhen_zh_dev_indices, zhen_en_dev_indices = token2index_dataset(zh_en_dev_pairs,\n",
    "                                                               zhen_zh_dev.word2index,\n",
    "                                                               zhen_en_dev.word2index)\n",
    "\n",
    "vien_vi_dev_indices, vien_en_dev_indices = token2index_dataset(vi_en_dev_pairs,\n",
    "                                                               vien_vi_dev.word2index,\n",
    "                                                               vien_en_dev.word2index)\n",
    "\n",
    "# test indices\n",
    "zhen_zh_test_indices, zhen_en_test_indices = token2index_dataset(zh_en_test_pairs,\n",
    "                                                                 zhen_zh_test.word2index,\n",
    "                                                                 zhen_en_test.word2index)\n",
    "\n",
    "vien_vi_test_indices, vien_en_test_indices = token2index_dataset(vi_en_test_pairs,\n",
    "                                                                 vien_vi_test.word2index,\n",
    "                                                                 vien_en_test.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EOS_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese training set length = 213376\n",
      "Chinese-English (En) training set length = 213376\n",
      "\n",
      "Vietnamese training set length = 133317\n",
      "Vietnamese-English (En) training set length = 133317\n",
      "\n",
      "Chinese dev set length = 1261\n",
      "Chinese-English (En) dev set length = 1261\n",
      "\n",
      "Vietnamese dev set length = 1268\n",
      "Vietnamese-English (En) dev set length = 1268\n",
      "\n",
      "Chinese test set length = 1397\n",
      "Chinese-English (En) test set length = 1397\n",
      "\n",
      "Vietnamese test set length = 1553\n",
      "Vietnamese-English (En) test set length = 1553\n"
     ]
    }
   ],
   "source": [
    "# check length\n",
    "# train\n",
    "print (\"Chinese training set length = \"+str(len(zhen_zh_train_indices)))\n",
    "print (\"Chinese-English (En) training set length = \"+str(len(zhen_en_train_indices)))\n",
    "print (\"\\nVietnamese training set length = \"+str(len(vien_vi_train_indices)))\n",
    "print (\"Vietnamese-English (En) training set length = \"+str(len(vien_en_train_indices)))\n",
    "# dev\n",
    "print (\"\\nChinese dev set length = \"+str(len(zhen_zh_dev_indices)))\n",
    "print (\"Chinese-English (En) dev set length = \"+str(len(zhen_en_dev_indices)))\n",
    "print (\"\\nVietnamese dev set length = \"+str(len(vien_vi_dev_indices)))\n",
    "print (\"Vietnamese-English (En) dev set length = \"+str(len(vien_en_dev_indices)))\n",
    "# test\n",
    "print (\"\\nChinese test set length = \"+str(len(zhen_zh_test_indices)))\n",
    "print (\"Chinese-English (En) test set length = \"+str(len(zhen_en_test_indices)))\n",
    "print (\"\\nVietnamese test set length = \"+str(len(vien_vi_test_indices)))\n",
    "print (\"Vietnamese-English (En) test set length = \"+str(len(vien_en_test_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNK_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88915"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zhen_zh_train.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO \n",
    "\n",
    "MAX_SENTENCE_LENGTH = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# zhen token2index vocabs\n",
    "zhen_zh_train_token2id = zhen_zh_train.word2index\n",
    "zhen_en_train_token2id = zhen_en_train.word2index\n",
    "\n",
    "# vien token2index vocabs\n",
    "vien_vi_train_token2id = vien_vi_train.word2index\n",
    "vien_en_train_token2id = vien_en_train.word2index\n",
    "\n",
    "class TranslationDataset():\n",
    "    \"\"\"\n",
    "    Class that represents a train/dev/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 data_source, # training indices data of the source language\n",
    "                 data_target, # training indices data of the target language\n",
    "                 token2id_source=None, # token2id dict of the source language\n",
    "                 token2id_target=None  # token2id dict of the target language\n",
    "                ):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.source_sentences, self.target_sentences =  data_source, data_target\n",
    "        \n",
    "        self.token2id_source = token2id_source\n",
    "        self.token2id_target = token2id_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def __getitem__(self, batch_index):\n",
    "\n",
    "#         source_word_idx, target_word_idx = [], []\n",
    "        source_mask, target_mask = [], []\n",
    "        \n",
    "        for index in self.source_sentences[batch_index][:MAX_SENTENCE_LENGTH]:\n",
    "            if index != UNK_IDX:\n",
    "                source_mask.append(0)\n",
    "            else:\n",
    "                source_mask.append(1)\n",
    "                \n",
    "        for index in self.target_sentences[batch_index][:MAX_SENTENCE_LENGTH]:\n",
    "            if index != UNK_IDX:\n",
    "                target_mask.append(0)\n",
    "            else:\n",
    "                target_mask.append(1)\n",
    "        \n",
    "        source_indices = [SOS_token] + self.source_sentences[batch_index][:MAX_SENTENCE_LENGTH]\n",
    "        target_indices = [SOS_token] + self.target_sentences[batch_index][:MAX_SENTENCE_LENGTH]\n",
    "        \n",
    "        source_list = [source_indices, source_mask, len(source_indices)]\n",
    "        target_list = [target_indices, target_mask, len(target_indices)]\n",
    "        \n",
    "        return source_list + target_list\n",
    "\n",
    "    \n",
    "def translation_collate(batch, max_sentence_length):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    source_data, target_data = [], []\n",
    "    source_mask, target_mask = [], []\n",
    "    source_lengths, target_lengths = [], []\n",
    "\n",
    "    for datum in batch:\n",
    "        source_lengths.append(datum[2])\n",
    "        target_lengths.append(datum[5])\n",
    "        \n",
    "        # PAD\n",
    "        source_data_padded = np.pad(np.array(datum[0]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        source_data.append(source_data_padded)\n",
    "        \n",
    "        source_mask_padded = np.pad(np.array(datum[1]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        source_mask.append(source_mask_padded)\n",
    "        \n",
    "        target_data_padded = np.pad(np.array(datum[3]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        target_data.append(target_data_padded)\n",
    "        \n",
    "        target_mask_padded = np.pad(np.array(datum[4]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])),\n",
    "                               mode=\"constant\", constant_values=0)\n",
    "        target_mask.append(target_mask_padded)\n",
    "        \n",
    "    ind_dec_order = np.argsort(source_lengths)[::-1]\n",
    "    source_data = np.array(source_data)[ind_dec_order]\n",
    "    target_data = np.array(target_data)[ind_dec_order]\n",
    "    source_mask = np.array(source_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    target_mask = np.array(target_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    source_lengths = np.array(source_lengths)[ind_dec_order]\n",
    "    target_lengths = np.array(target_lengths)[ind_dec_order]\n",
    "    \n",
    "    source_list = [torch.from_numpy(source_data), \n",
    "               torch.from_numpy(source_mask).float(), source_lengths]\n",
    "    target_list = [torch.from_numpy(target_data), \n",
    "               torch.from_numpy(target_mask).float(), target_lengths]\n",
    "        \n",
    "    return source_list + target_list\n",
    "\n",
    "\n",
    "zhen_train_dataset = TranslationDataset(zhen_zh_train_indices,\n",
    "                                       zhen_en_train_indices,\n",
    "                                       token2id_source=zhen_zh_train_token2id,\n",
    "                                       token2id_target=zhen_en_train_token2id)\n",
    "\n",
    "zhen_train_loader = torch.utils.data.DataLoader(dataset=zhen_train_dataset,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                               shuffle=False)\n",
    "\n",
    "zhen_dev_dataset = TranslationDataset(zhen_zh_dev_indices,\n",
    "                                       zhen_en_dev_indices,\n",
    "                                       token2id_source=zhen_zh_train_token2id,\n",
    "                                       token2id_target=zhen_en_train_token2id)\n",
    "\n",
    "zhen_dev_loader = torch.utils.data.DataLoader(dataset=zhen_dev_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                             shuffle=False)\n",
    "\n",
    "vien_train_dataset = TranslationDataset(vien_vi_train_indices,\n",
    "                                       vien_en_train_indices,\n",
    "                                       token2id_source=vien_vi_train_token2id,\n",
    "                                       token2id_target=vien_en_train_token2id)\n",
    "\n",
    "vien_train_loader = torch.utils.data.DataLoader(dataset=vien_train_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                             shuffle=False)\n",
    "\n",
    "vien_dev_dataset = TranslationDataset(vien_vi_dev_indices,\n",
    "                                       vien_en_dev_indices,\n",
    "                                       token2id_source=vien_vi_train_token2id,\n",
    "                                       token2id_target=vien_en_train_token2id)\n",
    "\n",
    "vien_dev_loader = torch.utils.data.DataLoader(dataset=vien_dev_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, MAX_SENTENCE_LENGTH),\n",
    "                             shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [*vien_dev_loader][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Evaluation Metric\n",
    "\n",
    "We use BLEU as the evaluation metric. Specifically, we focus on the corpus-level BLEU function. \n",
    "\n",
    "The code for BLEU is taken from https://github.com/mjpost/sacreBLEU/blob/master/sacrebleu.py#L1022-L1080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /Users/derin/anaconda/lib/python3.6/site-packages (1.2.12)\n",
      "Requirement already satisfied: typing in /Users/derin/anaconda/lib/python3.6/site-packages (from sacrebleu) (3.6.6)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sacrebleu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a42160e8fb94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msacrebleu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sacrebleu'"
     ]
    }
   ],
   "source": [
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Beam Search Algorithm\n",
    "\n",
    "In this section, we implement the Beam Search algorithm in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize k-many score lists\n",
    "# start only with the whole x\n",
    "# initialize k-many prev y's lists\n",
    "# choose top-k for y1 from the whole vocab\n",
    "# choose top-k for the second time step by expanding the first time step\n",
    "# compute scores by adding log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_size_k = 10\n",
    "\n",
    "# class BeamSearch:\n",
    "    \n",
    "#     \"\"\"RECURSE\"\"\"\n",
    "    \n",
    "#     def __init__(self,\n",
    "#                  beam_size=beam_size_k, ## insert num \n",
    "#                  softmax_out\n",
    "#                 ):\n",
    "#         \"\"\"\n",
    "#         Class that holds beam information, and search & score functions\n",
    "#         - beam_size = beam size\n",
    "#         - softmax_out = the softmax over the vocabulary at time step t, as computed by the RNN decoder,\n",
    "#                         given the source sequence X and the previously decoded y_<t tokens.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         self.beam_size = beam_size\n",
    "#         self.softmax_out = softmax_out\n",
    "        \n",
    "#         # initialize paths\n",
    "#         self.paths = np.empty((self.beam_size))\n",
    "        \n",
    "#         # initialize the dictionary that will hold the path scores \n",
    "#         # and update the scores at each time step\n",
    "#         self.path_score_dict = {}\n",
    "#         # we will later use each i < k as a key and populate this\n",
    "#         # dict with scores\n",
    "\n",
    "        \n",
    "#     def search():\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "#     def score(prev_ys = None):\n",
    "#         \"\"\"- prev_ys = previously decoded tokens (previously generated target language tokens)\n",
    "#         \"\"\"\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model\n",
    "\n",
    "1. Recurrent neural network based encoder-decoder without attention\n",
    "2. Recurrent neural network based encoder-decoder with attention\n",
    "2. Replace the recurrent encoder with either convolutional or self-attention based encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction loss = binary cross entropy between two (vocab_size x 1) vectors\n",
    "# used during training, since we can compare the real Y and and the generated Y\n",
    "# still at each time step of the decoder, we compare up to and including\n",
    "# the real t-th token and the generated t-th, then optimize\n",
    "\n",
    "def loss_function(y_hat, y):\n",
    "    \n",
    "    \"\"\"Takes as input;\n",
    "    - y: correct \"log-softmax\"(binary vector) that represents the correct t-th token in the target sentence,\n",
    "                 (vocab_size x 1) vector\n",
    "    - y_hat: predicted LogSoftmax for the predicted t-th token in the target sentence.\n",
    "             (vocab_size x 1) vector\n",
    "    Returns;\n",
    "    - NLL Loss in training time\"\"\"\n",
    "#     y_hat = torch.log(y_hat) # log softmax\n",
    "    loss = nn.functional.binary_cross_entropy(y_hat,y)\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "# generation/inference time - validation loss = BLEU\n",
    "\n",
    "def compute_BLEU(corpus_hat,corpus):\n",
    "    ## TODO\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_PATH_LENGTH = 400 # make changeable later !!!\n",
    "\n",
    "# class TargetOut:\n",
    "#     def __init__(self,\n",
    "#                  beam_size=5,\n",
    "#                  source_sentence_length=400,\n",
    "#                  time_step=0):\n",
    "#         \"\"\"\n",
    "#         - beam: the tensor that will be populated with beam_size-many paths in each timestep\n",
    "#         - beam_size: the width of the beam, top k tokens to include in the beam search,\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # initialized again for each timestep\n",
    "#         self.beam = torch.empty(beam_size)\n",
    "#         self.beam_size = beam_size\n",
    "#         self.beam_seq = beam_seq\n",
    "#         self.time_step = time_step\n",
    "        \n",
    "#         self.max_target_length = source_sentence_length*(1.5)\n",
    "#         # path is kept by hold_path\n",
    "#         self.path = torch.empty(beam_size, max_target_length)\n",
    "    \n",
    "#     def _add_and_score_paths(self, \n",
    "#              top_k_tokens):\n",
    "        \n",
    "#         \"\"\"top_k_tokens: torch.FloatTensor of indices according to logSoftmax \n",
    "#         (not embeddings - embedding matrix indices or vocab indices)\"\"\"\n",
    "        \n",
    "#         time_step = self.time_step\n",
    "#         self.path[:,time_step] = top_k_tokens\n",
    "        \n",
    "#         return self\n",
    "            \n",
    "#     def _score_paths(self,gru_out):\n",
    "        \n",
    "#         \"\"\"For each path, computes log(P(Y_i|Y_i-1,..,Y_i-n,X)) + log(P(Y_i-1|Y_i-2,..,Y_i-n,X)) + ...\n",
    "#         -gru_out is a softmax over the vocabulary for each timestep, so \n",
    "#         we need to take its log to obtain the scores\"\"\"\n",
    "#         if self.time_step = 0:\n",
    "            \n",
    "        \n",
    "    \n",
    "#     def _hold_path_score(self):\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: RNN-based Encoder-Decoder without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.FloatTensor([2]).view(-1,1),torch.FloatTensor([3]).view(-1,1)],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ReLU([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "class RNNencoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=len(zhen_zh_train_token2id), # for chinese\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=2):\n",
    "        \n",
    "        super(RNNencoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embedding_size\n",
    "        self.dropout = percent_dropout\n",
    "        self.embed_source = nn.Embedding(self.vocab_size,\n",
    "                                         self.embed_size,\n",
    "                                         padding_idx=0\n",
    "                                        )\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_gru_layers\n",
    "        \n",
    "        self.GRU = nn.GRU(self.embed_size, \n",
    "                          self.hidden_size, \n",
    "                          self.num_layers, \n",
    "                          batch_first=True, bidirectional=False)\n",
    "        \n",
    "        self.drop_out_function = nn.Dropout(self.dropout)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden_ = torch.zeros(self.num_layers, ## 2 for bidirectional\n",
    "                             batch_size, self.hidden_size).to(device)\n",
    "        return hidden_\n",
    "\n",
    "    def forward(self, source_sentence, source_mask, source_lengths):\n",
    "        \"\"\"Returns source lengths to feed into the decoder, since we do not want\n",
    "        the translation length to be above/below a certain treshold*source sentence length.\"\"\"\n",
    "        \n",
    "        sort_original_source = sorted(range(len(source_lengths)), \n",
    "                             key=lambda sentence: -source_lengths[sentence])\n",
    "        unsort_to_original_source = sorted(range(len(source_lengths)), \n",
    "                             key=lambda sentence: sort_original_source[sentence])\n",
    "        \n",
    "        source_sentence = source_sentence[sort_original_source]\n",
    "        _source_mask = source_mask[sort_original_source]\n",
    "        source_lengths = source_lengths[sort_original_source]\n",
    "        batch_size, seq_len_source = source_sentence.size()\n",
    "        # init hidden\n",
    "        self.hidden_source = self.init_hidden(batch_size)\n",
    "        embeds_source = self.embed_source(source_sentence)\n",
    "        embeds_source = source_mask*embeds_source + (1-_source_mask)*embeds_source.clone().detach()\n",
    "        \n",
    "        embeds_source = torch.nn.utils.rnn.pack_padded_sequence(embeds_source, \n",
    "                                                                source_lengths, \n",
    "                                                                batch_first=True)\n",
    "        \n",
    "        gru_out_source, self.hidden_source = self.GRU(embeds_source, self.hidden_source)\n",
    "        \n",
    "        # ref: pytorch documentation\n",
    "        # hidden source : h_n of shape \n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "#         print (\"hidden source size = \"+str(self.hidden_source.size()))\n",
    "        \n",
    "        # ref: pytorch documentation\n",
    "        # Like output, the layers can be separated using \n",
    "        # h_n.view(num_layers, num_directions, batch_size, hidden_size)\n",
    "        if self.GRU.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "            \n",
    "        hidden_source = self.hidden_source.view(self.num_layers, self.num_directions, \n",
    "                                                batch_size, self.hidden_size)\n",
    "        \n",
    "        # the following should print (2, 1, 32, 256) for this config\n",
    "#         print (\"hidden source size after view = \"+str(hidden_source.size()))\n",
    "        \n",
    "        # get the mean along 0th axis (over layers)\n",
    "        hidden_source = torch.mean(hidden_source, dim=0) ## mean instead of sum for source representation as suggested in the class\n",
    "        # the following should print (1, 32, 256)\n",
    "#         print (\"hidden source size after mean = \"+str(hidden_source.size()))\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            hidden_source = torch.cat([hidden_source[:,i,:] for i in range(self.num_directions)], dim=1)\n",
    "        else:\n",
    "            hidden_source = hidden_source\n",
    "            \n",
    "        # view before unsort\n",
    "        hidden_source = hidden_source.view(batch_size, self.hidden_size)\n",
    "        \n",
    "        # the following should print (32, 256)\n",
    "#         print(\"hidden source size before unsort = \"+str(hidden_source.size()))\n",
    "        hidden_source = hidden_source[unsort_to_original_source] ## back to original indices\n",
    "#         print (\"ENCODER hidden source size after unsort = \"+str(hidden_source.size()))\n",
    "        \n",
    "        gru_out_source, _ = torch.nn.utils.rnn.pad_packed_sequence(gru_out_source,\n",
    "                                                                  batch_first=True)\n",
    "        \n",
    "        gru_out_source = gru_out_source.view(batch_size, -1, 2, self.hidden_size)\n",
    "        gru_out_source = torch.mean(gru_out_source, dim=1) ## mean instead of sum for source representation as suggested in the class\n",
    "        gru_out_source = torch.cat([gru_out_source[:,i,:] for i in range(2)], dim=1)\n",
    "        gru_out_source = gru_out_source[unsort_to_original_source] ## back to original indices\n",
    "        \n",
    "        source_lengths = source_lengths[unsort_to_original_source]\n",
    "\n",
    "        return hidden_source, source_lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "class RNNdecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=len(zhen_en_train_token2id), # for chinese-english's english\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=1,\n",
    "                 max_sentence_len=300):\n",
    "        \n",
    "        super(RNNdecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embedding_size\n",
    "        self.dropout = percent_dropout\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_gru_layers\n",
    "        \n",
    "        self.GRU = nn.GRU(self.embed_size, \n",
    "                          self.hidden_size, \n",
    "                          self.num_layers, \n",
    "                          batch_first=True, bidirectional=False)\n",
    "        \n",
    "        self.ReLU = nn.ReLU\n",
    "        \n",
    "        self.drop_out_function = nn.Dropout(self.dropout)\n",
    "        \n",
    "        self.embed_target = nn.Embedding(self.vocab_size,\n",
    "                                         self.embed_size, padding_idx=0)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # *2 because we are concating hidden with embedding plus context\n",
    "        self.linear_layer = nn.Linear(self.hidden_size*2, self.vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=0)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, \n",
    "                             batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "    def forward(self,\n",
    "                hidden_source, ## context \n",
    "                previously_decoded_upto_t, # input\n",
    "                hidden, # initial hidden (will pass hidden source - same as context)\n",
    "                target_lengths,\n",
    "                target_mask,\n",
    "                time_step):\n",
    "        \n",
    "        # input = previously decoded y tokens = <SOS> token for first token\n",
    "        self.input = previously_decoded_upto_t\n",
    "\n",
    "        sort_original_target = sorted(range(len(target_lengths)), \n",
    "                             key=lambda sentence: -target_lengths[sentence])\n",
    "        unsort_to_original_target = sorted(range(len(target_lengths)), \n",
    "                             key=lambda sentence: sort_original_target[sentence])\n",
    "\n",
    "        self.input = self.input[sort_original_target]\n",
    "        _target_mask = target_mask[sort_original_target]\n",
    "        target_lengths = target_lengths[sort_original_target]\n",
    "        batch_size, seq_len_target = self.input.size()\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        \n",
    "        # context = hidden_source\n",
    "        self.context = hidden_source.view(batch_size, \n",
    "                                          self.num_directions,\n",
    "                                          self.hidden_size)\n",
    "            \n",
    "        # hidden => initial hidden will be the same as the context\n",
    "        # vector, which is the hidden_source tensor\n",
    "        self.hidden = hidden.view(self.num_layers*self.num_directions,\n",
    "                                  batch_size, self.hidden_size)\n",
    "        \n",
    "        # embed target\n",
    "        embeds_target = self.drop_out_function(self.embed_target(self.input.long()))\n",
    "\n",
    "        embeds_target = target_mask*embeds_target + (1-_target_mask)*embeds_target.clone().detach()\n",
    "#         print (\"------------------------\")\n",
    "#         print (\"embedding size before concat = \"+str(embeds_target.size()))\n",
    "        \n",
    "        self.context = self.context.repeat(1,self.embed_size,1)\n",
    "#         print (\"context vector size before concat = \"+str(self.context.size()))\n",
    "#         print (\"input size before concat = \"+str(self.input.size()))\n",
    "        concat_embed = torch.cat([embeds_target, self.context], dim=2)\n",
    "#         print (\"concat embed size = \"+str(concat_embed.size()))\n",
    "        \n",
    "        embeds_target = torch.nn.utils.rnn.pack_padded_sequence(embeds_target,\n",
    "                                                                target_lengths,\n",
    "                                                                batch_first=True)\n",
    "        \n",
    "        \n",
    "        gru_out_target, self.hidden = self.GRU(embeds_target, self.hidden)\n",
    "        \n",
    "        # ref: pytorch documentation\n",
    "        # hidden source : h_n of shape \n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "        # the following should print (1, 32, 256) for this config\n",
    "#         print (\"hidden size after GRU = \"+str(self.hidden.size()))\n",
    "        \n",
    "        # undo packing\n",
    "        gru_out_target, _ = torch.nn.utils.rnn.pad_packed_sequence(gru_out_target,\n",
    "                                                                   batch_first=True)\n",
    "        \n",
    "#         print (\"gru out size after GRU = \"+str(gru_out_target.size()))\n",
    "        \n",
    "        hidden = self.hidden.view(self.num_layers, self.num_directions,\n",
    "                                  batch_size, self.hidden_size)\n",
    "        \n",
    "        gru_out_target = gru_out_target.contiguous().view(self.max_sentence_len, \n",
    "                                             batch_size, \n",
    "                                             self.num_directions, \n",
    "                                             self.hidden_size)\n",
    "        \n",
    "        # the following should print (1, 1, 32, 256) for this config\n",
    "#         print (\"hidden size after view = \"+str(hidden.size()))\n",
    "        # the following should print (300, 32, 1, 256)\n",
    "#         print (\"gru out target size after view = \"+str(gru_out_target.size()))\n",
    "        \n",
    "        hidden = torch.sum(hidden, dim=0) # we don't divide here, just sum\n",
    "        # sum along sequence\n",
    "        gru_out_target = torch.sum(gru_out_target, dim=0) # we don't divide here, just sum\n",
    "        \n",
    "        # the following should print (32, 1, 256)\n",
    "#         print (\"gru out target size after sum = \"+ str(gru_out_target.size()))\n",
    "        # the following should print (1, 32, 256)\n",
    "#         print (\"hidden size after sum = \"+str(hidden.size()))\n",
    "        \n",
    "        if self.GRU.bidirectional:\n",
    "            hidden = torch.cat([hidden[:,i,:] for i in range(self.num_directions)], \n",
    "                               dim=0)\n",
    "            gru_out_target = torch.cat([gru_out_target[:,i,:] for i in range(self.num_directions)], \n",
    "                                       dim=1)\n",
    "        else:\n",
    "            hidden = hidden.view(batch_size, \n",
    "                                 self.num_directions, self.hidden_size)\n",
    "            gru_out_target = gru_out_target.view(batch_size,\n",
    "                                                 self.num_directions, self.hidden_size)\n",
    "        \n",
    "        # the following two should both print (32, 256)\n",
    "#         print (\"hidden size before unsort = \"+str(hidden.size()))\n",
    "#         print (\"gru out target size before unsort = \"+str(gru_out_target.size()))\n",
    "        \n",
    "        hidden = hidden[unsort_to_original_target] ## back to original indices\n",
    "        gru_out_target = gru_out_target[unsort_to_original_target] ## back to original indices\n",
    "\n",
    "        # the following two should both print (32, 256)\n",
    "#         print (\"hidden size after unsort = \"+str(hidden.size()))\n",
    "#         print (\"gru out target size after unsort = \"+str(gru_out_target.size()))\n",
    "\n",
    "        gru_out_target = self.sigmoid(gru_out_target)\n",
    "        # concating embedding + context = gru_out_target with hidden\n",
    "        out = torch.cat([gru_out_target,hidden], dim=2)\n",
    "        \n",
    "#         print (\"out size after concat = \"+str(out.size()))\n",
    "        \n",
    "        out = self.linear_layer(out)\n",
    "        \n",
    "#         print (\"out size after linear layer = \"+str(out.size()))\n",
    "        \n",
    "        # softmax over vocabulary\n",
    "        pred = self.log_softmax(out)\n",
    "\n",
    "        return pred, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_softmax(tensor_of_indices,\n",
    "                      vocab_size = len(zhen_en_train_token2id)):\n",
    "    \"\"\"\n",
    "    - tensor_of_indices: tensor of size (batch_size)\n",
    "    \n",
    "    Returns: tensor of size (batch_size x vocab size) that\n",
    "             represents the softmax over vocabulary for the true tokens\n",
    "             at each timestep for each sentence in the batch.\"\"\"\n",
    "    \n",
    "#     inp = torch.LongTensor(1, 32) % 100\n",
    "    index_tensor_ = torch.unsqueeze(tensor_of_indices, 2)\n",
    "\n",
    "    one_hot = torch.FloatTensor(1, 32, vocab_size).zero_()\n",
    "    one_hot.scatter_(2, index_tensor_, 1)\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translate(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, source_sentence, target_sentence, \n",
    "                source_mask, target_mask, source_lengths,\n",
    "                target_lengths):\n",
    "\n",
    "        # to hold previously decoded ys\n",
    "        y_outputs = torch.zeros(batch_size, \n",
    "                                target_sentence.size(1), \n",
    "                                len(zhen_en_train_token2id)).to(device)\n",
    "        \n",
    "        print (\"y_outputs size = \"+str(y_outputs.size()))\n",
    "#         print (breaksss)\n",
    "        #last hidden state of the encoder is the context\n",
    "        context, source_lengths = self.encoder(source_sentence,\n",
    "                                               source_mask,\n",
    "                                               source_lengths)\n",
    "\n",
    "        # context also used as the initial hidden state of the decoder\n",
    "        decoder_hidden = context\n",
    "\n",
    "        # # decoder should start with SOS tokens \n",
    "        # ref: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "        input = target_sentence[:,0].view(-1,1)\n",
    "#         print (\"input = \"+str(input))\n",
    "#         print (\"input size = \"+str(input.size()))\n",
    "        \n",
    "        for t in range(1, target_sentence.size(1)):\n",
    "            \n",
    "            decoder_out, decoder_hidden = self.decoder(context, # gru_out_source - instead of encoded_source[0]\n",
    "                                                 input, # instead of target sentence up to t \n",
    "                                                 decoder_hidden,\n",
    "                                                 target_lengths,  # target lengths\n",
    "                                                 target_mask,\n",
    "                                                 t)\n",
    "            \n",
    "#             print (\"decoder out size = \"+str(decoder_out.size()))\n",
    "            for s in range(batch_size):\n",
    "                y_outputs[s,t-1] = decoder_out[s,0]\n",
    "#             print (\"y_outputs size = \"+str(y_outputs.size()))\n",
    "#             print (\"decoder out = \"+str(decoder_out))\n",
    "#             print (\"decoder out size = \"+str(decoder_out.size()))\n",
    "#             print (\"decoder_out[s,0] = \"+str(decoder_out[s,0]))\n",
    "\n",
    "            token_out = torch.max(decoder_out.view(batch_size,self.decoder.vocab_size),1)[1]\n",
    "#             print (\"token out size = \"+str(token_out.size()))\n",
    "            print (\"token out at time step t = \"+str(token_out))\n",
    "            input = token_out.view(-1,1)\n",
    "            \n",
    "        return y_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chinese -> english\n",
    "enc = RNNencoder(vocab_size=len(zhen_zh_train_token2id), # for chinese\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=2)\n",
    "\n",
    "dec = RNNdecoder(vocab_size=len(zhen_en_train_token2id), # for chinese-english's english\n",
    "                 embedding_size=300,\n",
    "                 percent_dropout=0.3, \n",
    "                 hidden_size=256,\n",
    "                 num_gru_layers=1)\n",
    "\n",
    "model = Translate(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "def train(model, loader=zhen_train_loader,criterion=loss_function,\n",
    "          optimizer=None, \n",
    "          epoch=None):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_idx, (source_sentence, source_mask, source_lengths, \n",
    "                    target_sentence, target_mask, target_lengths)\\\n",
    "    in enumerate(loader):\n",
    "        \n",
    "        source_sentence, source_mask = source_sentence.to(device), source_mask.to(device),  \n",
    "        target_sentence, target_mask = target_sentence.to(device), target_mask.to(device),\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # output softmax as generated by decoder \n",
    "        output = model(source_sentence, target_sentence, \n",
    "                source_mask, target_mask, source_lengths,\n",
    "                target_lengths)\n",
    "        \n",
    "        # real target token\n",
    "        target = convert_to_softmax(target_sentence[:,t].view(1,target_sentence[:,t].size(0)))\n",
    "        \n",
    "        loss = criterion(out, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss/BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "y_outputs size = torch.Size([32, 300, 59327])\n",
      "token out at time step t = tensor([14714, 10064,  2042,  4271, 56510, 49882,  6436, 23596,  2433, 40416,\n",
      "        29259, 13277, 53454, 36690, 24355, 17092, 30626, 29536,  7056, 27282,\n",
      "        54411, 57394,  1881, 21724, 42954, 28554,  2833, 21571, 48102, 12462,\n",
      "        43285, 44228])\n",
      "token out at time step t = tensor([54116, 47878, 31732, 33475, 43650, 52621, 49729, 31161, 20775, 14036,\n",
      "        53681, 17673, 20463, 18203, 56931, 25397,  2067, 52235, 40142, 27697,\n",
      "        14075, 24722, 41214, 14882, 23218, 33889, 48692, 34210, 18373, 46360,\n",
      "        26753, 47316])\n",
      "token out at time step t = tensor([  522, 51773, 38649, 45863,  7951, 17801, 31229, 50426, 16265, 43200,\n",
      "        56245, 58044, 26280, 22160, 23734, 36323, 46856, 25713, 46829, 12879,\n",
      "        37160, 55381, 55740, 13997, 25746, 11876,  8134, 26345, 58239, 47464,\n",
      "          465, 15932])\n",
      "token out at time step t = tensor([20249, 54678, 42686, 37024, 47907, 39851, 37466, 31970, 23369, 30855,\n",
      "        53481, 44889, 34318,  2492, 33695, 11601,  5997,  8014, 14078, 24296,\n",
      "          650, 44167, 47460, 19027, 25587, 15837, 21722,  1655, 15720, 53905,\n",
      "        27529, 43316])\n",
      "token out at time step t = tensor([30277, 36321, 11213, 36509, 44326, 42760, 14682, 35631, 48002, 36266,\n",
      "        38703, 47381, 23014, 34645, 26860, 17479, 30223, 47515, 48722, 56306,\n",
      "         8821,  6590, 10459, 54946, 23124, 12175,  4036,  5167, 32822, 38508,\n",
      "        36027, 38658])\n",
      "token out at time step t = tensor([ 9349, 56648, 27124, 34274,  8767, 26683,  6032,  3976, 53102, 20968,\n",
      "        21731, 18439,  3934, 33865, 21005, 19381,  6145, 12973, 22967, 48397,\n",
      "         5243,  1593, 23242, 42477, 12496,  8143, 39104, 12905, 53136,  3299,\n",
      "        55350, 38440])\n",
      "token out at time step t = tensor([ 4564, 32724, 54902, 58329, 35292,   819, 38856, 30775, 13304, 26919,\n",
      "         8782, 21016,  1706, 30592, 25321, 42983, 25664, 17820,   913, 21034,\n",
      "        28196, 58841, 34258, 38196, 13688, 22990, 23845, 50469,  8536, 20542,\n",
      "          353,  6025])\n",
      "token out at time step t = tensor([28893, 17150,  3296, 38090, 36785, 16577, 18810, 33496,  2276, 45932,\n",
      "        23144, 50019, 42468, 36902, 19057, 23599, 57853, 18202, 47197,  5361,\n",
      "        49548, 48649, 37763, 16133, 15786,  5381, 36162, 32818, 19204,  1161,\n",
      "        20218, 40247])\n",
      "token out at time step t = tensor([29203, 48759, 50705, 29365, 12176, 14909, 54516, 21986, 43117,  2019,\n",
      "         7099, 31400, 26531, 43817, 40594, 12928, 53446, 43466,  7073, 21681,\n",
      "        33985, 50254, 33378, 58820,  9182, 44273, 47572, 46909, 19888,  3727,\n",
      "        52260, 53514])\n",
      "token out at time step t = tensor([ 5043, 18143, 54608, 55665,  6296, 59203, 58967, 16724, 25213, 12922,\n",
      "        56676, 19096,  7353,  3639, 51131, 51067, 34549, 34317,  4552, 18509,\n",
      "        48511, 27603, 13399, 46249,  2442, 12167, 26571,  4585, 53680, 28769,\n",
      "         7331,  3327])\n",
      "token out at time step t = tensor([19356, 11068, 24119, 31495, 35245, 30971,  7467, 21740, 39363, 11256,\n",
      "        23512, 58796,  7321,  5110, 42314,  9774, 36099, 57744,  5853, 24795,\n",
      "        51383,  6067, 16991, 28348, 44506, 39864, 30439, 47081,   596, 18886,\n",
      "        12922, 31792])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-1d070aae2c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                        optimizer = torch.optim.Adam(model.parameters(), \n\u001b[1;32m     14\u001b[0m                                                    lr=lr),\n\u001b[0;32m---> 15\u001b[0;31m                       epoch = epoch)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-164-b674dd3d30c8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m         output = model(source_sentence, target_sentence, \n\u001b[1;32m     21\u001b[0m                 \u001b[0msource_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 target_lengths)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# real target token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-162-720d5c08418a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source_sentence, target_sentence, source_mask, target_mask, source_lengths, target_lengths)\u001b[0m\n\u001b[1;32m     38\u001b[0m                                                  \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# target lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                                  \u001b[0mtarget_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                                                  t)\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#             print (\"decoder out size = \"+str(decoder_out.size()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-160-fbaee8d4a06a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_source, previously_decoded_upto_t, hidden, target_lengths, target_mask, time_step)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mgru_out_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# ref: pytorch documentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvariable_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mflat_hidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mGRUCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mgi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mgh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mi_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mh_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 3\n",
    "lr = 1e-4\n",
    "# batch_\n",
    "\n",
    "loss_train = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print (\"epoch = \"+str(epoch))\n",
    "\n",
    "    loss = train(model,\n",
    "                       loader = zhen_train_loader,\n",
    "                       optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                                   lr=lr),\n",
    "                      epoch = epoch)\n",
    "    \n",
    "    loss_train.append(loss)\n",
    "    \n",
    "    print (loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 32\n",
    "# def train(RNNenc=RNNencoder,\n",
    "#           RNNdec=RNNdecoder,\n",
    "#           generator=Linear_Layers, # Linear_Layers to generate next 1 token\n",
    "#           loader=zhen_train_loader, # automate\n",
    "#           optimizer=None, \n",
    "#           epoch=None):\n",
    "    \n",
    "#     RNNenc.train()\n",
    "#     RNNdec.train()\n",
    "#     generator.train()\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     # previously decoded upto t goes into decoder\n",
    "    \n",
    "#     for batch_idx, (source_sentence, source_mask, source_lengths, \n",
    "#                     target_sentence, target_mask, target_lengths)\\\n",
    "#     in enumerate(DataLoader):\n",
    "            \n",
    "#         source_sentence, source_mask = source_sentence.to(device), source_mask.to(device),  \n",
    "#         target_sentence, target_mask = target_sentence.to(device), target_mask.to(device),\n",
    "        \n",
    "#         RNNenc.train()\n",
    "#         RNNdec.train()\n",
    "#         generator.train()\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         generated_target = torch.empty(0)\n",
    "#         # decoder should start with SOS tokens \n",
    "#         # ref: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "#         prev_decoded_y_tokens = torch.zeros(BATCH_SIZE) + SOS_token\n",
    "        \n",
    "#         encoder_hidden, source_lengths = RNNenc(source_sentence,\n",
    "#                                                         source_mask,\n",
    "#                                                         source_lengths)\n",
    "        \n",
    "#         hidden_ = encoder_hidden\n",
    "        \n",
    "#         for t in range(target_sentence.size(1)):\n",
    "            \n",
    "#             print (\"Time Step = \"+str(t))\n",
    "            \n",
    "# #             target_sentence_upto_t = target_sentence[:,t].view(batch_size,t+1)\n",
    "#             target_sentence_upto_t = target_sentence[:,:t]\n",
    "#             print (\"target_sentence_upto_t size = \"+str(target_sentence_upto_t.size()))\n",
    "            \n",
    "#             decoder_out, decoder_hidden = RNNdec(encoder_hidden, # gru_out_source - instead of encoded_source[0]\n",
    "#                                                  prev_decoded_y_tokens, # instead of target sentence up to t \n",
    "#                                                  hidden_,\n",
    "#                                                  target_lengths,  # target lengths\n",
    "#                                                  target_mask,\n",
    "#                                                  t)\n",
    "\n",
    "#             out = generator(decoder_out,t) # returns softmax, t for timestep\n",
    "            \n",
    "#             # vector embeddings\n",
    "#             generated_target = torch.cat([generated_target, out], dim=1)\n",
    "#             # tokens\n",
    "#             print (\"prev_decoded_y_tokens size = \"+str(prev_decoded_y_tokens.size()))\n",
    "#             print (\"max out size = \"+str(torch.max(out,1)[1].view(BATCH_SIZE,t+1).long().size()))\n",
    "#             prev_decoded_y_tokens = torch.cat([prev_decoded_y_tokens.view(BATCH_SIZE,t+1).long(), torch.max(out,1)[1].view(BATCH_SIZE,t+1).long()], \n",
    "#                                               dim=1)\n",
    "            \n",
    "#             print (\"prev_decoded_y_tokens: \"+str(prev_decoded_y_tokens))\n",
    "#             print (\"prev_decoded_y_tokens size = \"+str(prev_decoded_y_tokens.size()))\n",
    "#             print (\"target tokens to generate at time t (for each sentence in batch) : \"+str(target_sentence[:,t]))\n",
    "#             print (\"model-generated tokens at time t : \"+str(torch.max(out,1)[1]))\n",
    "# #             print (\"type torch max out = \"+str(type(torch.max(out,1)[1])))\n",
    "            \n",
    "#             # only to calculate the loss at each time step t\n",
    "#             target = convert_to_softmax(target_sentence[:,t].view(1,target_sentence[:,t].size(0)))\n",
    "#             print (\"target_size = \"+str(target.size()))\n",
    "            \n",
    "#             loss = loss_function(out, target)\n",
    "#             print (\"Loss = \"+str(loss))\n",
    "# #             loss.cuda().backward()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item() * len(source_sentence) / len(DataLoader.dataset)\n",
    "        \n",
    "# #         if (batch_idx+1) % (len(DataLoader.dataset)//(20*labels.shape[0])) == 0:\n",
    "# #             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "# #                 epoch, (batch_idx+1) * labels.shape[0], len(DataLoader.dataset),\n",
    "# #                 100. * (batch_idx+1) / len(DataLoader), loss.item()))\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     return total_loss\n",
    "\n",
    "\n",
    "# # def test(RNN, \n",
    "# #          Linear_Classifier, \n",
    "# #          DataLoader, \n",
    "# #          criterion):\n",
    "\n",
    "# #     RNN.eval()\n",
    "# #     Linear_Classifier.eval()\n",
    "    \n",
    "# #     test_loss = 0\n",
    "# #     label_list = []\n",
    "# #     output_list = []\n",
    "    \n",
    "# #     with torch.no_grad():\n",
    "# #         for batch_idx, (sentence1, s1_original, sentence1_lengths, \n",
    "# #                     sentence2, s2_original, sentence2_lengths, labels)\\\n",
    "# #                     in enumerate(DataLoader):\n",
    "\n",
    "# #             sentence1, s1_original = sentence1.to(device), s1_original.to(device),  \n",
    "# #             sentence2, s2_original = sentence2.to(device), s2_original.to(device),\n",
    "# #             labels = labels.to(device)\n",
    "            \n",
    "# #             # Forward\n",
    "# #             output_s1 = RNN(sentence1, \n",
    "# #                                   s1_original, \n",
    "# #                                   sentence1_lengths)\n",
    "# #             # Reverse\n",
    "# #             output_s2 = RNN(sentence2, \n",
    "# #                                   s2_original, \n",
    "# #                                   sentence2_lengths)\n",
    "            \n",
    "# #             out = Linear_Classifier(output_s1, output_s2)\n",
    "        \n",
    "# #             loss = criterion(out, labels)\n",
    "\n",
    "# #             test_loss += loss.item()/len(DataLoader.dataset)\n",
    "\n",
    "# #             output_list.append(out)\n",
    "# #             label_list.append(labels)\n",
    "            \n",
    "# #             print (\"outputs= \"+str(torch.cat(output_list, dim=0)))\n",
    "# #             print (\"labels= \"+str(torch.cat(label_list, dim=0)))\n",
    "            \n",
    "# #     return test_loss, torch.cat(output_list, dim=0), torch.cat(label_list, dim=0)\n",
    "\n",
    "# # def accuracy(RNN, \n",
    "# #              Linear_Classifier, \n",
    "# #              DataLoader, \n",
    "# #              criterion):\n",
    "    \n",
    "# #     _, predicted, true_labels = test(RNN = RNN,\n",
    "# #                               Linear_Classifier = Linear_Classifier,\n",
    "# #                               DataLoader = DataLoader,\n",
    "# #                               criterion = criterion)\n",
    "\n",
    "# #     predicted = predicted.max(1)[1]\n",
    "# #     return 100 * predicted.eq(true_labels.data.view_as(predicted)).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RNNencoder_model = RNNencoder() # encoder model\n",
    "RNNdecoder_model = RNNdecoder() # decoder model and concatenation\n",
    "generator_model = Linear_Layers() # logsoftmax over vocab\n",
    "DataLoader = zhen_train_loader\n",
    "\n",
    "num_epochs = 3\n",
    "lr = 1e-4\n",
    "# batch_\n",
    "\n",
    "loss_train = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print (\"epoch = \"+str(epoch))\n",
    "\n",
    "    loss = train(RNNencoder_model,\n",
    "                 RNNdecoder_model,\n",
    "                 generator_model,\n",
    "                       loader = DataLoader,\n",
    "                       optimizer = torch.optim.Adam(list(RNNencoder_model.parameters()) + \\\n",
    "                                                    list(RNNdecoder_model.parameters()) + \\\n",
    "                                                   list(generator_model.parameters()), \n",
    "                                                   lr=lr),\n",
    "                      epoch = epoch)\n",
    "    \n",
    "    loss_train.append(loss)\n",
    "    \n",
    "    print (loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.empty(0).unsqueeze(1) # only case where the unsquueze func cannot be executed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(32,1).repeat(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight tensor is empty - initialize weigths for rnn ? why doesn't this happen for the encoder ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 RNN-based Encoder-Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://github.com/yanwii/seq2seq/blob/master/seq2seq.py\n",
    "\n",
    "###################\n",
    "## ORIGINAL CODE ##\n",
    "###################\n",
    "\n",
    "# ENCODER\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "\n",
    "    def forward(self, word_inputs, hidden):\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden\n",
    "    \n",
    "# ATTENTION\n",
    "    \n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size, max_length):\n",
    "        super(Attn, self).__init__()\n",
    "\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        attn_energies = Variable(torch.zeros(seq_len)) # B x 1 x S\n",
    "        if USE_CUDA: attn_energies = attn_energies.cuda()\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
    "\n",
    "        return F.softmax(attn_energies).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    def score(self, hidden, encoder_output):\n",
    "        if self.method == 'dot':\n",
    "            energy = torch.dot(hidden.view(-1), encoder_output.view(-1))\n",
    "            return energy\n",
    "\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = torch.dot(hidden.view(-1), encoder_output.view(-1))\n",
    "            return energy\n",
    "        \n",
    "# DECODER\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=10):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size, self.max_length)\n",
    "\n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "\n",
    "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        attn_weights = self.attn(rnn_output.squeeze(0), encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)))\n",
    "        #output = self.out(torch.cat((rnn_output, context), 1))\n",
    "        return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.max_epoches = 100000\n",
    "        self.batch_index = 0\n",
    "        self.GO_token = 2\n",
    "        self.EOS_token = 1\n",
    "        self.input_size = 14\n",
    "        self.output_size = 15\n",
    "        self.hidden_size = 100\n",
    "        self.max_length = 15\n",
    "        self.show_epoch = 100\n",
    "        self.use_cuda = USE_CUDA\n",
    "        self.model_path = \"./model/\"\n",
    "        self.n_layers = 1\n",
    "        self.dropout_p = 0.05\n",
    "        self.beam_search = True\n",
    "        self.top_k = 5\n",
    "        self.alpha = 0.5\n",
    "\n",
    "        self.enc_vec = []\n",
    "        self.dec_vec = []\n",
    "\n",
    "        # 初始化encoder和decoder\n",
    "        self.encoder = EncoderRNN(self.input_size, self.hidden_size, self.n_layers)\n",
    "        self.decoder = AttnDecoderRNN('general', self.hidden_size, self.output_size, self.n_layers, self.dropout_p, self.max_length)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            self.encoder = self.encoder.cuda()\n",
    "            self.decoder = self.decoder.cuda()\n",
    "\n",
    "        self.encoder_optimizer = optim.Adam(self.encoder.parameters())\n",
    "        self.decoder_optimizer = optim.Adam(self.decoder.parameters())\n",
    "        self.criterion = nn.NLLLoss()\n",
    "\n",
    "    def loadData(self):\n",
    "        with open(\"./data/enc.vec\") as enc:\n",
    "            line = enc.readline()\n",
    "            while line:\n",
    "                self.enc_vec.append(line.strip().split())\n",
    "                line = enc.readline()\n",
    "\n",
    "        with open(\"./data/dec.vec\") as dec:\n",
    "            line = dec.readline()\n",
    "            while line:\n",
    "                self.dec_vec.append(line.strip().split())\n",
    "                line = dec.readline()\n",
    "\n",
    "    def next(self, batch_size, eos_token=1, go_token=2, shuffle=False):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "\n",
    "        if shuffle:\n",
    "            ind = random.choice(range(len(self.enc_vec)))\n",
    "            enc = [self.enc_vec[ind]]\n",
    "            dec = [self.dec_vec[ind]]\n",
    "        else:\n",
    "            if self.batch_index+batch_size >= len(self.enc_vec):\n",
    "                enc = self.enc_vec[self.batch_index:]\n",
    "                dec = self.dec_vec[self.batch_index:]\n",
    "                self.batch_index = 0\n",
    "            else:\n",
    "                enc = self.enc_vec[self.batch_index:self.batch_index+batch_size]\n",
    "                dec = self.dec_vec[self.batch_index:self.batch_index+batch_size]\n",
    "                self.batch_index += batch_size\n",
    "        for index in range(len(enc)):\n",
    "            enc = enc[0][:self.max_length] if len(enc[0]) > self.max_length else enc[0]\n",
    "            dec = dec[0][:self.max_length] if len(dec[0]) > self.max_length else dec[0]\n",
    "\n",
    "            enc = [int(i) for i in enc]\n",
    "            dec = [int(i) for i in dec]\n",
    "            dec.append(eos_token)\n",
    "\n",
    "            inputs.append(enc)\n",
    "            targets.append(dec)\n",
    "\n",
    "        inputs = Variable(torch.LongTensor(inputs)).transpose(1, 0).contiguous()\n",
    "        targets = Variable(torch.LongTensor(targets)).transpose(1, 0).contiguous()\n",
    "        if USE_CUDA:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        return inputs, targets\n",
    "\n",
    "    def train(self):\n",
    "        self.loadData()\n",
    "        try:\n",
    "            self.load_state_dict(torch.load(self.model_path+'params.pkl'))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"No model!\")\n",
    "        loss_track = []\n",
    "\n",
    "        for epoch in range(self.max_epoches):\n",
    "            start = time.time()\n",
    "            inputs, targets = self.next(1, shuffle=False)\n",
    "            loss, logits = self.step(inputs, targets, self.max_length)\n",
    "            loss_track.append(loss)\n",
    "            _,v = torch.topk(logits, 1)\n",
    "            pre = v.cpu().data.numpy().T.tolist()[0][0]\n",
    "            tar = targets.cpu().data.numpy().T.tolist()[0]\n",
    "            stop = time.time()\n",
    "            if epoch % self.show_epoch == 0:\n",
    "                print(\"-\"*50)\n",
    "                print(\"epoch:\", epoch)\n",
    "                print(\"    loss:\", loss)\n",
    "                print(\"    target:%s\\n    output:%s\" % (tar, pre))\n",
    "                print(\"    per-time:\", (stop-start))\n",
    "                torch.save(self.state_dict(), self.model_path+'params.pkl')\n",
    "\n",
    "    def step(self, input_variable, target_variable, max_length):\n",
    "        teacher_forcing_ratio = 0.1\n",
    "        clip = 5.0\n",
    "        loss = 0 # Added onto for each word\n",
    "\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "\n",
    "        input_length = input_variable.size()[0]\n",
    "        target_length = target_variable.size()[0]\n",
    "\n",
    "        encoder_hidden = self.encoder.init_hidden()\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_variable, encoder_hidden)\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "        decoder_context = Variable(torch.zeros(1, self.decoder.hidden_size))\n",
    "        decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "        if USE_CUDA:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "            decoder_context = decoder_context.cuda()\n",
    "\n",
    "        decoder_outputs = []\n",
    "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "        use_teacher_forcing = True\n",
    "        if use_teacher_forcing:\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_context, decoder_hidden, decoder_attention = self.decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "                loss += self.criterion(decoder_output, target_variable[di])\n",
    "                decoder_input = target_variable[di]\n",
    "                decoder_outputs.append(decoder_output.unsqueeze(0))\n",
    "        else:\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_context, decoder_hidden, decoder_attention = self.decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "                loss += self.criterion(decoder_output, target_variable[di])\n",
    "                decoder_outputs.append(decoder_output.unsqueeze(0))\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                ni = topi[0][0]\n",
    "\n",
    "                decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "                if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "                if ni == EOS_token: break\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.encoder.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm(self.decoder.parameters(), clip)\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        decoder_outputs = torch.cat(decoder_outputs, 0)\n",
    "        return loss.data[0] / target_length, decoder_outputs\n",
    "\n",
    "    def make_infer_fd(self, input_vec):\n",
    "        inputs = []\n",
    "        enc = input_vec[:self.max_length] if len(input_vec) > self.max_length else input_vec\n",
    "        inputs.append(enc)\n",
    "        inputs = Variable(torch.LongTensor(inputs)).transpose(1, 0).contiguous()\n",
    "        if USE_CUDA:\n",
    "            inputs = inputs.cuda()\n",
    "        return inputs\n",
    "\n",
    "    def predict(self):\n",
    "        try:\n",
    "            self.load_state_dict(torch.load(self.model_path+'params.pkl'))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"No model!\")\n",
    "        loss_track = []\n",
    "\n",
    "        # 加载字典\n",
    "        str_to_vec = {}\n",
    "        with open(\"./data/enc.vocab\") as enc_vocab:\n",
    "            for index,word in enumerate(enc_vocab.readlines()):\n",
    "                str_to_vec[word.strip()] = index\n",
    "\n",
    "        vec_to_str = {}\n",
    "        with open(\"./data/dec.vocab\") as dec_vocab:\n",
    "            for index,word in enumerate(dec_vocab.readlines()):\n",
    "                vec_to_str[index] = word.strip()\n",
    "\n",
    "        while True:\n",
    "            input_strs = input(\"me > \")\n",
    "            # 字符串转向量\n",
    "            segement = jieba.lcut(input_strs)\n",
    "            input_vec = [str_to_vec.get(i, 3) for i in segement]\n",
    "            input_vec = self.make_infer_fd(input_vec)\n",
    "\n",
    "            # inference\n",
    "            if self.beam_search:\n",
    "                samples = self.beamSearchDecoder(input_vec)\n",
    "                for sample in samples:\n",
    "                    outstrs = []\n",
    "                    for i in sample[0]:\n",
    "                        if i == 1:\n",
    "                            break\n",
    "                        outstrs.append(vec_to_str.get(i, \"Un\"))\n",
    "                    print(\"ai > \", \"\".join(outstrs), sample[3])\n",
    "            else:\n",
    "                logits = self.infer(input_vec)\n",
    "                _,v = torch.topk(logits, 1)\n",
    "                pre = v.cpu().data.numpy().T.tolist()[0][0]\n",
    "                outstrs = []\n",
    "                for i in pre:\n",
    "                    if i == 1:\n",
    "                        break\n",
    "                    outstrs.append(vec_to_str.get(i, \"Un\"))\n",
    "                print(\"ai > \", \"\".join(outstrs))\n",
    "\n",
    "    def infer(self, input_variable):\n",
    "        input_length = input_variable.size()[0]\n",
    "\n",
    "        encoder_hidden = self.encoder.init_hidden()\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_variable, encoder_hidden)\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "        decoder_context = Variable(torch.zeros(1, self.decoder.hidden_size))\n",
    "        decoder_hidden = encoder_hidden\n",
    "        if USE_CUDA:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "            decoder_context = decoder_context.cuda()\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(self.max_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = self.decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            decoder_outputs.append(decoder_output.unsqueeze(0))\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]])) # Chosen word is next input\n",
    "            if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "            if ni == EOS_token: break\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, 0)\n",
    "        return decoder_outputs\n",
    "\n",
    "    def tensorToList(self, tensor):\n",
    "        return tensor.cpu().data.numpy().tolist()[0]\n",
    "\n",
    "    def beamSearchDecoder(self, input_variable):\n",
    "        input_length = input_variable.size()[0]\n",
    "        encoder_hidden = self.encoder.init_hidden()\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_variable, encoder_hidden)\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "        decoder_context = Variable(torch.zeros(1, self.decoder.hidden_size))\n",
    "        decoder_hidden = encoder_hidden\n",
    "        if USE_CUDA:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "            decoder_context = decoder_context.cuda()\n",
    "\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = self.decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        topk = decoder_output.data.topk(self.top_k)\n",
    "        samples = [[] for i in range(self.top_k)]\n",
    "        dead_k = 0\n",
    "        final_samples = []\n",
    "        for index in range(self.top_k):\n",
    "            topk_prob = topk[0][0][index]\n",
    "            topk_index = int(topk[1][0][index])\n",
    "            samples[index] = [[topk_index], topk_prob, 0, 0, decoder_context, decoder_hidden, decoder_attention, encoder_outputs]\n",
    "\n",
    "        for _ in range(self.max_length):\n",
    "            tmp = []\n",
    "            for index in range(len(samples)):\n",
    "                tmp.extend(self.beamSearchInfer(samples[index], index))\n",
    "            samples = []\n",
    "\n",
    "            # 筛选出topk\n",
    "            df = pd.DataFrame(tmp)\n",
    "            df.columns = ['sequence', 'pre_socres', 'fin_scores', \"ave_scores\", \"decoder_context\", \"decoder_hidden\", \"decoder_attention\", \"encoder_outputs\"]\n",
    "            sequence_len = df.sequence.apply(lambda x:len(x))\n",
    "            df['ave_scores'] = df['fin_scores'] / sequence_len\n",
    "            df = df.sort_values('ave_scores', ascending=False).reset_index().drop(['index'], axis=1)\n",
    "            df = df[:(self.top_k-dead_k)]\n",
    "            for index in range(len(df)):\n",
    "                group = df.ix[index]\n",
    "                if group.tolist()[0][-1] == 1:\n",
    "                    final_samples.append(group.tolist())\n",
    "                    df = df.drop([index], axis=0)\n",
    "                    dead_k += 1\n",
    "                    print(\"drop {}, {}\".format(group.tolist()[0], dead_k))\n",
    "            samples = df.values.tolist()\n",
    "            if len(samples) == 0:\n",
    "                break\n",
    "\n",
    "        if len(final_samples) < self.top_k:\n",
    "            final_samples.extend(samples[:(self.top_k-dead_k)])\n",
    "        return final_samples\n",
    "\n",
    "    def beamSearchInfer(self, sample, k):\n",
    "        samples = []\n",
    "        decoder_input = Variable(torch.LongTensor([[sample[0][-1]]]))\n",
    "        if USE_CUDA:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "        sequence, pre_scores, fin_scores, ave_scores, decoder_context, decoder_hidden, decoder_attention, encoder_outputs = sample\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = self.decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "\n",
    "        # choose topk\n",
    "        topk = decoder_output.data.topk(self.top_k)\n",
    "        for k in range(self.top_k):\n",
    "            topk_prob = topk[0][0][k]\n",
    "            topk_index = int(topk[1][0][k])\n",
    "            pre_scores += topk_prob\n",
    "            fin_scores = pre_scores - (k - 1 ) * self.alpha\n",
    "            samples.append([sequence+[topk_index], pre_scores, fin_scores, ave_scores, decoder_context, decoder_hidden, decoder_attention, encoder_outputs])\n",
    "        return samples\n",
    "\n",
    "    def retrain(self):\n",
    "        try:\n",
    "            os.remove(self.model_path)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        self.train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    seq = seq2seq()\n",
    "    if sys.argv[1] == 'train':\n",
    "        seq.train()\n",
    "    elif sys.argv[1] == 'predict':\n",
    "        seq.predict()\n",
    "    elif sys.argv[1] == 'retrain':\n",
    "        seq.retrain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoder Replacement with Convolutional or Self-attention-based Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://github.com/yanwii/seq2seq/blob/master/seq2seq.py\n",
    "\n",
    "# ENCODER\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \n",
    "        super(EncoderCNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        self.CNN = nn.CNN()### TODO\n",
    "\n",
    "    def forward(self, word_inputs, hidden):\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Fully self-attention Translation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Multilingual Translation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
