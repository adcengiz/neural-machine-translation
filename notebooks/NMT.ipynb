{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Spring 2018 NLP Class Project: Neural Machine Translation</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pdb\n",
    "import os\n",
    "from underthesea import word_tokenize\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install spacy && python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Project Overview\n",
    "\n",
    "The goal of this project is to build a neural machine translation system and experience how recent advances have made their way. Each team will build the following sequence of neural translation systems for two language pairs, __Vietnamese (Vi)→English (En)__ and __Chinese (Zh)→En__ (prepared corpora is be provided):\n",
    "\n",
    "1. Recurrent neural network based encoder-decoder without attention\n",
    "2. Recurrent neural network based encoder-decoder with attention\n",
    "2. Replace the recurrent encoder with either convolutional or self-attention based encoder.\n",
    "4. [Optional] Build either or both fully self-attention translation system or/and multilingual translation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Upload & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of sentence\n",
    "SOS_token = 0\n",
    "# end of sentence\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    \"\"\"About \"NFC\" and \"NFD\": \n",
    "    \n",
    "    For each character, there are two normal forms: normal form C \n",
    "    and normal form D. Normal form D (NFD) is also known as canonical \n",
    "    decomposition, and translates each character into its decomposed form. \n",
    "    Normal form C (NFC) first applies a canonical decomposition, then composes \n",
    "    pre-combined characters again.\n",
    "    \n",
    "    About unicodedata.category: \n",
    "    \n",
    "    Returns the general category assigned to the Unicode character \n",
    "    unichr as string.\"\"\"\n",
    "    \n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Trim\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False,\n",
    "             dataset=\"train\"):\n",
    "    \n",
    "    \"\"\"Takes as input;\n",
    "    - lang1, lang2: either (vi, en) or (zh, en)\n",
    "    - dataset: one of (\"train\",\"dev\",\"test\")\"\"\"\n",
    "    print(\"Reading lines...\")\n",
    "    eos = [\".\",\"?\",\"!\",\"\\n\"]\n",
    "    # Read the pretokenized lang1 file and split into lines\n",
    "    lang1_lines = open(\"../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-%s-%s-processed/%s.tok.%s\" % (lang1, lang2, dataset, lang1), encoding=\"utf-8\").\\\n",
    "        read().strip().split(\"\\n\")\n",
    "    # Read the lang2 file and split into lines\n",
    "    lang2_lines = open(\"../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-%s-%s-processed/%s.tok.%s\" % (lang1, lang2, dataset, lang2), encoding=\"utf-8\").\\\n",
    "        read().strip().split(\"\\n\")\n",
    "    \n",
    "    # create sentence pairs (lists of length 2 that consist of string pairs)\n",
    "    # e.g. [\"And we &apos;re going to tell you some stories from the sea here in video .\",\n",
    "    #       \"我们 将 用 一些 影片 来讲 讲述 一些 深海 海里 的 故事  \"]\n",
    "    # check if there are the same number of sentences in each set\n",
    "    assert len(lang1_lines) == len(lang2_lines), \"Two languages must have the same number of sentences. \"+ str(len(lang1_lines)) + \" sentences were passed for \" + str(lang1) + \".\" + str(len(lang2_lines)) + \" sentences were passed for \" + str(lang2)+\".\"\n",
    "    # normalize if not Chinese, Chinese normalization is already handeled\n",
    "    if lang1 == \"zh\":\n",
    "        lang1_lines = lang1_lines\n",
    "    else:\n",
    "        lang1_lines = [normalizeString(s) for s in lang1_lines]\n",
    "    lang2_lines = [normalizeString(s) for s in lang2_lines]\n",
    "    # construct pairs\n",
    "    pair_ran = range(len(lang1_lines))\n",
    "    pairs = [[lang1_lines[i]] + [lang2_lines[i]] for i in pair_ran]\n",
    "    \n",
    "#     # Split every line into pairs and normalize\n",
    "#     pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 133317 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 16144\n",
      "en 47568\n",
      "['Anh a lam rat tot voi at lien voi mat at .', 'You did a great job with the land the dirt .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False, dataset=\"train\"):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse, dataset=dataset)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# example\n",
    "input_lang, output_lang, pairs = prepareData('vi', 'en', False, dataset=\"train\")\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213376 sentence pairs\n",
      "Trimmed to 213376 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 88917\n",
      "en 59329\n",
      "['而 它 就 分布 在 细胞 细胞膜 胞膜 中 并且 自身 还 带 了 个 小孔  ', 'And it sits in the membrane of the cell and it apos s got a pore in it .']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('zh', 'en', False, dataset=\"train\")\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vietnamese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.vi 1000 133317\n",
      "train.vi 2000 133317\n",
      "train.vi 3000 133317\n",
      "train.vi 4000 133317\n",
      "train.vi 5000 133317\n",
      "train.vi 6000 133317\n",
      "train.vi 7000 133317\n",
      "train.vi 8000 133317\n",
      "train.vi 9000 133317\n",
      "train.vi 10000 133317\n",
      "train.vi 11000 133317\n",
      "train.vi 12000 133317\n",
      "train.vi 13000 133317\n",
      "train.vi 14000 133317\n",
      "train.vi 15000 133317\n",
      "train.vi 16000 133317\n",
      "train.vi 17000 133317\n",
      "train.vi 18000 133317\n",
      "train.vi 19000 133317\n",
      "train.vi 20000 133317\n",
      "train.vi 21000 133317\n",
      "train.vi 22000 133317\n",
      "train.vi 23000 133317\n",
      "train.vi 24000 133317\n",
      "train.vi 25000 133317\n",
      "train.vi 26000 133317\n",
      "train.vi 27000 133317\n",
      "train.vi 28000 133317\n",
      "train.vi 29000 133317\n",
      "train.vi 30000 133317\n",
      "train.vi 31000 133317\n",
      "train.vi 32000 133317\n",
      "train.vi 33000 133317\n",
      "train.vi 34000 133317\n",
      "train.vi 35000 133317\n",
      "train.vi 36000 133317\n",
      "train.vi 37000 133317\n",
      "train.vi 38000 133317\n",
      "train.vi 39000 133317\n",
      "train.vi 40000 133317\n",
      "train.vi 41000 133317\n",
      "train.vi 42000 133317\n",
      "train.vi 43000 133317\n",
      "train.vi 44000 133317\n",
      "train.vi 45000 133317\n",
      "train.vi 46000 133317\n",
      "train.vi 47000 133317\n",
      "train.vi 48000 133317\n",
      "train.vi 49000 133317\n",
      "train.vi 50000 133317\n",
      "train.vi 51000 133317\n",
      "train.vi 52000 133317\n",
      "train.vi 53000 133317\n",
      "train.vi 54000 133317\n",
      "train.vi 55000 133317\n",
      "train.vi 56000 133317\n",
      "train.vi 57000 133317\n",
      "train.vi 58000 133317\n",
      "train.vi 59000 133317\n",
      "train.vi 60000 133317\n",
      "train.vi 61000 133317\n",
      "train.vi 62000 133317\n",
      "train.vi 63000 133317\n",
      "train.vi 64000 133317\n",
      "train.vi 65000 133317\n",
      "train.vi 66000 133317\n",
      "train.vi 67000 133317\n",
      "train.vi 68000 133317\n",
      "train.vi 69000 133317\n",
      "train.vi 70000 133317\n",
      "train.vi 71000 133317\n",
      "train.vi 72000 133317\n",
      "train.vi 73000 133317\n",
      "train.vi 74000 133317\n",
      "train.vi 75000 133317\n",
      "train.vi 76000 133317\n",
      "train.vi 77000 133317\n",
      "train.vi 78000 133317\n",
      "train.vi 79000 133317\n",
      "train.vi 80000 133317\n",
      "train.vi 81000 133317\n",
      "train.vi 82000 133317\n",
      "train.vi 83000 133317\n",
      "train.vi 84000 133317\n",
      "train.vi 85000 133317\n",
      "train.vi 86000 133317\n",
      "train.vi 87000 133317\n",
      "train.vi 88000 133317\n",
      "train.vi 89000 133317\n",
      "train.vi 90000 133317\n",
      "train.vi 91000 133317\n",
      "train.vi 92000 133317\n",
      "train.vi 93000 133317\n",
      "train.vi 94000 133317\n",
      "train.vi 95000 133317\n",
      "train.vi 96000 133317\n",
      "train.vi 97000 133317\n",
      "train.vi 98000 133317\n",
      "train.vi 99000 133317\n",
      "train.vi 100000 133317\n",
      "train.vi 101000 133317\n",
      "train.vi 102000 133317\n",
      "train.vi 103000 133317\n",
      "train.vi 104000 133317\n",
      "train.vi 105000 133317\n",
      "train.vi 106000 133317\n",
      "train.vi 107000 133317\n",
      "train.vi 108000 133317\n",
      "train.vi 109000 133317\n",
      "train.vi 110000 133317\n",
      "train.vi 111000 133317\n",
      "train.vi 112000 133317\n",
      "train.vi 113000 133317\n",
      "train.vi 114000 133317\n",
      "train.vi 115000 133317\n",
      "train.vi 116000 133317\n",
      "train.vi 117000 133317\n",
      "train.vi 118000 133317\n",
      "train.vi 119000 133317\n",
      "train.vi 120000 133317\n",
      "train.vi 121000 133317\n",
      "train.vi 122000 133317\n",
      "train.vi 123000 133317\n",
      "train.vi 124000 133317\n",
      "train.vi 125000 133317\n",
      "train.vi 126000 133317\n",
      "train.vi 127000 133317\n",
      "train.vi 128000 133317\n",
      "train.vi 129000 133317\n",
      "train.vi 130000 133317\n",
      "train.vi 131000 133317\n",
      "train.vi 132000 133317\n",
      "train.vi 133000 133317\n",
      "dev.vi 1000 1268\n",
      "test.vi 1000 1553\n",
      "train.en 1000 133317\n",
      "train.en 2000 133317\n",
      "train.en 3000 133317\n",
      "train.en 4000 133317\n",
      "train.en 5000 133317\n",
      "train.en 6000 133317\n",
      "train.en 7000 133317\n",
      "train.en 8000 133317\n",
      "train.en 9000 133317\n",
      "train.en 10000 133317\n",
      "train.en 11000 133317\n",
      "train.en 12000 133317\n",
      "train.en 13000 133317\n",
      "train.en 14000 133317\n",
      "train.en 15000 133317\n",
      "train.en 16000 133317\n",
      "train.en 17000 133317\n",
      "train.en 18000 133317\n",
      "train.en 19000 133317\n",
      "train.en 20000 133317\n",
      "train.en 21000 133317\n",
      "train.en 22000 133317\n",
      "train.en 23000 133317\n",
      "train.en 24000 133317\n",
      "train.en 25000 133317\n",
      "train.en 26000 133317\n",
      "train.en 27000 133317\n",
      "train.en 28000 133317\n",
      "train.en 29000 133317\n",
      "train.en 30000 133317\n",
      "train.en 31000 133317\n",
      "train.en 32000 133317\n",
      "train.en 33000 133317\n",
      "train.en 34000 133317\n",
      "train.en 35000 133317\n",
      "train.en 36000 133317\n",
      "train.en 37000 133317\n",
      "train.en 38000 133317\n",
      "train.en 39000 133317\n",
      "train.en 40000 133317\n",
      "train.en 41000 133317\n",
      "train.en 42000 133317\n",
      "train.en 43000 133317\n",
      "train.en 44000 133317\n",
      "train.en 45000 133317\n",
      "train.en 46000 133317\n",
      "train.en 47000 133317\n",
      "train.en 48000 133317\n",
      "train.en 49000 133317\n",
      "train.en 50000 133317\n",
      "train.en 51000 133317\n",
      "train.en 52000 133317\n",
      "train.en 53000 133317\n",
      "train.en 54000 133317\n",
      "train.en 55000 133317\n",
      "train.en 56000 133317\n",
      "train.en 57000 133317\n",
      "train.en 58000 133317\n",
      "train.en 59000 133317\n",
      "train.en 60000 133317\n",
      "train.en 61000 133317\n",
      "train.en 62000 133317\n",
      "train.en 63000 133317\n",
      "train.en 64000 133317\n",
      "train.en 65000 133317\n",
      "train.en 66000 133317\n",
      "train.en 67000 133317\n",
      "train.en 68000 133317\n",
      "train.en 69000 133317\n",
      "train.en 70000 133317\n",
      "train.en 71000 133317\n",
      "train.en 72000 133317\n",
      "train.en 73000 133317\n",
      "train.en 74000 133317\n",
      "train.en 75000 133317\n",
      "train.en 76000 133317\n",
      "train.en 77000 133317\n",
      "train.en 78000 133317\n",
      "train.en 79000 133317\n",
      "train.en 80000 133317\n",
      "train.en 81000 133317\n",
      "train.en 82000 133317\n",
      "train.en 83000 133317\n",
      "train.en 84000 133317\n",
      "train.en 85000 133317\n",
      "train.en 86000 133317\n",
      "train.en 87000 133317\n",
      "train.en 88000 133317\n",
      "train.en 89000 133317\n",
      "train.en 90000 133317\n",
      "train.en 91000 133317\n",
      "train.en 92000 133317\n",
      "train.en 93000 133317\n",
      "train.en 94000 133317\n",
      "train.en 95000 133317\n",
      "train.en 96000 133317\n",
      "train.en 97000 133317\n",
      "train.en 98000 133317\n",
      "train.en 99000 133317\n",
      "train.en 100000 133317\n",
      "train.en 101000 133317\n",
      "train.en 102000 133317\n",
      "train.en 103000 133317\n",
      "train.en 104000 133317\n",
      "train.en 105000 133317\n",
      "train.en 106000 133317\n",
      "train.en 107000 133317\n",
      "train.en 108000 133317\n",
      "train.en 109000 133317\n",
      "train.en 110000 133317\n",
      "train.en 111000 133317\n",
      "train.en 112000 133317\n",
      "train.en 113000 133317\n",
      "train.en 114000 133317\n",
      "train.en 115000 133317\n",
      "train.en 116000 133317\n",
      "train.en 117000 133317\n",
      "train.en 118000 133317\n",
      "train.en 119000 133317\n",
      "train.en 120000 133317\n",
      "train.en 121000 133317\n",
      "train.en 122000 133317\n",
      "train.en 123000 133317\n",
      "train.en 124000 133317\n",
      "train.en 125000 133317\n",
      "train.en 126000 133317\n",
      "train.en 127000 133317\n",
      "train.en 128000 133317\n",
      "train.en 129000 133317\n",
      "train.en 130000 133317\n",
      "train.en 131000 133317\n",
      "train.en 132000 133317\n",
      "train.en 133000 133317\n",
      "dev.en 1000 1268\n",
      "test.en 1000 1553\n"
     ]
    }
   ],
   "source": [
    "# # Please find the original tokenizing code provided by Elman Mansimov in the following link:\n",
    "# # https://github.com/derincen/neural-machine-translation/tree/master/data/tokens_and_preprocessing_em/preprocess_translation\n",
    "\n",
    "# def tokenize_vi(f_names, f_out_names):\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             tok_lines.write(word_tokenize(sentence, format=\"text\") + '\\n')\n",
    "#         tok_lines.close()\n",
    "\n",
    "# def tokenize_en(f_names, f_out_names):\n",
    "#     tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             # replaced tokenizer(sentence) with str(tokenizer(sentence)) to avoid \n",
    "#             # type error while joining\n",
    "#             tok_lines.write(' '.join(str(tokenizer(sentence))) + '\\n')\n",
    "#         tok_lines.close()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     root = '../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-vi-en-processed/'\n",
    "#     tokenize_vi([os.path.join(root, 'train.vi'), os.path.join(root, 'dev.vi'), \n",
    "#                  os.path.join(root, 'test.vi')],\\\n",
    "#                [os.path.join(root, 'train.tok.vi'), os.path.join(root, 'dev.tok.vi'), \n",
    "#                 os.path.join(root, 'test.tok.vi')])\n",
    "\n",
    "#     tokenize_en([os.path.join(root, 'train.en'), os.path.join(root, 'dev.en'), \n",
    "#                  os.path.join(root, 'test.en')],\\\n",
    "#                 [os.path.join(root, 'train.tok.en'), os.path.join(root, 'dev.tok.en'), \n",
    "#                  os.path.join(root, 'test.tok.en')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 133317 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 16144\n",
      "en 47568\n",
      "Reading lines...\n",
      "Read 1268 sentence pairs\n",
      "Trimmed to 1268 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 1370\n",
      "en 3816\n",
      "Reading lines...\n",
      "Read 1553 sentence pairs\n",
      "Trimmed to 1553 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 1325\n",
      "en 3619\n"
     ]
    }
   ],
   "source": [
    "# Format: languagepair_language_dataset\n",
    "# Train \n",
    "vien_vi_train, vien_en_train, vi_en_train_pairs = prepareData('vi', 'en', False, dataset=\"train\")\n",
    "# Dev \n",
    "vien_vi_dev, vien_en_dev, vi_en_dev_pairs = prepareData('vi', 'en', False, dataset=\"dev\")\n",
    "# Test\n",
    "vien_vi_test, vien_en_test, vi_en_test_pairs = prepareData('vi', 'en', False, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Chinese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.zh 1000 1261\n",
      "test.zh 1000 1397\n",
      "train.zh 1000 213377\n",
      "train.zh 2000 213377\n",
      "train.zh 3000 213377\n",
      "train.zh 4000 213377\n",
      "train.zh 5000 213377\n",
      "train.zh 6000 213377\n",
      "train.zh 7000 213377\n",
      "train.zh 8000 213377\n",
      "train.zh 9000 213377\n",
      "train.zh 10000 213377\n",
      "train.zh 11000 213377\n",
      "train.zh 12000 213377\n",
      "train.zh 13000 213377\n",
      "train.zh 14000 213377\n",
      "train.zh 15000 213377\n",
      "train.zh 16000 213377\n",
      "train.zh 17000 213377\n",
      "train.zh 18000 213377\n",
      "train.zh 19000 213377\n",
      "train.zh 20000 213377\n",
      "train.zh 21000 213377\n",
      "train.zh 22000 213377\n",
      "train.zh 23000 213377\n",
      "train.zh 24000 213377\n",
      "train.zh 25000 213377\n",
      "train.zh 26000 213377\n",
      "train.zh 27000 213377\n",
      "train.zh 28000 213377\n",
      "train.zh 29000 213377\n",
      "train.zh 30000 213377\n",
      "train.zh 31000 213377\n",
      "train.zh 32000 213377\n",
      "train.zh 33000 213377\n",
      "train.zh 34000 213377\n",
      "train.zh 35000 213377\n",
      "train.zh 36000 213377\n",
      "train.zh 37000 213377\n",
      "train.zh 38000 213377\n",
      "train.zh 39000 213377\n",
      "train.zh 40000 213377\n",
      "train.zh 41000 213377\n",
      "train.zh 42000 213377\n",
      "train.zh 43000 213377\n",
      "train.zh 44000 213377\n",
      "train.zh 45000 213377\n",
      "train.zh 46000 213377\n",
      "train.zh 47000 213377\n",
      "train.zh 48000 213377\n",
      "train.zh 49000 213377\n",
      "train.zh 50000 213377\n",
      "train.zh 51000 213377\n",
      "train.zh 52000 213377\n",
      "train.zh 53000 213377\n",
      "train.zh 54000 213377\n",
      "train.zh 55000 213377\n",
      "train.zh 56000 213377\n",
      "train.zh 57000 213377\n",
      "train.zh 58000 213377\n",
      "train.zh 59000 213377\n",
      "train.zh 60000 213377\n",
      "train.zh 61000 213377\n",
      "train.zh 62000 213377\n",
      "train.zh 63000 213377\n",
      "train.zh 64000 213377\n",
      "train.zh 65000 213377\n",
      "train.zh 66000 213377\n",
      "train.zh 67000 213377\n",
      "train.zh 68000 213377\n",
      "train.zh 69000 213377\n",
      "train.zh 70000 213377\n",
      "train.zh 71000 213377\n",
      "train.zh 72000 213377\n",
      "train.zh 73000 213377\n",
      "train.zh 74000 213377\n",
      "train.zh 75000 213377\n",
      "train.zh 76000 213377\n",
      "train.zh 77000 213377\n",
      "train.zh 78000 213377\n",
      "train.zh 79000 213377\n",
      "train.zh 80000 213377\n",
      "train.zh 81000 213377\n",
      "train.zh 82000 213377\n",
      "train.zh 83000 213377\n",
      "train.zh 84000 213377\n",
      "train.zh 85000 213377\n",
      "train.zh 86000 213377\n",
      "train.zh 87000 213377\n",
      "train.zh 88000 213377\n",
      "train.zh 89000 213377\n",
      "train.zh 90000 213377\n",
      "train.zh 91000 213377\n",
      "train.zh 92000 213377\n",
      "train.zh 93000 213377\n",
      "train.zh 94000 213377\n",
      "train.zh 95000 213377\n",
      "train.zh 96000 213377\n",
      "train.zh 97000 213377\n",
      "train.zh 98000 213377\n",
      "train.zh 99000 213377\n",
      "train.zh 100000 213377\n",
      "train.zh 101000 213377\n",
      "train.zh 102000 213377\n",
      "train.zh 103000 213377\n",
      "train.zh 104000 213377\n",
      "train.zh 105000 213377\n",
      "train.zh 106000 213377\n",
      "train.zh 107000 213377\n",
      "train.zh 108000 213377\n",
      "train.zh 109000 213377\n",
      "train.zh 110000 213377\n",
      "train.zh 111000 213377\n",
      "train.zh 112000 213377\n",
      "train.zh 113000 213377\n",
      "train.zh 114000 213377\n",
      "train.zh 115000 213377\n",
      "train.zh 116000 213377\n",
      "train.zh 117000 213377\n",
      "train.zh 118000 213377\n",
      "train.zh 119000 213377\n",
      "train.zh 120000 213377\n",
      "train.zh 121000 213377\n",
      "train.zh 122000 213377\n",
      "train.zh 123000 213377\n",
      "train.zh 124000 213377\n",
      "train.zh 125000 213377\n",
      "train.zh 126000 213377\n",
      "train.zh 127000 213377\n",
      "train.zh 128000 213377\n",
      "train.zh 129000 213377\n",
      "train.zh 130000 213377\n",
      "train.zh 131000 213377\n",
      "train.zh 132000 213377\n",
      "train.zh 133000 213377\n",
      "train.zh 134000 213377\n",
      "train.zh 135000 213377\n",
      "train.zh 136000 213377\n",
      "train.zh 137000 213377\n",
      "train.zh 138000 213377\n",
      "train.zh 139000 213377\n",
      "train.zh 140000 213377\n",
      "train.zh 141000 213377\n",
      "train.zh 142000 213377\n",
      "train.zh 143000 213377\n",
      "train.zh 144000 213377\n",
      "train.zh 145000 213377\n",
      "train.zh 146000 213377\n",
      "train.zh 147000 213377\n",
      "train.zh 148000 213377\n",
      "train.zh 149000 213377\n",
      "train.zh 150000 213377\n",
      "train.zh 151000 213377\n",
      "train.zh 152000 213377\n",
      "train.zh 153000 213377\n",
      "train.zh 154000 213377\n",
      "train.zh 155000 213377\n",
      "train.zh 156000 213377\n",
      "train.zh 157000 213377\n",
      "train.zh 158000 213377\n",
      "train.zh 159000 213377\n",
      "train.zh 160000 213377\n",
      "train.zh 161000 213377\n",
      "train.zh 162000 213377\n",
      "train.zh 163000 213377\n",
      "train.zh 164000 213377\n",
      "train.zh 165000 213377\n",
      "train.zh 166000 213377\n",
      "train.zh 167000 213377\n",
      "train.zh 168000 213377\n",
      "train.zh 169000 213377\n",
      "train.zh 170000 213377\n",
      "train.zh 171000 213377\n",
      "train.zh 172000 213377\n",
      "train.zh 173000 213377\n",
      "train.zh 174000 213377\n",
      "train.zh 175000 213377\n",
      "train.zh 176000 213377\n",
      "train.zh 177000 213377\n",
      "train.zh 178000 213377\n",
      "train.zh 179000 213377\n",
      "train.zh 180000 213377\n",
      "train.zh 181000 213377\n",
      "train.zh 182000 213377\n",
      "train.zh 183000 213377\n",
      "train.zh 184000 213377\n",
      "train.zh 185000 213377\n",
      "train.zh 186000 213377\n",
      "train.zh 187000 213377\n",
      "train.zh 188000 213377\n",
      "train.zh 189000 213377\n",
      "train.zh 190000 213377\n",
      "train.zh 191000 213377\n",
      "train.zh 192000 213377\n",
      "train.zh 193000 213377\n",
      "train.zh 194000 213377\n",
      "train.zh 195000 213377\n",
      "train.zh 196000 213377\n",
      "train.zh 197000 213377\n",
      "train.zh 198000 213377\n",
      "train.zh 199000 213377\n",
      "train.zh 200000 213377\n",
      "train.zh 201000 213377\n",
      "train.zh 202000 213377\n",
      "train.zh 203000 213377\n",
      "train.zh 204000 213377\n",
      "train.zh 205000 213377\n",
      "train.zh 206000 213377\n",
      "train.zh 207000 213377\n",
      "train.zh 208000 213377\n",
      "train.zh 209000 213377\n",
      "train.zh 210000 213377\n",
      "train.zh 211000 213377\n",
      "train.zh 212000 213377\n",
      "train.zh 213000 213377\n"
     ]
    }
   ],
   "source": [
    "# # Please find the original tokenizing code provided by Elman Mansimov in the following link:\n",
    "# # https://github.com/derincen/neural-machine-translation/tree/master/data/tokens_and_preprocessing_em/preprocess_translation\n",
    "\n",
    "# def tokenize_zh(f_names, f_out_names):\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             tok_lines.write(' '.join(jieba.cut(sentence, cut_all=True)))\n",
    "#         tok_lines.close()\n",
    "\n",
    "# def tokenize_en(f_names, f_out_names):\n",
    "#     tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             # replaced tokenizer(sentence) with str(tokenizer(sentence)) to avoid \n",
    "#             # type error while joining\n",
    "#             tok_lines.write(' '.join(str(tokenizer(sentence))) + '\\n')\n",
    "#         tok_lines.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     root = '../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-zh-en-processed/'\n",
    "#     tokenize_zh([os.path.join(root, 'dev.zh'), os.path.join(root, 'test.zh'), os.path.join(root, 'train.zh')],\\\n",
    "#                 [os.path.join(root, 'dev.tok.zh'), os.path.join(root, 'test.tok.zh'), os.path.join(root, 'train.tok.zh')])\n",
    "\n",
    "# #     tokenize_en([os.path.join(root, 'dev.en'), os.path.join(root, 'test.en'), os.path.join(root, 'train.en')],\\\n",
    "# #                [os.path.join(root, 'dev.tok.en'), os.path.join(root, 'test.tok.en'), os.path.join(root, 'train.tok.en')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213376 sentence pairs\n",
      "Trimmed to 213376 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 88917\n",
      "en 59329\n",
      "Reading lines...\n",
      "Read 1261 sentence pairs\n",
      "Trimmed to 1261 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 6132\n",
      "en 3916\n",
      "Reading lines...\n",
      "Read 1397 sentence pairs\n",
      "Trimmed to 1397 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 5214\n",
      "en 3423\n"
     ]
    }
   ],
   "source": [
    "# Format: languagepair_language_dataset\n",
    "# Train \n",
    "zhen_zh_train, zhen_en_train, zh_en_train_pairs = prepareData('zh', 'en', False, dataset=\"train\")\n",
    "# Dev \n",
    "zhen_zh_dev, zhen_en_dev, zh_en_dev_pairs = prepareData('zh', 'en', False, dataset=\"dev\")\n",
    "# Test\n",
    "zhen_zh_test, zhen_en_test, zh_en_test_pairs = prepareData('zh', 'en', False, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我们 将 用 一些 影片 来讲 讲述 一些 深海 海里 的 故事  ',\n",
       " 'And we apos re going to tell you some stories from the sea here in video .']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_en_train_pairs[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Check Source & Target Vocabs\n",
    "\n",
    "Since the source and target languages can have very different table lookup layers, it's good practice to have separate vocabularies for each. Thus, we build vocabularies for each language that we will be using. \n",
    "\n",
    "In the first class (Lang) of this section, we have already defined vocabularies for all languages. So, there is no need to redefine another function. We chech each vocabulary below.\n",
    "\n",
    "#### Chinese Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in Chinese training corpus is 88917\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in Chinese training corpus is \" + str(zhen_zh_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10479"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_zh_train.word2index[\"格\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'格'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_zh_train.index2word[10479]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vietnamese Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in Vietnamese training corpus is 16144\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in Vietnamese training corpus is \" + str(vien_vi_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6752"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_vi_train.word2index[\"Hamburger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hamburger'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_vi_train.index2word[6752]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Vocabulary for Zh-En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in English training corpus for Zh-En is 59329\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in English training corpus for Zh-En is \" + str(zhen_en_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1451"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_en_train.word2index[\"translate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'translate'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_en_train.index2word[1451]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Vocabulary for Vi-En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in English training corpus for Vi-En is 47568\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in English training corpus for Vi-En is \" + str(vien_en_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "847"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_train.word2index[\"machine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_train.index2word[847]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Prepare Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(paired_tokens, \n",
    "                        lang1_token2id_vocab,\n",
    "                        lang2_token2id_vocab):\n",
    "    \"\"\"Takes as input:\n",
    "    - paired_tokens: a list of sentence pairs that consist of source & target lang sentences.\n",
    "    - lang1_token2id_vocab: token2index vocabulary for the first language. \n",
    "                            Get by method Lang_dataset.word2index\n",
    "    - lang2_token2id_vocab: token2index vocabulary for the second language. \n",
    "                            Get by method Lang_dataset.word2index\n",
    "                            \n",
    "    Returns:\n",
    "    - indices_data_lang_1, indices_data_lang2: A list of lists where each sub-list holds corresponding indices for each\n",
    "                                               token in the sentence.\"\"\"\n",
    "    indices_data_lang_1, indices_data_lang_2 = [], []\n",
    "    vocabs = [lang1_token2id_vocab, lang2_token2id_vocab]\n",
    "    \n",
    "    # lang1\n",
    "    for t in range(len(paired_tokens)):\n",
    "        index_list = [vocabs[0][token] if token in vocabs[0] else UNK_IDX for token in paired_tokens[t][0]]\n",
    "        indices_data_lang_1.append(index_list)\n",
    "    # lang2\n",
    "    for t in range(len(paired_tokens)):\n",
    "        index_list = [vocabs[1][token] if token in vocabs[1] else UNK_IDX for token in paired_tokens[t][1]]\n",
    "        indices_data_lang_2.append(index_list)\n",
    "        \n",
    "    return indices_data_lang_1, indices_data_lang_2\n",
    "\n",
    "# train indices\n",
    "zhen_zh_train_indices, zhen_en_train_indices = token2index_dataset(zh_en_train_pairs,\n",
    "                                                                   zhen_zh_train.word2index,\n",
    "                                                                   zhen_en_train.word2index)\n",
    "\n",
    "vien_vi_train_indices, vien_en_train_indices = token2index_dataset(vi_en_train_pairs,\n",
    "                                                                   vien_vi_train.word2index,\n",
    "                                                                   vien_en_train.word2index)\n",
    "\n",
    "# dev indices\n",
    "zhen_zh_dev_indices, zhen_en_dev_indices = token2index_dataset(zh_en_dev_pairs,\n",
    "                                                               zhen_zh_dev.word2index,\n",
    "                                                               zhen_en_dev.word2index)\n",
    "\n",
    "vien_vi_dev_indices, vien_en_dev_indices = token2index_dataset(vi_en_dev_pairs,\n",
    "                                                               vien_vi_dev.word2index,\n",
    "                                                               vien_en_dev.word2index)\n",
    "\n",
    "# test indices\n",
    "zhen_zh_test_indices, zhen_en_test_indices = token2index_dataset(zh_en_test_pairs,\n",
    "                                                                 zhen_zh_test.word2index,\n",
    "                                                                 zhen_en_test.word2index)\n",
    "\n",
    "vien_vi_test_indices, vien_en_test_indices = token2index_dataset(vi_en_test_pairs,\n",
    "                                                                 vien_vi_test.word2index,\n",
    "                                                                 vien_en_test.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese training set length = 213376\n",
      "Chinese-English (En) training set length = 213376\n",
      "\n",
      "Vietnamese training set length = 133317\n",
      "Vietnamese-English (En) training set length = 133317\n",
      "\n",
      "Chinese dev set length = 1261\n",
      "Chinese-English (En) dev set length = 1261\n",
      "\n",
      "Vietnamese dev set length = 1268\n",
      "Vietnamese-English (En) dev set length = 1268\n",
      "\n",
      "Chinese test set length = 1397\n",
      "Chinese-English (En) test set length = 1397\n",
      "\n",
      "Vietnamese test set length = 1553\n",
      "Vietnamese-English (En) test set length = 1553\n"
     ]
    }
   ],
   "source": [
    "# check length\n",
    "# train\n",
    "print (\"Chinese training set length = \"+str(len(zhen_zh_train_indices)))\n",
    "print (\"Chinese-English (En) training set length = \"+str(len(zhen_en_train_indices)))\n",
    "print (\"\\nVietnamese training set length = \"+str(len(vien_vi_train_indices)))\n",
    "print (\"Vietnamese-English (En) training set length = \"+str(len(vien_en_train_indices)))\n",
    "# dev\n",
    "print (\"\\nChinese dev set length = \"+str(len(zhen_zh_dev_indices)))\n",
    "print (\"Chinese-English (En) dev set length = \"+str(len(zhen_en_dev_indices)))\n",
    "print (\"\\nVietnamese dev set length = \"+str(len(vien_vi_dev_indices)))\n",
    "print (\"Vietnamese-English (En) dev set length = \"+str(len(vien_en_dev_indices)))\n",
    "# test\n",
    "print (\"\\nChinese test set length = \"+str(len(zhen_zh_test_indices)))\n",
    "print (\"Chinese-English (En) test set length = \"+str(len(zhen_en_test_indices)))\n",
    "print (\"\\nVietnamese test set length = \"+str(len(vien_vi_test_indices)))\n",
    "print (\"Vietnamese-English (En) test set length = \"+str(len(vien_en_test_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Evaluation Metric\n",
    "\n",
    "We use BLEU as the evaluation metric. Specifically, we focus on the corpus-level BLEU function. \n",
    "\n",
    "The code for BLEU is taken from https://github.com/mjpost/sacreBLEU/blob/master/sacrebleu.py#L1022-L1080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /Users/derin/anaconda/lib/python3.6/site-packages (1.2.12)\n",
      "Requirement already satisfied: typing in /Users/derin/anaconda/lib/python3.6/site-packages (from sacrebleu) (3.6.6)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Beam Search Algorithm\n",
    "\n",
    "In this section, we implement the Beam Search algorithm in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 0], 0.025600863289563108]\n",
      "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 1], 0.03384250043584397]\n",
      "[[4, 0, 4, 0, 4, 0, 4, 0, 3, 0], 0.03384250043584397]\n"
     ]
    }
   ],
   "source": [
    "## python example\n",
    "\n",
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    " \n",
    "# beam search\n",
    "def beam_search_decoder(data, k):\n",
    "\tsequences = [[list(), 1.0]]\n",
    "\t# walk over each step in sequence\n",
    "\tfor row in data:\n",
    "\t\tall_candidates = list()\n",
    "\t\t# expand each current candidate\n",
    "\t\tfor i in range(len(sequences)):\n",
    "\t\t\tseq, score = sequences[i]\n",
    "\t\t\tfor j in range(len(row)):\n",
    "\t\t\t\tcandidate = [seq + [j], score * -log(row[j])]\n",
    "\t\t\t\tall_candidates.append(candidate)\n",
    "\t\t# order all candidates by score\n",
    "\t\tordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "\t\t# select k best\n",
    "\t\tsequences = ordered[:k]\n",
    "\treturn sequences\n",
    " \n",
    "# define a sequence of 10 words over a vocab of 5 words\n",
    "data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1]]\n",
    "data = array(data)\n",
    "# decode sequence\n",
    "result = beam_search_decoder(data, 3)\n",
    "# print result\n",
    "for seq in result:\n",
    "\tprint(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code borrowed from \n",
    "# \n",
    "\n",
    "class Beam(object):\n",
    "    \"\"\"\n",
    "    Class for managing the internals of the beam search process.\n",
    "    Takes care of beams, back pointers, and scores.\n",
    "    Args:\n",
    "       size (int): beam size\n",
    "       pad, bos, eos (int): indices of padding, beginning, and ending.\n",
    "       n_best (int): nbest size to use\n",
    "       cuda (bool): use gpu\n",
    "       global_scorer (:obj:`GlobalScorer`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, pad, bos, eos,\n",
    "                 n_best=1, cuda=False,\n",
    "                 global_scorer=None,\n",
    "                 min_length=0,\n",
    "                 stepwise_penalty=False,\n",
    "                 block_ngram_repeat=0,\n",
    "                 exclusion_tokens=set()):\n",
    "\n",
    "        self.size = size\n",
    "        self.tt = torch.cuda if cuda else torch\n",
    "\n",
    "        # The score for each translation on the beam.\n",
    "        self.scores = self.tt.FloatTensor(size).zero_()\n",
    "        self.all_scores = []\n",
    "\n",
    "        # The backpointers at each time-step.\n",
    "        self.prev_ks = []\n",
    "\n",
    "        # The outputs at each time-step.\n",
    "        self.next_ys = [self.tt.LongTensor(size)\n",
    "                        .fill_(pad)]\n",
    "        self.next_ys[0][0] = bos\n",
    "\n",
    "        # Has EOS topped the beam yet.\n",
    "        self._eos = eos\n",
    "        self.eos_top = False\n",
    "\n",
    "        # The attentions (matrix) for each time.\n",
    "        self.attn = []\n",
    "\n",
    "        # Time and k pair for finished.\n",
    "        self.finished = []\n",
    "        self.n_best = n_best\n",
    "\n",
    "        # Information for global scoring.\n",
    "        self.global_scorer = global_scorer\n",
    "        self.global_state = {}\n",
    "\n",
    "        # Minimum prediction length\n",
    "        self.min_length = min_length\n",
    "\n",
    "        # Apply Penalty at every step\n",
    "        self.stepwise_penalty = stepwise_penalty\n",
    "        self.block_ngram_repeat = block_ngram_repeat\n",
    "        self.exclusion_tokens = exclusion_tokens\n",
    "\n",
    "    def get_current_state(self):\n",
    "        \"Get the outputs for the current timestep.\"\n",
    "        return self.next_ys[-1]\n",
    "\n",
    "    def get_current_origin(self):\n",
    "        \"Get the backpointers for the current timestep.\"\n",
    "        return self.prev_ks[-1]\n",
    "\n",
    "    def advance(self, word_probs, attn_out):\n",
    "        \"\"\"\n",
    "        Given prob over words for every last beam `wordLk` and attention\n",
    "        `attn_out`: Compute and update the beam search.\n",
    "        Parameters:\n",
    "        * `word_probs`- probs of advancing from the last step (K x words)\n",
    "        * `attn_out`- attention at the last step\n",
    "        Returns: True if beam search is complete.\n",
    "        \"\"\"\n",
    "        num_words = word_probs.size(1)\n",
    "        if self.stepwise_penalty:\n",
    "            self.global_scorer.update_score(self, attn_out)\n",
    "        # force the output to be longer than self.min_length\n",
    "        cur_len = len(self.next_ys)\n",
    "        if cur_len < self.min_length:\n",
    "            for k in range(len(word_probs)):\n",
    "                word_probs[k][self._eos] = -1e20\n",
    "        # Sum the previous scores.\n",
    "        if len(self.prev_ks) > 0:\n",
    "            beam_scores = word_probs + \\\n",
    "                self.scores.unsqueeze(1).expand_as(word_probs)\n",
    "            # Don't let EOS have children.\n",
    "            for i in range(self.next_ys[-1].size(0)):\n",
    "                if self.next_ys[-1][i] == self._eos:\n",
    "                    beam_scores[i] = -1e20\n",
    "\n",
    "            # Block ngram repeats\n",
    "            if self.block_ngram_repeat > 0:\n",
    "                ngrams = []\n",
    "                le = len(self.next_ys)\n",
    "                for j in range(self.next_ys[-1].size(0)):\n",
    "                    hyp, _ = self.get_hyp(le - 1, j)\n",
    "                    ngrams = set()\n",
    "                    fail = False\n",
    "                    gram = []\n",
    "                    for i in range(le - 1):\n",
    "                        # Last n tokens, n = block_ngram_repeat\n",
    "                        gram = (gram +\n",
    "                                [hyp[i].item()])[-self.block_ngram_repeat:]\n",
    "                        # Skip the blocking if it is in the exclusion list\n",
    "                        if set(gram) & self.exclusion_tokens:\n",
    "                            continue\n",
    "                        if tuple(gram) in ngrams:\n",
    "                            fail = True\n",
    "                        ngrams.add(tuple(gram))\n",
    "                    if fail:\n",
    "                        beam_scores[j] = -10e20\n",
    "        else:\n",
    "            beam_scores = word_probs[0]\n",
    "        flat_beam_scores = beam_scores.view(-1)\n",
    "        best_scores, best_scores_id = flat_beam_scores.topk(self.size, 0,\n",
    "                                                            True, True)\n",
    "\n",
    "        self.all_scores.append(self.scores)\n",
    "        self.scores = best_scores\n",
    "\n",
    "        # best_scores_id is flattened beam x word array, so calculate which\n",
    "        # word and beam each score came from\n",
    "        prev_k = best_scores_id / num_words\n",
    "        self.prev_ks.append(prev_k)\n",
    "        self.next_ys.append((best_scores_id - prev_k * num_words))\n",
    "        self.attn.append(attn_out.index_select(0, prev_k))\n",
    "        self.global_scorer.update_global_state(self)\n",
    "\n",
    "        for i in range(self.next_ys[-1].size(0)):\n",
    "            if self.next_ys[-1][i] == self._eos:\n",
    "                global_scores = self.global_scorer.score(self, self.scores)\n",
    "                s = global_scores[i]\n",
    "                self.finished.append((s, len(self.next_ys) - 1, i))\n",
    "\n",
    "        # End condition is when top-of-beam is EOS and no global score.\n",
    "        if self.next_ys[-1][0] == self._eos:\n",
    "            self.all_scores.append(self.scores)\n",
    "            self.eos_top = True\n",
    "\n",
    "    def done(self):\n",
    "        return self.eos_top and len(self.finished) >= self.n_best\n",
    "\n",
    "    def sort_finished(self, minimum=None):\n",
    "        if minimum is not None:\n",
    "            i = 0\n",
    "            # Add from beam until we have minimum outputs.\n",
    "            while len(self.finished) < minimum:\n",
    "                global_scores = self.global_scorer.score(self, self.scores)\n",
    "                s = global_scores[i]\n",
    "                self.finished.append((s, len(self.next_ys) - 1, i))\n",
    "                i += 1\n",
    "\n",
    "        self.finished.sort(key=lambda a: -a[0])\n",
    "        scores = [sc for sc, _, _ in self.finished]\n",
    "        ks = [(t, k) for _, t, k in self.finished]\n",
    "        return scores, ks\n",
    "\n",
    "    def get_hyp(self, timestep, k):\n",
    "        \"\"\"\n",
    "        Walk back to construct the full hypothesis.\n",
    "        \"\"\"\n",
    "        hyp, attn = [], []\n",
    "        for j in range(len(self.prev_ks[:timestep]) - 1, -1, -1):\n",
    "            hyp.append(self.next_ys[j + 1][k])\n",
    "            attn.append(self.attn[j][k])\n",
    "            k = self.prev_ks[j][k]\n",
    "        return hyp[::-1], torch.stack(attn[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model\n",
    "\n",
    "1. Recurrent neural network based encoder-decoder without attention\n",
    "2. Recurrent neural network based encoder-decoder with attention\n",
    "2. Replace the recurrent encoder with either convolutional or self-attention based encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: RNN-based Encoder-Decoder without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 RNN-based Encoder-Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoder Replacement with Eonvolutional or Self-attention-based Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Fully self-attention Translation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Multilingual Translation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
