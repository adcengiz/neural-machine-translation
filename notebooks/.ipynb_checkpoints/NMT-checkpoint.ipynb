{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Spring 2018 NLP Class Project: Neural Machine Translation</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pdb\n",
    "import os\n",
    "from underthesea import word_tokenize\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install spacy && python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Project Overview\n",
    "\n",
    "The goal of this project is to build a neural machine translation system and experience how recent advances have made their way. Each team will build the following sequence of neural translation systems for two language pairs, __Vietnamese (Vi)→English (En)__ and __Chinese (Zh)→En__ (prepared corpora is be provided):\n",
    "\n",
    "1. Recurrent neural network based encoder-decoder without attention\n",
    "2. Recurrent neural network based encoder-decoder with attention\n",
    "2. Replace the recurrent encoder with either convolutional or self-attention based encoder.\n",
    "4. [Optional] Build either or both fully self-attention translation system or/and multilingual translation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Upload & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of sentence\n",
    "SOS_token = 0\n",
    "# end of sentence\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    \"\"\"About \"NFC\" and \"NFD\": \n",
    "    \n",
    "    For each character, there are two normal forms: normal form C \n",
    "    and normal form D. Normal form D (NFD) is also known as canonical \n",
    "    decomposition, and translates each character into its decomposed form. \n",
    "    Normal form C (NFC) first applies a canonical decomposition, then composes \n",
    "    pre-combined characters again.\n",
    "    \n",
    "    About unicodedata.category: \n",
    "    \n",
    "    Returns the general category assigned to the Unicode character \n",
    "    unichr as string.\"\"\"\n",
    "    \n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Trim\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False,\n",
    "             dataset=\"train\"):\n",
    "    \n",
    "    \"\"\"Takes as input;\n",
    "    - lang1, lang2: either (vi, en) or (zh, en)\n",
    "    - dataset: one of (\"train\",\"dev\",\"test\")\"\"\"\n",
    "    print(\"Reading lines...\")\n",
    "    eos = [\".\",\"?\",\"!\",\"\\n\"]\n",
    "    # Read the pretokenized lang1 file and split into lines\n",
    "    lang1_lines = open(\"../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-%s-%s-processed/%s.tok.%s\" % (lang1, lang2, dataset, lang1), encoding=\"utf-8\").\\\n",
    "        read().strip().split(\"\\n\")\n",
    "    # Read the lang2 file and split into lines\n",
    "    lang2_lines = open(\"../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-%s-%s-processed/%s.tok.%s\" % (lang1, lang2, dataset, lang2), encoding=\"utf-8\").\\\n",
    "        read().strip().split(\"\\n\")\n",
    "    \n",
    "    # create sentence pairs (lists of length 2 that consist of string pairs)\n",
    "    # e.g. [\"And we &apos;re going to tell you some stories from the sea here in video .\",\n",
    "    #       \"我们 将 用 一些 影片 来讲 讲述 一些 深海 海里 的 故事  \"]\n",
    "    # check if there are the same number of sentences in each set\n",
    "    assert len(lang1_lines) == len(lang2_lines), \"Two languages must have the same number of sentences. \"+ str(len(lang1_lines)) + \" sentences were passed for \" + str(lang1) + \".\" + str(len(lang2_lines)) + \" sentences were passed for \" + str(lang2)+\".\"\n",
    "    # normalize if not Chinese, Chinese normalization is already handeled\n",
    "    if lang1 == \"zh\":\n",
    "        lang1_lines = lang1_lines\n",
    "    else:\n",
    "        lang1_lines = [normalizeString(s) for s in lang1_lines]\n",
    "    lang2_lines = [normalizeString(s) for s in lang2_lines]\n",
    "    # construct pairs\n",
    "    pair_ran = range(len(lang1_lines))\n",
    "    pairs = [[lang1_lines[i]] + [lang2_lines[i]] for i in pair_ran]\n",
    "    \n",
    "#     # Split every line into pairs and normalize\n",
    "#     pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 133317 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 16144\n",
      "en 47568\n",
      "['Lam the nao chung toi tham gia vao nhung tran au lon ?', 'How can we go to a greater battle ? quot ']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False, dataset=\"train\"):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse, dataset=dataset)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# example\n",
    "input_lang, output_lang, pairs = prepareData('vi', 'en', False, dataset=\"train\")\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213376 sentence pairs\n",
      "Trimmed to 213376 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 88917\n",
      "en 59329\n",
      "['就 在 那么 大 的 空间 里   有人 写出 了   一个 完整 的 飞行 模拟 模拟程序 程序 ', 'And in that amount of memory someone programmed a full flight simulation program .']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('zh', 'en', False, dataset=\"train\")\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vietnamese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Please find the original tokenizing code provided by Elman Mansimov in the following link:\n",
    "# # https://github.com/derincen/neural-machine-translation/tree/master/data/tokens_and_preprocessing_em/preprocess_translation\n",
    "\n",
    "# def tokenize_vi(f_names, f_out_names):\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             tok_lines.write(word_tokenize(sentence, format=\"text\") + '\\n')\n",
    "#         tok_lines.close()\n",
    "\n",
    "# def tokenize_en(f_names, f_out_names):\n",
    "#     tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             # replaced tokenizer(sentence) with str(tokenizer(sentence)) to avoid \n",
    "#             # type error while joining\n",
    "#             tok_lines.write(' '.join(str(tokenizer(sentence))) + '\\n')\n",
    "#         tok_lines.close()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     root = '../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-vi-en-processed/'\n",
    "#     tokenize_vi([os.path.join(root, 'train.vi'), os.path.join(root, 'dev.vi'), \n",
    "#                  os.path.join(root, 'test.vi')],\\\n",
    "#                [os.path.join(root, 'train.tok.vi'), os.path.join(root, 'dev.tok.vi'), \n",
    "#                 os.path.join(root, 'test.tok.vi')])\n",
    "\n",
    "#     tokenize_en([os.path.join(root, 'train.en'), os.path.join(root, 'dev.en'), \n",
    "#                  os.path.join(root, 'test.en')],\\\n",
    "#                 [os.path.join(root, 'train.tok.en'), os.path.join(root, 'dev.tok.en'), \n",
    "#                  os.path.join(root, 'test.tok.en')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 133317 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 16144\n",
      "en 47568\n",
      "Reading lines...\n",
      "Read 1268 sentence pairs\n",
      "Trimmed to 1268 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 1370\n",
      "en 3816\n",
      "Reading lines...\n",
      "Read 1553 sentence pairs\n",
      "Trimmed to 1553 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 1325\n",
      "en 3619\n"
     ]
    }
   ],
   "source": [
    "# Format: languagepair_language_dataset\n",
    "# Train \n",
    "vien_vi_train, vien_en_train, vi_en_train_pairs = prepareData('vi', 'en', False, dataset=\"train\")\n",
    "# Dev \n",
    "vien_vi_dev, vien_en_dev, vi_en_dev_pairs = prepareData('vi', 'en', False, dataset=\"dev\")\n",
    "# Test\n",
    "vien_vi_test, vien_en_test, vi_en_test_pairs = prepareData('vi', 'en', False, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Chinese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Please find the original tokenizing code provided by Elman Mansimov in the following link:\n",
    "# # https://github.com/derincen/neural-machine-translation/tree/master/data/tokens_and_preprocessing_em/preprocess_translation\n",
    "\n",
    "# def tokenize_zh(f_names, f_out_names):\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             tok_lines.write(' '.join(jieba.cut(sentence, cut_all=True)))\n",
    "#         tok_lines.close()\n",
    "\n",
    "# def tokenize_en(f_names, f_out_names):\n",
    "#     tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "#     for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "#         lines = open(f_name, 'r').readlines()\n",
    "#         tok_lines = open(f_out_name, 'w')\n",
    "#         for i, sentence in enumerate(lines):\n",
    "#             if i > 0 and i % 1000 == 0:\n",
    "#                 print (f_name.split('/')[-1], i, len(lines))\n",
    "#             # replaced tokenizer(sentence) with str(tokenizer(sentence)) to avoid \n",
    "#             # type error while joining\n",
    "#             tok_lines.write(' '.join(str(tokenizer(sentence))) + '\\n')\n",
    "#         tok_lines.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     root = '../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-zh-en-processed/'\n",
    "#     tokenize_zh([os.path.join(root, 'dev.zh'), os.path.join(root, 'test.zh'), os.path.join(root, 'train.zh')],\\\n",
    "#                 [os.path.join(root, 'dev.tok.zh'), os.path.join(root, 'test.tok.zh'), os.path.join(root, 'train.tok.zh')])\n",
    "\n",
    "# #     tokenize_en([os.path.join(root, 'dev.en'), os.path.join(root, 'test.en'), os.path.join(root, 'train.en')],\\\n",
    "# #                [os.path.join(root, 'dev.tok.en'), os.path.join(root, 'test.tok.en'), os.path.join(root, 'train.tok.en')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213376 sentence pairs\n",
      "Trimmed to 213376 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 88917\n",
      "en 59329\n",
      "Reading lines...\n",
      "Read 1261 sentence pairs\n",
      "Trimmed to 1261 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 6132\n",
      "en 3916\n",
      "Reading lines...\n",
      "Read 1397 sentence pairs\n",
      "Trimmed to 1397 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 5214\n",
      "en 3423\n"
     ]
    }
   ],
   "source": [
    "# Format: languagepair_language_dataset\n",
    "# Train \n",
    "zhen_zh_train, zhen_en_train, zh_en_train_pairs = prepareData('zh', 'en', False, dataset=\"train\")\n",
    "# Dev \n",
    "zhen_zh_dev, zhen_en_dev, zh_en_dev_pairs = prepareData('zh', 'en', False, dataset=\"dev\")\n",
    "# Test\n",
    "zhen_zh_test, zhen_en_test, zh_en_test_pairs = prepareData('zh', 'en', False, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我们 将 用 一些 影片 来讲 讲述 一些 深海 海里 的 故事  ',\n",
       " 'And we apos re going to tell you some stories from the sea here in video .']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_en_train_pairs[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Check Source & Target Vocabs\n",
    "\n",
    "Since the source and target languages can have very different table lookup layers, it's good practice to have separate vocabularies for each. Thus, we build vocabularies for each language that we will be using. \n",
    "\n",
    "In the first class (Lang) of this section, we have already defined vocabularies for all languages. So, there is no need to redefine another function. We chech each vocabulary below.\n",
    "\n",
    "#### Chinese Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in Chinese training corpus is 88917\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in Chinese training corpus is \" + str(zhen_zh_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10479"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_zh_train.word2index[\"格\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'格'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_zh_train.index2word[10479]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vietnamese Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in Vietnamese training corpus is 16144\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in Vietnamese training corpus is \" + str(vien_vi_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6752"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_vi_train.word2index[\"Hamburger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hamburger'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_vi_train.index2word[6752]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Vocabulary for Zh-En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in English training corpus for Zh-En is 59329\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in English training corpus for Zh-En is \" + str(zhen_en_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1451"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_en_train.word2index[\"translate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'translate'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhen_en_train.index2word[1451]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Vocabulary for Vi-En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in English training corpus for Vi-En is 47568\n"
     ]
    }
   ],
   "source": [
    "print (\"The number of words in English training corpus for Vi-En is \" + str(vien_en_train.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "847"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_train.word2index[\"machine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vien_en_train.index2word[847]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Prepare Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = 2\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(paired_tokens, \n",
    "                        lang1_token2id_vocab,\n",
    "                        lang2_token2id_vocab):\n",
    "    \"\"\"Takes as input:\n",
    "    - paired_tokens: a list of sentence pairs that consist of source & target lang sentences.\n",
    "    - lang1_token2id_vocab: token2index vocabulary for the first language. \n",
    "                            Get by method Lang_dataset.word2index\n",
    "    - lang2_token2id_vocab: token2index vocabulary for the second language. \n",
    "                            Get by method Lang_dataset.word2index\n",
    "                            \n",
    "    Returns:\n",
    "    - indices_data_lang_1, indices_data_lang2: A list of lists where each sub-list holds corresponding indices for each\n",
    "                                               token in the sentence.\"\"\"\n",
    "    indices_data_lang_1, indices_data_lang_2 = [], []\n",
    "    vocabs = [lang1_token2id_vocab, lang2_token2id_vocab]\n",
    "    \n",
    "    # lang1\n",
    "    for t in range(len(paired_tokens)):\n",
    "        index_list = [SOS_token] + [vocabs[0][token] if token in vocabs[0]\\\n",
    "                                    else UNK_IDX for token in paired_tokens[t][0]] + [EOS_token]\n",
    "        indices_data_lang_1.append(index_list)\n",
    "    # lang2\n",
    "    for t in range(len(paired_tokens)):\n",
    "        index_list = [SOS_token] + [vocabs[1][token] if token in vocabs[1] \\\n",
    "                                    else UNK_IDX for token in paired_tokens[t][1]] + [EOS_token]\n",
    "        indices_data_lang_2.append(index_list)\n",
    "        \n",
    "    return indices_data_lang_1, indices_data_lang_2\n",
    "\n",
    "# train indices\n",
    "zhen_zh_train_indices, zhen_en_train_indices = token2index_dataset(zh_en_train_pairs,\n",
    "                                                                   zhen_zh_train.word2index,\n",
    "                                                                   zhen_en_train.word2index)\n",
    "\n",
    "vien_vi_train_indices, vien_en_train_indices = token2index_dataset(vi_en_train_pairs,\n",
    "                                                                   vien_vi_train.word2index,\n",
    "                                                                   vien_en_train.word2index)\n",
    "\n",
    "# dev indices\n",
    "zhen_zh_dev_indices, zhen_en_dev_indices = token2index_dataset(zh_en_dev_pairs,\n",
    "                                                               zhen_zh_dev.word2index,\n",
    "                                                               zhen_en_dev.word2index)\n",
    "\n",
    "vien_vi_dev_indices, vien_en_dev_indices = token2index_dataset(vi_en_dev_pairs,\n",
    "                                                               vien_vi_dev.word2index,\n",
    "                                                               vien_en_dev.word2index)\n",
    "\n",
    "# test indices\n",
    "zhen_zh_test_indices, zhen_en_test_indices = token2index_dataset(zh_en_test_pairs,\n",
    "                                                                 zhen_zh_test.word2index,\n",
    "                                                                 zhen_en_test.word2index)\n",
    "\n",
    "vien_vi_test_indices, vien_en_test_indices = token2index_dataset(vi_en_test_pairs,\n",
    "                                                                 vien_vi_test.word2index,\n",
    "                                                                 vien_en_test.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EOS_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese training set length = 213376\n",
      "Chinese-English (En) training set length = 213376\n",
      "\n",
      "Vietnamese training set length = 133317\n",
      "Vietnamese-English (En) training set length = 133317\n",
      "\n",
      "Chinese dev set length = 1261\n",
      "Chinese-English (En) dev set length = 1261\n",
      "\n",
      "Vietnamese dev set length = 1268\n",
      "Vietnamese-English (En) dev set length = 1268\n",
      "\n",
      "Chinese test set length = 1397\n",
      "Chinese-English (En) test set length = 1397\n",
      "\n",
      "Vietnamese test set length = 1553\n",
      "Vietnamese-English (En) test set length = 1553\n"
     ]
    }
   ],
   "source": [
    "# check length\n",
    "# train\n",
    "print (\"Chinese training set length = \"+str(len(zhen_zh_train_indices)))\n",
    "print (\"Chinese-English (En) training set length = \"+str(len(zhen_en_train_indices)))\n",
    "print (\"\\nVietnamese training set length = \"+str(len(vien_vi_train_indices)))\n",
    "print (\"Vietnamese-English (En) training set length = \"+str(len(vien_en_train_indices)))\n",
    "# dev\n",
    "print (\"\\nChinese dev set length = \"+str(len(zhen_zh_dev_indices)))\n",
    "print (\"Chinese-English (En) dev set length = \"+str(len(zhen_en_dev_indices)))\n",
    "print (\"\\nVietnamese dev set length = \"+str(len(vien_vi_dev_indices)))\n",
    "print (\"Vietnamese-English (En) dev set length = \"+str(len(vien_en_dev_indices)))\n",
    "# test\n",
    "print (\"\\nChinese test set length = \"+str(len(zhen_zh_test_indices)))\n",
    "print (\"Chinese-English (En) test set length = \"+str(len(zhen_en_test_indices)))\n",
    "print (\"\\nVietnamese test set length = \"+str(len(vien_vi_test_indices)))\n",
    "print (\"Vietnamese-English (En) test set length = \"+str(len(vien_en_test_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNK_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO \n",
    "\n",
    "MAX_SENTENCE_LENGTH = 250\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# zhen token2index vocabs\n",
    "zhen_zh_train_token2id = zhen_zh_train.word2index\n",
    "zhen_en_train_token2id = zhen_en_train.word2index\n",
    "\n",
    "# vien token2index vocabs\n",
    "vien_vi_train_token2id = vien_vi_train.word2index\n",
    "vien_en_train_token2id = vien_en_train.word2index\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/dev/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 data_source, # training indices data of the source language\n",
    "                 data_target, # training indices data of the target language\n",
    "                 token2id_source=None, # token2id dict of the source language\n",
    "                 token2id_target=None  # token2id dict of the target language\n",
    "                ):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.source_sentences, self.target_sentences =  data_source, data_target\n",
    "        \n",
    "        self.token2id_source = token2id_source\n",
    "        self.token2id_target = token2id_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def __getitem__(self, batch_index):\n",
    "\n",
    "#         source_word_idx, target_word_idx = [], []\n",
    "        source_mask, target_mask = [], []\n",
    "        \n",
    "        for index in source_sentences[batch_index][:MAX_SENTENCE_LENGTH]:\n",
    "            if index != UNK_IDX:\n",
    "                source_mask.append(0)\n",
    "            else:\n",
    "                source_mask.append(1)\n",
    "                \n",
    "        for index in target_sentences[batch_index][:MAX_SENTENCE_LENGTH]:\n",
    "            if index != UNK_IDX:\n",
    "                target_mask.append(0)\n",
    "            else:\n",
    "                target_mask.append(1)\n",
    "        \n",
    "        source_indices = self.source_sentences[batch_index]\n",
    "        target_indices = self.target_sentences[batch_index]\n",
    "        \n",
    "        source_list = [source_indices, source_mask, len(source_indices)]\n",
    "        target_list = [target_indices, target_mask, len(target_indices)]\n",
    "        \n",
    "        return source_list + target_list\n",
    "\n",
    "    \n",
    "def translation_collate(batch, max_sentence_length):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    source_data, target_data = [], []\n",
    "    source_mask, target_mask = [], []\n",
    "    source_lengths, target_lengths = [], []\n",
    "\n",
    "    for datum in batch:\n",
    "        source_lengths.append(datum[2])\n",
    "        target_lengths.append(datum[5])\n",
    "        \n",
    "        # PAD\n",
    "        source_data_padded = np.pad(np.array(datum[0]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        source_data.append(source_data_padded)\n",
    "        \n",
    "        source_mask_padded = np.pad(np.array(datum[1]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        source_mask.append(source_mask_padded)\n",
    "        \n",
    "        target_data_padded = np.pad(np.array(datum[3]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        target_data.append(target_data_padded)\n",
    "        \n",
    "        target_mask_padded = np.pad(np.array(datum[4]), pad_width=((0, MAX_SENTENCE_LENGTH-datum[5])),\n",
    "                               mode=\"constant\", constant_values=0)\n",
    "        target_mask.append(target_mask_padded)\n",
    "        \n",
    "    ind_dec_order = np.argsort(source_lengths)[::-1]\n",
    "    source_data = np.array(source_data)[ind_dec_order]\n",
    "    target_data = np.array(target_data)[ind_dec_order]\n",
    "    source_mask = np.array(source_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    target_mask = np.array(target_mask)[ind_dec_order].reshape(len(batch), -1, 1)\n",
    "    source_lengths = np.array(source_lengths)[ind_dec_order]\n",
    "    target_lengths = np.array(target_lengths)[ind_dec_order]\n",
    "    \n",
    "    source_list = [torch.from_numpy(source_data), \n",
    "               torch.from_numpy(source_mask).float(), source_lengths]\n",
    "    target_list = [torch.from_numpy(target_data), \n",
    "               torch.from_numpy(target_mask).float(), target_lengths]\n",
    "        \n",
    "    return source_list + target_list\n",
    "\n",
    "\n",
    "zhen_train_dataset = TranslationDataset(zhen_zh_train_indices,\n",
    "                                       zhen_en_train_indices,\n",
    "                                       token2id_source=zhen_zh_train_token2id,\n",
    "                                       token2id_target=zhen_en_train_token2id)\n",
    "\n",
    "zhen_train_loader = torch.utils.data.DataLoader(dataset=zhen_train_dataset,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, max_sentence_length),\n",
    "                               shuffle=False)\n",
    "\n",
    "zhen_dev_dataset = TranslationDataset(zhen_zh_dev_indices,\n",
    "                                       zhen_en_dev_indices,\n",
    "                                       token2id_source=zhen_zh_train_token2id,\n",
    "                                       token2id_target=zhen_en_train_token2id)\n",
    "\n",
    "zhen_dev_loader = torch.utils.data.DataLoader(dataset=zhen_dev_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, max_sentence_length),\n",
    "                             shuffle=False)\n",
    "\n",
    "vien_train_dataset = TranslationDataset(vien_vi_train_indices,\n",
    "                                       vien_en_train_indices,\n",
    "                                       token2id_source=vien_vi_train_token2id,\n",
    "                                       token2id_target=vien_en_train_token2id)\n",
    "\n",
    "vien_train_loader = torch.utils.data.DataLoader(dataset=vien_train_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, max_sentence_length),\n",
    "                             shuffle=False)\n",
    "\n",
    "vien_dev_dataset = TranslationDataset(vien_vi_dev_indices,\n",
    "                                       vien_en_dev_indices,\n",
    "                                       token2id_source=vien_vi_train_token2id,\n",
    "                                       token2id_target=vien_en_train_token2id)\n",
    "\n",
    "vien_dev_loader = torch.utils.data.DataLoader(dataset=vien_dev_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             collate_fn=lambda x, max_sentence_length=MAX_SENTENCE_LENGTH: translation_collate(x, max_sentence_length),\n",
    "                             shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*vien_dev_loader][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Evaluation Metric\n",
    "\n",
    "We use BLEU as the evaluation metric. Specifically, we focus on the corpus-level BLEU function. \n",
    "\n",
    "The code for BLEU is taken from https://github.com/mjpost/sacreBLEU/blob/master/sacrebleu.py#L1022-L1080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /Users/derin/anaconda/lib/python3.6/site-packages (1.2.12)\n",
      "Requirement already satisfied: typing in /Users/derin/anaconda/lib/python3.6/site-packages (from sacrebleu) (3.6.6)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Beam Search Algorithm\n",
    "\n",
    "In this section, we implement the Beam Search algorithm in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize k-many score lists\n",
    "# start only with the whole x\n",
    "# initialize k-many prev y's lists\n",
    "# choose top-k for y1 from the whole vocab\n",
    "# choose top-k for the second time step by expanding the first time step\n",
    "# compute scores by adding log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1 = {\"Asena\": 1, \"Derin\":2}\n",
    "dict1.pop(\"Asena\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Derin': 2}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size_k = 3\n",
    "\n",
    "class BeamSearch:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 encoded_X,\n",
    "                 beam_size=beam_size_k,\n",
    "                 time_step=None ## insert num \n",
    "                ):\n",
    "        \"\"\"\n",
    "        - beam_size = beam size\n",
    "        \n",
    "        \"\"\"\n",
    "        self.beam_size = beam_size\n",
    "        self.time_step = time_step\n",
    "        \n",
    "        # initialize the dictionary that will hold lists of path indices\n",
    "        # e.g. [5th elm of vocab, 7th elm of vocab, etc.]\n",
    "        # and update the lists at each time step\n",
    "        self.path_dict = {}\n",
    "        # initialize k-many path description lists\n",
    "        for i in range(self.beam_size):\n",
    "            self.path_dict[i] = []\n",
    "        \n",
    "        # initialize the dictionary that will hold the path scores \n",
    "        # and update the scores at each time step\n",
    "        self.path_score_dict = {}\n",
    "        # we will later use each i < k as a key and populate this\n",
    "        # dict with scores\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def score(prev_ys = None):\n",
    "        \"\"\"- prev_ys = previously decoded tokens (previously generated target language tokens)\n",
    "        \"\"\"\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model\n",
    "\n",
    "1. Recurrent neural network based encoder-decoder without attention\n",
    "2. Recurrent neural network based encoder-decoder with attention\n",
    "2. Replace the recurrent encoder with either convolutional or self-attention based encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: RNN-based Encoder-Decoder without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN-based encoder-decoder without attention\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embedding_size,\n",
    "                 percent_dropout, \n",
    "                 hidden_size,\n",
    "                 num_gru_layers):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embedding_size\n",
    "        self.dropout = percent_dropout\n",
    "        self.embed_source = nn.Embedding(self.vocab_size, \n",
    "                                  self.embed_dim, padding_idx=0)\n",
    "        self.embed_target = nn.Embedding(self.vocab_size, \n",
    "                                  self.embed_dim, padding_idx=0)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_gru_layers\n",
    "        \n",
    "        self.GRU = nn.GRU(self.embed_size, \n",
    "                          self.hidden_size, \n",
    "                          self.num_layers, \n",
    "                          batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.drop_out_function = nn.Dropout(self.dropout)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.randn(2*self.num_layers, ## 2 for bidirectional\n",
    "                             batch_size, self.hidden_size).to(device)\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, source_sentence, target_sentence,\n",
    "                mask, lengths):\n",
    "        \n",
    "        sort_original = sorted(range(len(lengths)), \n",
    "                             key=lambda sentence: -lengths[sentence])\n",
    "        unsort_to_original = sorted(range(len(lengths)), \n",
    "                             key=lambda sentence: sort_original[sentence])\n",
    "        \n",
    "        sentence = sentence[sort_original]\n",
    "        _mask = mask[sort_original]\n",
    "        lengths = lengths[sort_original]\n",
    "        \n",
    "        batch_size, seq_len = sentence.size()\n",
    "        \n",
    "        # init hidden\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        embeds_source = self.embed_source(source_sentence)\n",
    "        embeds_target = self.embed_target(target_sentence)\n",
    "        embeds = mask*embeds + (1-_mask)*embeds.clone().detach()\n",
    "        embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, lengths, \n",
    "                                                         batch_first=True)\n",
    "        \n",
    "        gru_out, self.hidden = self.GRU(embeds, self.hidden)\n",
    "        \n",
    "        # undo packing\n",
    "        gru_out, _ = torch.nn.utils.rnn.pad_packed_sequence(gru_out, \n",
    "                                                            batch_first=True)\n",
    "        \n",
    "        gru_out = gru_out.view(batch_size, -1, 2, self.hidden_size)\n",
    "        gru_out = torch.sum(gru_out, dim=1)\n",
    "        gru_out = torch.cat([gru_out[:,i,:] for i in range(2)], dim=1)\n",
    "        gru_out = gru_out[unsort_to_original] ## back to original indices\n",
    "        \n",
    "        return gru_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 RNN-based Encoder-Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoder Replacement with Eonvolutional or Self-attention-based Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Fully self-attention Translation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Multilingual Translation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
