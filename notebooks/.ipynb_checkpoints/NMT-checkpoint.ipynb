{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Spring 2018 NLP Class Project: Neural Machine Translation</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pdb\n",
    "import os\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Project Overview\n",
    "\n",
    "The goal of this project is to build a neural machine translation system and experience how recent advances have made their way. Each team will build the following sequence of neural translation systems for two language pairs, __Vietnamese (Vi)→English (En)__ and __Chinese (Zh)→En__ (prepared corpora is be provided):\n",
    "\n",
    "1. Recurrent neural network based encoder-decoder without attention\n",
    "2. Recurrent neural network based encoder-decoder with attention\n",
    "2. Replace the recurrent encoder with either convolutional or self-attention based encoder.\n",
    "4. [Optional] Build either or both fully self-attention translation system or/and multilingual translation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Upload & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of sentence\n",
    "SOS_token = 0\n",
    "# end of sentence\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    \"\"\"About \"NFC\" and \"NFD\": \n",
    "    \n",
    "    For each character, there are two normal forms: normal form C \n",
    "    and normal form D. Normal form D (NFD) is also known as canonical \n",
    "    decomposition, and translates each character into its decomposed form. \n",
    "    Normal form C (NFC) first applies a canonical decomposition, then composes \n",
    "    pre-combined characters again.\n",
    "    \n",
    "    About unicodedata.category: \n",
    "    \n",
    "    Returns the general category assigned to the Unicode character \n",
    "    unichr as string.\"\"\"\n",
    "    \n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Trim\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False,\n",
    "             dataset=\"train\"):\n",
    "    \n",
    "    \"\"\"Takes as input;\n",
    "    - lang1, lang2: either (vi, en) or (zh, en)\n",
    "    - dataset: one of (\"train\",\"dev\",\"test\")\"\"\"\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the lang1 file and split into lines\n",
    "    lang1_lines = open(\"../data/iwslt-%s-%s/%s.tok.%s\" % (lang1, lang2, dataset, lang1), encoding=\"utf-8\").\\\n",
    "        read().strip().split(\"\\n\")\n",
    "    # Read the lang2 file and split into lines\n",
    "    lang2_lines = open(\"../data/iwslt-%s-%s/%s.tok.%s\" % (lang1, lang2, dataset, lang2), encoding=\"utf-8\").\\\n",
    "        read().strip().split(\"\\n\")\n",
    "    \n",
    "    # create sentence pairs (lists of length 2 that consist of string pairs)\n",
    "    # e.g. [\"And we &apos;re going to tell you some stories from the sea here in video .\",\n",
    "    #       \"我们 将 用 一些 影片 来讲 讲述 一些 深海 海里 的 故事  \"]\n",
    "    # check if there are the same number of sentences in each set\n",
    "    assert len(lang1_lines) == len(lang2_lines), \"Two languages must have the same number of sentences. \"+ str(len(lang1_lines)) + \" sentences were passed for \" + str(lang1) + \".\" + str(len(lang2_lines)) + \" sentences were passed for \" + str(lang2)+\".\"\n",
    "    # normalize\n",
    "    lang1_lines = [normalizeString(s) for s in lang1_lines]\n",
    "    lang2_lines = [normalizeString(s) for s in lang2_lines]\n",
    "    # construct pairs\n",
    "    pair_ran = range(len(lang1_lines))\n",
    "    pairs = [[lang1_lines[i]] + [lang2_lines[i]] for i in pair_ran]\n",
    "    \n",
    "#     # Split every line into pairs and normalize\n",
    "#     pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 133317 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "en 47568\n",
      "vi 16144\n",
      "['And Madhav said quot Why do we have to do it that way ?', 'Va Madhav a noi rang Tai sao chung ta phai lam theo cach o ?']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False, dataset=\"train\"):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse, dataset=dataset)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('vi', 'en', False, dataset=\"train\")\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vietnamese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 133317 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 16144\n",
      "en 47568\n",
      "Reading lines...\n",
      "Read 1268 sentence pairs\n",
      "Trimmed to 1268 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 1370\n",
      "en 3816\n",
      "Reading lines...\n",
      "Read 1553 sentence pairs\n",
      "Trimmed to 1553 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "vi 1325\n",
      "en 3619\n"
     ]
    }
   ],
   "source": [
    "# Format: languagepair_language_dataset\n",
    "# Train \n",
    "vien_vi_train, vien_en_train, vi_en_train_pairs = prepareData('vi', 'en', False, dataset=\"train\")\n",
    "# Dev \n",
    "vien_vi_dev, vien_en_dev, vi_en_dev_pairs = prepareData('vi', 'en', False, dataset=\"dev\")\n",
    "# Test\n",
    "vien_vi_test, vien_en_test, vi_en_test_pairs = prepareData('vi', 'en', False, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Chinese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, spacy.tokens.token.Token found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3a52a01085b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#                 [os.path.join(root, 'dev.tok.zh'), os.path.join(root, 'test.tok.zh'), os.path.join(root, 'train.tok.zh')])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtokenize_en\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dev.en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test.en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m               \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dev.tok.en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test.tok.en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.tok.en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-3a52a01085b4>\u001b[0m in \u001b[0;36mtokenize_en\u001b[0;34m(f_names, f_out_names)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mtok_lines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtok_lines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, spacy.tokens.token.Token found"
     ]
    }
   ],
   "source": [
    "# Please find the original tokenizing code provided by Elman Mansimov in the following link:\n",
    "# \n",
    "\n",
    "def tokenize_zh(f_names, f_out_names):\n",
    "    for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "        lines = open(f_name, 'r').readlines()\n",
    "        tok_lines = open(f_out_name, 'w')\n",
    "        for i, sentence in enumerate(lines):\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                print (f_name.split('/')[-1], i, len(lines))\n",
    "            tok_lines.write(' '.join(jieba.cut(sentence, cut_all=True)))\n",
    "        tok_lines.close()\n",
    "\n",
    "def tokenize_en(f_names, f_out_names):\n",
    "    tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "    for f_name, f_out_name in zip(f_names, f_out_names):\n",
    "        lines = open(f_name, 'r').readlines()\n",
    "        tok_lines = open(f_out_name, 'w')\n",
    "        for i, sentence in enumerate(lines):\n",
    "            if i > 0 and i % 10000 == 0:\n",
    "                print (f_name.split('/')[-1], i, len(lines))\n",
    "            tok_lines.write(' '.join(tokenizer(sentence)) + '\\n')\n",
    "        tok_lines.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = '../data/tokens_and_preprocessing_em/pretokenized_data/iwslt-zh-en-processed/'\n",
    "#     tokenize_zh([os.path.join(root, 'dev.zh'), os.path.join(root, 'test.zh'), os.path.join(root, 'train.zh')],\\\n",
    "#                 [os.path.join(root, 'dev.tok.zh'), os.path.join(root, 'test.tok.zh'), os.path.join(root, 'train.tok.zh')])\n",
    "\n",
    "    tokenize_en([os.path.join(root, 'dev.en'), os.path.join(root, 'test.en'), os.path.join(root, 'train.en')],\\\n",
    "               [os.path.join(root, 'dev.tok.en'), os.path.join(root, 'test.tok.en'), os.path.join(root, 'train.tok.en')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213376 sentence pairs\n",
      "Trimmed to 213376 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 8006\n",
      "en 59329\n",
      "Reading lines...\n",
      "Read 1261 sentence pairs\n",
      "Trimmed to 1261 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 92\n",
      "en 3916\n",
      "Reading lines...\n",
      "Read 1397 sentence pairs\n",
      "Trimmed to 1397 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "zh 66\n",
      "en 3423\n"
     ]
    }
   ],
   "source": [
    "# Format: languagepair_language_dataset\n",
    "# Train \n",
    "zhen_zh_train, zhen_en_train, zh_en_train_pairs = prepareData('zh', 'en', False, dataset=\"train\")\n",
    "# Dev \n",
    "zhen_zh_dev, zhen_en_dev, zh_en_dev_pairs = prepareData('zh', 'en', False, dataset=\"dev\")\n",
    "# Test\n",
    "zhen_zh_test, zhen_en_test, zh_en_test_pairs = prepareData('zh', 'en', False, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model\n",
    "\n",
    "1. Recurrent neural network based encoder-decoder without attention\n",
    "2. Recurrent neural network based encoder-decoder with attention\n",
    "2. Replace the recurrent encoder with either convolutional or self-attention based encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: RNN-based Encoder-Decoder without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 RNN-based Encoder-Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoder Replacement with Eonvolutional or Self-attention-based Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Fully self-attention Translation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Multilingual Translation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
